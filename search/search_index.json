{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NREL HPC Resources Intro A collection of various resources, examples, and executables for the general NREL HPC user community's benefit. Documentation Contributing These docs are driven by the NREL HPC community. They are currently under active development. If you would like to contribute or recommend a topic to be covered please open an issue or pull request in the repository. Docs repository Workshops The HPC community also hosts workshops covering various topics. Check the training calendar below as well as the Computational Sciences Tutorials team to view all tutorials and workshops put on by the Computational Science Center. Join the team to receive notifications, seek out new or upcoming tutorials, see past slide desks, and access recordings of past trainings. For more information on joining this channel as a user external to NREL, see our CSC Tutorials Team - External Users . Additional NREL resources User Basics Eagle Slurm partitions Calendar Calendar of training events and office hours for NREL's HPC.","title":"Home"},{"location":"#nrel-hpc-resources","text":"","title":"NREL HPC Resources"},{"location":"#intro","text":"A collection of various resources, examples, and executables for the general NREL HPC user community's benefit. Documentation","title":"Intro"},{"location":"#contributing","text":"These docs are driven by the NREL HPC community. They are currently under active development. If you would like to contribute or recommend a topic to be covered please open an issue or pull request in the repository. Docs repository","title":"Contributing"},{"location":"#workshops","text":"The HPC community also hosts workshops covering various topics. Check the training calendar below as well as the Computational Sciences Tutorials team to view all tutorials and workshops put on by the Computational Science Center. Join the team to receive notifications, seek out new or upcoming tutorials, see past slide desks, and access recordings of past trainings. For more information on joining this channel as a user external to NREL, see our CSC Tutorials Team - External Users .","title":"Workshops"},{"location":"#additional-nrel-resources","text":"User Basics Eagle Slurm partitions","title":"Additional NREL resources"},{"location":"#calendar","text":"Calendar of training events and office hours for NREL's HPC.","title":"Calendar"},{"location":"Announcements/2020-11-03-announcement/","text":"ESIF-HPC-3 Project Update The ESIF-HPC-3 project has begun! The effort to acquire Eagle\u2019s successor involves ongoing engagement with stakeholders (EERE, Lab Program Management, and the HPC user community), tracking industry trends, analysis of Eagle\u2019s workload over its life to date, external design review, and a carefully managed Request for Proposals targeted for release later this year. We are currently in the process of completing the draft technical specifications for the ESIF-HPC-3 system, and are targeting the start of FY23 for general production access. If you would like to weigh in on how your work would benefit from existing or new features you could envision, please feel free to send a note to hpc-help@nrel.gov. We will open a discussion on the draft design in its current form if there is sufficient interest. Lustre Quotas Effective with the new Fiscal Year 2021 Project allocations for Eagle, quotas for approved storage allocations' capacities have been implemented on /projects and MSS on Eagle. This was to encourage users to manage their /projects data usage and usage of /scratch for jobs. HPC Operations is developing reporting capabilities of usage, but in the mean time, users may request help from the HPC Help Desk, or utilize these procedures from an Eagle Login node: To view project quotas and usage: Get the ProjectID for your /projects directory: lfs project -d /projects/csc000 110255 P /projects/csc000 Get the usage and quota in kbytes: lfs quota -p 110255 /projects/csc000 Disk quotas for prj 110255 (pid 110255): Filesystem kbytes quota limit grace files quota limit grace /projects/csc000 3165308* 3072 4096 - 48 1073741824 2147483648 - An * means you have exceeded your soft quota of 3072kb, the hard limit of 4096kb reached means no more writes are allowed. Grace period is set to default of 7 days but will show time until writes are suspended. \"files\" indicate the number of inodes used and soft and hard limits. We encourage users to run their jobs in Eagle /scratch and copy results and other necessary project files to /projects, possibly using tar and zip to conserve space (tar czvf tar-file-name.tgz source-directory-files-to-tar-and zip). If you are over your project quota, we recommend removing unneeded files and directories or moving them to your /scratch directory until no longer needed. Remember /scratch files are purged regularly per NREL\u2019s HPC storage retention policies. Changes to Eagle Mass Storage System (MSS) What: NREL HPC Operations is in the process of retiring the on-premise MSS capability and has started using cloud-based data storage capability. Why: The vendor has announced end-of-life for the technology previously used to provide MSS. In exploring alternatives, a cloud-based solution leverages expertise of the Advanced Computing Operations team and is significantly more cost effective. Adequate bandwith is available to/from Cloud (10x more bandwith than to/from the previous on-premise MSS). Also a very small percentage of data written to MSS is ever read again, thus prompting a change to a fixed 15-month retention from when data is written to MSS. When/How: On December 1st, all new writes to MSS will be to cloud-based storage. Reading data from the existing on-premise MSS capability will be supported through March 31, 2021. Active data (written or last read within the last 15 months prior to December 1st, 2020) will be migrated from the on-premise MSS to the new cloud-based storage. What stays the same: MSS is an additional location available to projects active on Eagle to keep and protect important data in addition to the Eagle high-performance storage (/projects, /shared-projects, /datasets) What changes: Data will be retained for 15 months from the date written. This differs from the current retention policy of minimum 15 months with deletion if needed. Restore requests of MSS data that are cloud-based will initially require a request to the HPC Help Desk, and may require 48 hours to be able to recover. Also a reminder: Project data (/projects) for FY20 projects not continuing into FY21 will have until December 31, 2020 to move data off Eagle to MSS or other long-term storage, before it is purged from Eagle on January 1st, 2021.","title":"November 2020 Monthly Update"},{"location":"Announcements/2020-11-03-announcement/#esif-hpc-3-project-update","text":"The ESIF-HPC-3 project has begun! The effort to acquire Eagle\u2019s successor involves ongoing engagement with stakeholders (EERE, Lab Program Management, and the HPC user community), tracking industry trends, analysis of Eagle\u2019s workload over its life to date, external design review, and a carefully managed Request for Proposals targeted for release later this year. We are currently in the process of completing the draft technical specifications for the ESIF-HPC-3 system, and are targeting the start of FY23 for general production access. If you would like to weigh in on how your work would benefit from existing or new features you could envision, please feel free to send a note to hpc-help@nrel.gov. We will open a discussion on the draft design in its current form if there is sufficient interest.","title":"ESIF-HPC-3 Project Update"},{"location":"Announcements/2020-11-03-announcement/#lustre-quotas","text":"Effective with the new Fiscal Year 2021 Project allocations for Eagle, quotas for approved storage allocations' capacities have been implemented on /projects and MSS on Eagle. This was to encourage users to manage their /projects data usage and usage of /scratch for jobs. HPC Operations is developing reporting capabilities of usage, but in the mean time, users may request help from the HPC Help Desk, or utilize these procedures from an Eagle Login node: To view project quotas and usage: Get the ProjectID for your /projects directory: lfs project -d /projects/csc000 110255 P /projects/csc000 Get the usage and quota in kbytes: lfs quota -p 110255 /projects/csc000 Disk quotas for prj 110255 (pid 110255): Filesystem kbytes quota limit grace files quota limit grace /projects/csc000 3165308* 3072 4096 - 48 1073741824 2147483648 - An * means you have exceeded your soft quota of 3072kb, the hard limit of 4096kb reached means no more writes are allowed. Grace period is set to default of 7 days but will show time until writes are suspended. \"files\" indicate the number of inodes used and soft and hard limits. We encourage users to run their jobs in Eagle /scratch and copy results and other necessary project files to /projects, possibly using tar and zip to conserve space (tar czvf tar-file-name.tgz source-directory-files-to-tar-and zip). If you are over your project quota, we recommend removing unneeded files and directories or moving them to your /scratch directory until no longer needed. Remember /scratch files are purged regularly per NREL\u2019s HPC storage retention policies.","title":"Lustre Quotas"},{"location":"Announcements/2020-11-03-announcement/#changes-to-eagle-mass-storage-system-mss","text":"What: NREL HPC Operations is in the process of retiring the on-premise MSS capability and has started using cloud-based data storage capability. Why: The vendor has announced end-of-life for the technology previously used to provide MSS. In exploring alternatives, a cloud-based solution leverages expertise of the Advanced Computing Operations team and is significantly more cost effective. Adequate bandwith is available to/from Cloud (10x more bandwith than to/from the previous on-premise MSS). Also a very small percentage of data written to MSS is ever read again, thus prompting a change to a fixed 15-month retention from when data is written to MSS. When/How: On December 1st, all new writes to MSS will be to cloud-based storage. Reading data from the existing on-premise MSS capability will be supported through March 31, 2021. Active data (written or last read within the last 15 months prior to December 1st, 2020) will be migrated from the on-premise MSS to the new cloud-based storage. What stays the same: MSS is an additional location available to projects active on Eagle to keep and protect important data in addition to the Eagle high-performance storage (/projects, /shared-projects, /datasets) What changes: Data will be retained for 15 months from the date written. This differs from the current retention policy of minimum 15 months with deletion if needed. Restore requests of MSS data that are cloud-based will initially require a request to the HPC Help Desk, and may require 48 hours to be able to recover. Also a reminder: Project data (/projects) for FY20 projects not continuing into FY21 will have until December 31, 2020 to move data off Eagle to MSS or other long-term storage, before it is purged from Eagle on January 1st, 2021.","title":"Changes to Eagle Mass Storage System (MSS)"},{"location":"Announcements/2020-12-03-announcement/","text":"FY20 Expired Projects' Data Reminder that FY20 expired Projects' data will be removed from Eagle on January 1st, 2021. Any data needed needs to either be copied to the new AWS MSS or other arrangements made outside of HPC. Due to vendor ending support for the old MSS equipment, the new HPC Mass Storage System (MSS) environment will reside on Amazon Web Services. The old Gyrfalcon MSS data will be made read-only on December 1st, 2020. Any data 15 months old or less will be migrated to AWS MSS. Changes to Eagle Mass Storage System (MSS) What: NREL HPC Operations is in the process of retiring the on-premise MSS capability and has started using cloud-based data storage capability. Why: The vendor has announced end-of-life for the technology previously used to provide MSS. In exploring alternatives, a cloud-based solution leverages expertise of the Advanced Computing Operations team and is significantly more cost effective. Adequate bandwith is available to/from Cloud (10x more bandwith than to/from the previous on-premise MSS). Also a very small percentage of data written to MSS is ever read again, thus prompting a change to a fixed 15-month retention from when data is written to MSS. When/How: On December 1st, all new writes to MSS will be to cloud-based storage. Reading data from the existing on-premise MSS capability will be supported through March 31, 2021. Active data (written or last read within the last 15 months prior to December 1st, 2020) will be migrated from the on-premise MSS to the new cloud-based storage. What stays the same: MSS is an additional location available to projects active on Eagle to keep and protect important data in addition to the Eagle high-performance storage (/projects, /shared-projects, /datasets) What changes: Data will be retained for 15 months from the date written. This differs from the current retention policy of minimum 15 months with deletion if needed. Restore requests of MSS data that are cloud-based will initially require a request to the HPC Help Desk, and may require 48 hours to be able to recover. ESIF-HPC-3 Project Update The ESIF-HPC-3 project moves on. The Request for Proposals and its many pieces (including the technical specifications, benchmark suite and specifications, and workload analysis) should be live by the time you read this. At this time, our hope is that vendors are busy designing the next-generation computing and storage systems that will serve EERE research starting in FY23, and preparing proposals for review by a cross-Directorate NREL Source Evaluation Team starting in mid-January 2021.","title":"December 2020 Monthly Update"},{"location":"Announcements/2020-12-03-announcement/#fy20-expired-projects-data","text":"Reminder that FY20 expired Projects' data will be removed from Eagle on January 1st, 2021. Any data needed needs to either be copied to the new AWS MSS or other arrangements made outside of HPC. Due to vendor ending support for the old MSS equipment, the new HPC Mass Storage System (MSS) environment will reside on Amazon Web Services. The old Gyrfalcon MSS data will be made read-only on December 1st, 2020. Any data 15 months old or less will be migrated to AWS MSS.","title":"FY20 Expired Projects' Data"},{"location":"Announcements/2020-12-03-announcement/#changes-to-eagle-mass-storage-system-mss","text":"What: NREL HPC Operations is in the process of retiring the on-premise MSS capability and has started using cloud-based data storage capability. Why: The vendor has announced end-of-life for the technology previously used to provide MSS. In exploring alternatives, a cloud-based solution leverages expertise of the Advanced Computing Operations team and is significantly more cost effective. Adequate bandwith is available to/from Cloud (10x more bandwith than to/from the previous on-premise MSS). Also a very small percentage of data written to MSS is ever read again, thus prompting a change to a fixed 15-month retention from when data is written to MSS. When/How: On December 1st, all new writes to MSS will be to cloud-based storage. Reading data from the existing on-premise MSS capability will be supported through March 31, 2021. Active data (written or last read within the last 15 months prior to December 1st, 2020) will be migrated from the on-premise MSS to the new cloud-based storage. What stays the same: MSS is an additional location available to projects active on Eagle to keep and protect important data in addition to the Eagle high-performance storage (/projects, /shared-projects, /datasets) What changes: Data will be retained for 15 months from the date written. This differs from the current retention policy of minimum 15 months with deletion if needed. Restore requests of MSS data that are cloud-based will initially require a request to the HPC Help Desk, and may require 48 hours to be able to recover.","title":"Changes to Eagle Mass Storage System (MSS)"},{"location":"Announcements/2020-12-03-announcement/#esif-hpc-3-project-update","text":"The ESIF-HPC-3 project moves on. The Request for Proposals and its many pieces (including the technical specifications, benchmark suite and specifications, and workload analysis) should be live by the time you read this. At this time, our hope is that vendors are busy designing the next-generation computing and storage systems that will serve EERE research starting in FY23, and preparing proposals for review by a cross-Directorate NREL Source Evaluation Team starting in mid-January 2021.","title":"ESIF-HPC-3 Project Update"},{"location":"Announcements/2021-01-04-announcement/","text":"Arbiter2 On Tuesday, January 12, we will be upgrading the Arbiter2 software on the Eagle login nodes. The upgrade improves stability of the program, as well as fixes some broken features. Arbiter2 limits individual resources on these shared resources within a range. Certain processes (for example, those related to code compilation) are exempt from these limits. For other processes, consistently high processor utilization leads to resource throttling in order to equalize the net amount of resource users have access to over time. As usage returns below a level consistent with the smooth operation of the shared login node, the throttling is relaxed. Users exceeding per-user resource limits on login nodes (\"in violation\") will receive emails when they trigger a violation, and when their usage returns below limits. From users' perspective the upgrade will not change limits or throttling behavior, it will just turn on notifications. ESIF-HPC-3 Project Update The ESIF-HPC-3 Request for Proposals is live! For those interested, the content can be found on SAM.gov. Application Updates Q-Chem has been upgraded to version 5.3.2. See changes here. Star-CCM version 15.06.008 is available on Eagle. ARM Forge version 20.2 is available on Eagle. We are working on acquiring a Maintenance license for VASP 6. Once we have this in place, users will need to have an upgraded VASP 6 Research workgroup license in order to use our VASP 6 builds on Eagle.","title":"January 2021 Monthly Update"},{"location":"Announcements/2021-01-04-announcement/#arbiter2","text":"On Tuesday, January 12, we will be upgrading the Arbiter2 software on the Eagle login nodes. The upgrade improves stability of the program, as well as fixes some broken features. Arbiter2 limits individual resources on these shared resources within a range. Certain processes (for example, those related to code compilation) are exempt from these limits. For other processes, consistently high processor utilization leads to resource throttling in order to equalize the net amount of resource users have access to over time. As usage returns below a level consistent with the smooth operation of the shared login node, the throttling is relaxed. Users exceeding per-user resource limits on login nodes (\"in violation\") will receive emails when they trigger a violation, and when their usage returns below limits. From users' perspective the upgrade will not change limits or throttling behavior, it will just turn on notifications.","title":"Arbiter2"},{"location":"Announcements/2021-01-04-announcement/#esif-hpc-3-project-update","text":"The ESIF-HPC-3 Request for Proposals is live! For those interested, the content can be found on SAM.gov.","title":"ESIF-HPC-3 Project Update"},{"location":"Announcements/2021-01-04-announcement/#application-updates","text":"Q-Chem has been upgraded to version 5.3.2. See changes here. Star-CCM version 15.06.008 is available on Eagle. ARM Forge version 20.2 is available on Eagle. We are working on acquiring a Maintenance license for VASP 6. Once we have this in place, users will need to have an upgraded VASP 6 Research workgroup license in order to use our VASP 6 builds on Eagle.","title":"Application Updates"},{"location":"Announcements/2021-02-02-announcement/","text":"Eagle Outage Eagle will be taking an outage on March 2nd. We will be making time critical hardware repairs to the Lustre file system and doing some security patching. Small tweaks are being made to slurm to reduce the amount of advertised memory and reduce the number of E-cells jobs run on. Both should make nodes and jobs more reliable and increase performance for larger jobs. Jupyterhub Documentation We have written up a how-to guide for using the Europa Jupyterhub server, including setting up custom Python, Julia, and R kernels and interacting with Eagle. See https://www.nrel.gov/hpc/jupyterhub.html for more. ESIF-HPC-3 Project Update We have pushed the proposal deadline out to February 18th and expect to have reviews completed sometime in mid-April. We have been fielding questions from various interested offerors, and are looking forward to seeing what they've got!","title":"February 2021 Monthly Update"},{"location":"Announcements/2021-02-02-announcement/#eagle-outage","text":"Eagle will be taking an outage on March 2nd. We will be making time critical hardware repairs to the Lustre file system and doing some security patching. Small tweaks are being made to slurm to reduce the amount of advertised memory and reduce the number of E-cells jobs run on. Both should make nodes and jobs more reliable and increase performance for larger jobs.","title":"Eagle Outage"},{"location":"Announcements/2021-02-02-announcement/#jupyterhub-documentation","text":"We have written up a how-to guide for using the Europa Jupyterhub server, including setting up custom Python, Julia, and R kernels and interacting with Eagle. See https://www.nrel.gov/hpc/jupyterhub.html for more.","title":"Jupyterhub Documentation"},{"location":"Announcements/2021-02-02-announcement/#esif-hpc-3-project-update","text":"We have pushed the proposal deadline out to February 18th and expect to have reviews completed sometime in mid-April. We have been fielding questions from various interested offerors, and are looking forward to seeing what they've got!","title":"ESIF-HPC-3 Project Update"},{"location":"Announcements/2021-03-03-announcement/","text":"Elevate Your Work With New Tracking for Advanced Computing in the NREL Publishing Tracker There is a new question on the User Facilities & Program Areas page when you enter a publication into the Pub Tracker \u2013 \"The High Performance Computing Facility was used to produce results or data used in this publication.\" Please be sure to check Yes on this question for your work that made use of the HPC User Facility or other systems in the ESIF HPC Data Center. In addition, there are three new Program Areas to use to tag your publication under the Advanced Computing heading: Cloud, HPC and Visualization & Insight Center. Making use of these metadata will enable us to elevate your work through communications highlights, feature stories, and reporting to EERE. More information about the NREL Publishing Tracker can be found by visiting the Access and Use the NREL Publishing Tracker page on the Source. Fiscal Year 2021 Quarterly Allocation Reductions You may have noticed that NREL did not make any reductions to allocations that were under-using their AUs during Q1. This is for two reasons. First, we were in the process of putting together a new, more transparent, fairer allocation reduction policy. Second, we are aware that many users were inconvenienced by the fact that allocation decisions were issued on October 1. We realize allocation reductions for low use are not popular with our users. However, they are physically necessary. AUs are an \"expiring resource.\" If an AU is not used in Q1, it cannot be stored and saved for use in Q4. Because of this, we have to remove some percentage of the unused AUs every quarter. Otherwise we can hit a situation where we have many more AUs available to users than the machine can physically provide as the year progresses. This creates long queue times that make Eagle physically unusable. Our new allocation policy is given below. We had an informal discussion with users across multiple centers before designing this policy. Users emphasized the need for a policy to be clear enough so they could see what they might lose at the end of the quarter. Users have also increasingly requested allocations that had different usages in different quarters to deal with their project needs, and we wanted a policy that treated these fairly. Shortly after Q1, Q2, and Q3 ends, allocations will be automatically adjusted to account for low utilization against planned usage. At the end of each quarter, the total allocation units used for the year to date will be compared to the total planned usage for the year to date for each allocation. At the end of Q1, the total used for the year to date is compared to the Q1 planned usage; at the end of Q2, the total used for the year to date is compared to the Q1+Q2 planned usage, and at the end of Q3, the toal used for the year to date is compared to the Q1+Q2+Q3 planned usage. Allocation units are then removed based on the table below. Note that allocation reductions are meant to be cumulative over the course of a year, and not compounding. If, for instance, a project was reduced by 10,000 AUs for low usage in Q1, and the reduction table suggests the project should be reduced by 25,000 AUs at the end of Q2, 15,000 (25,000-15,000) AUs should be removed at the end of Q2 to make the total removal for the year to date equal to 25,000 AUs. Percentage of planned AUs used to date Percentage of planned to date AUs removed More than 70% 0% (No reduction) Less than 70% but greater than 55% 20% Less than 55% but greater than 40% 35% Less than 40% but greater than 20% 55% Less than 20% 80% To understand how this process would work, we consider the following two 100,000 AU allocations, one with a uniform distribution of planned AUs throughout the year, and one with a distribution designed to enable development in Q1 and production runs in Q2 through Q4. The two allocations are described in the table below. Quarter Allocation \"Renewables\" Allocation \"Efficiency\" Q1 25,000 10,000 Q2 25,000 30,000 Q3 25,000 30,000 Q4 25,000 30,000 If \"Renewables\" uses 9,000 AUs in Q1, 20,000 AUs in Q2, and 25,000 AUs in Q3, it would be reduced in the following manner: After Q1, the project will have used 36% (9,000/25,000) if its allocation, leading to 13,750 (55% x 25,000) AUs being removed. After Q2, the project will have used 58% [(9,000+20,000)/(25,000+25,000)] of its allocation, leading to 10,000 AUs (20% x 50,000) being potentially removed. However, because 12,500 AUs were removed in Q1, no removal is performed. Note that AUs are not restored in this case. After Q3, the project will have used 72 % [(9,000+20,000+25,000)/(25,000+25,000+25,000)] of its allocation. No reduction would be made. The project would then begin Q4 with 33,500 AUs (100,000-9,000-20,000-25,000-12,500). If \"Efficiency\" uses AUs under the same schedule, after Q1, the project will have used 90% (9,000/10,000) of its allocation and will not be penalized. After Q2, the project will have used 72.5% [(9,000+20,000)/(10,000+30,000)] of its allocation and will not be penalized. After Q3, the project will have used 77% [(9,000+20,000+25,000)/(10,000+30,000+30,000)] of its allocation, and will not be penalized. The project would then begin Q4 with 46,000 AUs. Note that \"Renewables\" and \"Efficiency\" have the same total (100,000 AUs) but lose very different amounts of AUs over the course of the year. This is because the allocation request \"Efficiency\" is more closely tuned to the user\u2019s actual us of HPC resources. NREL allows users to tune their allocation request through the use of \"profiles\" in the allocation request to avoid this sorts of reductions. This information can also be found by visiting the Fiscal Year 2021 Quarterly Allocation Reductions page on our website.","title":"March 2021 Monthly Update"},{"location":"Announcements/2021-03-03-announcement/#elevate-your-work-with-new-tracking-for-advanced-computing-in-the-nrel-publishing-tracker","text":"There is a new question on the User Facilities & Program Areas page when you enter a publication into the Pub Tracker \u2013 \"The High Performance Computing Facility was used to produce results or data used in this publication.\" Please be sure to check Yes on this question for your work that made use of the HPC User Facility or other systems in the ESIF HPC Data Center. In addition, there are three new Program Areas to use to tag your publication under the Advanced Computing heading: Cloud, HPC and Visualization & Insight Center. Making use of these metadata will enable us to elevate your work through communications highlights, feature stories, and reporting to EERE. More information about the NREL Publishing Tracker can be found by visiting the Access and Use the NREL Publishing Tracker page on the Source.","title":"Elevate Your Work With New Tracking for Advanced Computing in the NREL Publishing Tracker"},{"location":"Announcements/2021-03-03-announcement/#fiscal-year-2021-quarterly-allocation-reductions","text":"You may have noticed that NREL did not make any reductions to allocations that were under-using their AUs during Q1. This is for two reasons. First, we were in the process of putting together a new, more transparent, fairer allocation reduction policy. Second, we are aware that many users were inconvenienced by the fact that allocation decisions were issued on October 1. We realize allocation reductions for low use are not popular with our users. However, they are physically necessary. AUs are an \"expiring resource.\" If an AU is not used in Q1, it cannot be stored and saved for use in Q4. Because of this, we have to remove some percentage of the unused AUs every quarter. Otherwise we can hit a situation where we have many more AUs available to users than the machine can physically provide as the year progresses. This creates long queue times that make Eagle physically unusable. Our new allocation policy is given below. We had an informal discussion with users across multiple centers before designing this policy. Users emphasized the need for a policy to be clear enough so they could see what they might lose at the end of the quarter. Users have also increasingly requested allocations that had different usages in different quarters to deal with their project needs, and we wanted a policy that treated these fairly. Shortly after Q1, Q2, and Q3 ends, allocations will be automatically adjusted to account for low utilization against planned usage. At the end of each quarter, the total allocation units used for the year to date will be compared to the total planned usage for the year to date for each allocation. At the end of Q1, the total used for the year to date is compared to the Q1 planned usage; at the end of Q2, the total used for the year to date is compared to the Q1+Q2 planned usage, and at the end of Q3, the toal used for the year to date is compared to the Q1+Q2+Q3 planned usage. Allocation units are then removed based on the table below. Note that allocation reductions are meant to be cumulative over the course of a year, and not compounding. If, for instance, a project was reduced by 10,000 AUs for low usage in Q1, and the reduction table suggests the project should be reduced by 25,000 AUs at the end of Q2, 15,000 (25,000-15,000) AUs should be removed at the end of Q2 to make the total removal for the year to date equal to 25,000 AUs. Percentage of planned AUs used to date Percentage of planned to date AUs removed More than 70% 0% (No reduction) Less than 70% but greater than 55% 20% Less than 55% but greater than 40% 35% Less than 40% but greater than 20% 55% Less than 20% 80% To understand how this process would work, we consider the following two 100,000 AU allocations, one with a uniform distribution of planned AUs throughout the year, and one with a distribution designed to enable development in Q1 and production runs in Q2 through Q4. The two allocations are described in the table below. Quarter Allocation \"Renewables\" Allocation \"Efficiency\" Q1 25,000 10,000 Q2 25,000 30,000 Q3 25,000 30,000 Q4 25,000 30,000 If \"Renewables\" uses 9,000 AUs in Q1, 20,000 AUs in Q2, and 25,000 AUs in Q3, it would be reduced in the following manner: After Q1, the project will have used 36% (9,000/25,000) if its allocation, leading to 13,750 (55% x 25,000) AUs being removed. After Q2, the project will have used 58% [(9,000+20,000)/(25,000+25,000)] of its allocation, leading to 10,000 AUs (20% x 50,000) being potentially removed. However, because 12,500 AUs were removed in Q1, no removal is performed. Note that AUs are not restored in this case. After Q3, the project will have used 72 % [(9,000+20,000+25,000)/(25,000+25,000+25,000)] of its allocation. No reduction would be made. The project would then begin Q4 with 33,500 AUs (100,000-9,000-20,000-25,000-12,500). If \"Efficiency\" uses AUs under the same schedule, after Q1, the project will have used 90% (9,000/10,000) of its allocation and will not be penalized. After Q2, the project will have used 72.5% [(9,000+20,000)/(10,000+30,000)] of its allocation and will not be penalized. After Q3, the project will have used 77% [(9,000+20,000+25,000)/(10,000+30,000+30,000)] of its allocation, and will not be penalized. The project would then begin Q4 with 46,000 AUs. Note that \"Renewables\" and \"Efficiency\" have the same total (100,000 AUs) but lose very different amounts of AUs over the course of the year. This is because the allocation request \"Efficiency\" is more closely tuned to the user\u2019s actual us of HPC resources. NREL allows users to tune their allocation request through the use of \"profiles\" in the allocation request to avoid this sorts of reductions. This information can also be found by visiting the Fiscal Year 2021 Quarterly Allocation Reductions page on our website.","title":"Fiscal Year 2021 Quarterly Allocation Reductions"},{"location":"Announcements/2021-04-06-announcement/","text":"Eagle File System Usage The Lustre file systems that hosts /projects and /scratch works most efficiently when it is under 80% full. Please do your part to keep the file system under 80% by cleaning up your /projects and /scratch spaces. Eagle System Time Eagle will have a system time for a work week starting May 3rd. There will be a power outage in the data center at this time, and we will need to do some work on Eagle as well. FY22 HPC Allocation Process The Eagle allocation process for FY22 will open up in May, with applications due in June (the exact dates are still to be decided.) The application process will be very similar to FY21. HPC Operations will again host a seminar to explain the application process. Watch this space for announcements. FY22 Cloud Allocation Process HPC and Cloud are both supported by Advanced Computing Operations (ACO) in the Computational Sciences Center. We are aligning the request process for both computing and cloud resources. The Cloud allocation process for FY22 will open up in May, with applications due in June (the exact dates are still to be decided.) The annual allocation process for cloud resources will be updated this year, and a new web interface will be provided for submitting your request. Cloud allocations have a different funding model from high performance computing: baseline services and administration of the cloud environment is funded by NREL, and project-specific computing and services is funded directly by the project. This means it\u2019s important to work with the cloud team to determine your needs in the cloud and to generate cost estimates for your project. Requests for cloud resources are reviewed and approved by the IACAC. Watch for the Call for Requests notification, and attend the upcoming Cloud Allocation Request webinar in May to learn more! Coming Soon: New Cloud User Website We are transitioning from the CSC Cloud Team Wiki to a website modeled after https://hpc.nrel.gov . The website will have a similar structure and look and feel that the user community is accustomed to.","title":"April 2021 Monthly Update"},{"location":"Announcements/2021-04-06-announcement/#eagle-file-system-usage","text":"The Lustre file systems that hosts /projects and /scratch works most efficiently when it is under 80% full. Please do your part to keep the file system under 80% by cleaning up your /projects and /scratch spaces.","title":"Eagle File System Usage"},{"location":"Announcements/2021-04-06-announcement/#eagle-system-time","text":"Eagle will have a system time for a work week starting May 3rd. There will be a power outage in the data center at this time, and we will need to do some work on Eagle as well.","title":"Eagle System Time"},{"location":"Announcements/2021-04-06-announcement/#fy22-hpc-allocation-process","text":"The Eagle allocation process for FY22 will open up in May, with applications due in June (the exact dates are still to be decided.) The application process will be very similar to FY21. HPC Operations will again host a seminar to explain the application process. Watch this space for announcements.","title":"FY22 HPC Allocation Process"},{"location":"Announcements/2021-04-06-announcement/#fy22-cloud-allocation-process","text":"HPC and Cloud are both supported by Advanced Computing Operations (ACO) in the Computational Sciences Center. We are aligning the request process for both computing and cloud resources. The Cloud allocation process for FY22 will open up in May, with applications due in June (the exact dates are still to be decided.) The annual allocation process for cloud resources will be updated this year, and a new web interface will be provided for submitting your request. Cloud allocations have a different funding model from high performance computing: baseline services and administration of the cloud environment is funded by NREL, and project-specific computing and services is funded directly by the project. This means it\u2019s important to work with the cloud team to determine your needs in the cloud and to generate cost estimates for your project. Requests for cloud resources are reviewed and approved by the IACAC. Watch for the Call for Requests notification, and attend the upcoming Cloud Allocation Request webinar in May to learn more!","title":"FY22 Cloud Allocation Process"},{"location":"Announcements/2021-04-06-announcement/#coming-soon-new-cloud-user-website","text":"We are transitioning from the CSC Cloud Team Wiki to a website modeled after https://hpc.nrel.gov . The website will have a similar structure and look and feel that the user community is accustomed to.","title":"Coming Soon: New Cloud User Website"},{"location":"Announcements/2021-05-05-announcement/","text":"Slurm Fairshare Refresher FY21 saw the introduction of the \"fairshare\" priority algorithm in Eagle's job scheduler, Slurm. Queue times have been high during the Q2-Q3 rush and we've received some questions, so here's a quick refresher on Fairshare and what it means in regards to job scheduling. The fairshare algorithm is a part of the Slurm \"multi-factor priority\" plugin that determines when a job should run. This algorithm is designed to help moderate queue usage by promoting jobs from under-utilized allocations, while over-utilized allocations get shifted towards CPU time that would otherwise be idle. The base fairshare value for an allocation is determined by the number of AUs allocated to a project, and is currently re-calculated on a quarterly basis. Every job that runs will affect the fairshare value, reducing the priority of future jobs. Larger jobs will have a larger impact, running smaller jobs will have less of an impact. The effects of any job on fairshare value will reduce by half every two weeks. And most importantly, fairshare only accounts for about half of job priority calculations--the rest relies on other factors, including the job's size, QOS setting, and partition. Queue Times The allocation year transitioned from Q2 to Q3 on April 1st. The job queue leading up to the end of Q2 saw a very large spike in jobs submitted, and queue depth (job wait time) rose accordingly. A few projects saw some effect of fairshare, but much of the pressure came from over a third of all jobs being submitted as qos=high. Because of the large surge in jobs submitted, interactions with fairshare and a few projects that have used up their allocation we have been analyzing the scheduling algorithms. Based on some recommendations from SchedMD and internal analysis we have made a few adjustments to the slurm configuration. Those changes thus far appear to have alleviated some of the pressure on the queues as well as a reduction in the number of jobs submitted with qos=high. Advanced Jupyter workshop (10am May 13th, 2021) Beyond the basics: this advanced Jupyter workshop will survey topics which enable you to get more out of your interactive notebooks. It will build on the recent Intro to Jupyter workshop and introduce additional Magic commands. Interacting with Slurm from a notebook will also be covered, and how this can be used to achieve multi-node parallelism. Additional topics include utilizing GPUs from a notebook, and parameterized notebook execution with Papermill.","title":"May 2021 Monthly Update"},{"location":"Announcements/2021-05-05-announcement/#slurm-fairshare-refresher","text":"FY21 saw the introduction of the \"fairshare\" priority algorithm in Eagle's job scheduler, Slurm. Queue times have been high during the Q2-Q3 rush and we've received some questions, so here's a quick refresher on Fairshare and what it means in regards to job scheduling. The fairshare algorithm is a part of the Slurm \"multi-factor priority\" plugin that determines when a job should run. This algorithm is designed to help moderate queue usage by promoting jobs from under-utilized allocations, while over-utilized allocations get shifted towards CPU time that would otherwise be idle. The base fairshare value for an allocation is determined by the number of AUs allocated to a project, and is currently re-calculated on a quarterly basis. Every job that runs will affect the fairshare value, reducing the priority of future jobs. Larger jobs will have a larger impact, running smaller jobs will have less of an impact. The effects of any job on fairshare value will reduce by half every two weeks. And most importantly, fairshare only accounts for about half of job priority calculations--the rest relies on other factors, including the job's size, QOS setting, and partition.","title":"Slurm Fairshare Refresher"},{"location":"Announcements/2021-05-05-announcement/#queue-times","text":"The allocation year transitioned from Q2 to Q3 on April 1st. The job queue leading up to the end of Q2 saw a very large spike in jobs submitted, and queue depth (job wait time) rose accordingly. A few projects saw some effect of fairshare, but much of the pressure came from over a third of all jobs being submitted as qos=high. Because of the large surge in jobs submitted, interactions with fairshare and a few projects that have used up their allocation we have been analyzing the scheduling algorithms. Based on some recommendations from SchedMD and internal analysis we have made a few adjustments to the slurm configuration. Those changes thus far appear to have alleviated some of the pressure on the queues as well as a reduction in the number of jobs submitted with qos=high.","title":"Queue Times"},{"location":"Announcements/2021-05-05-announcement/#advanced-jupyter-workshop-10am-may-13th-2021","text":"Beyond the basics: this advanced Jupyter workshop will survey topics which enable you to get more out of your interactive notebooks. It will build on the recent Intro to Jupyter workshop and introduce additional Magic commands. Interacting with Slurm from a notebook will also be covered, and how this can be used to achieve multi-node parallelism. Additional topics include utilizing GPUs from a notebook, and parameterized notebook execution with Papermill.","title":"Advanced Jupyter workshop (10am May 13th, 2021)"},{"location":"Announcements/2021-06-01-announcement/","text":"Fiscal Year 2022 HPC Annual Call for Allocation Requests The deadline for requests for HPC requests is next Monday, June 7, at Midnight Mountain Time. The submission portal will remain open after this date. However, late requests will receive lower priority than on-time requests. Please submit a request if you are a researcher at any national laboratory, university, or other organization pursing EERE-funded research, or if you are an NREL-affiliated researcher performing research aligned with the EERE mission funded through other organizations. Requests are welcome for current projects, projects where a funding request has been submitted, and projects where a funding request for FY21 is in preparation. Additional information on the Eagle allocation process is available at our Resource Allocation Requests page. Please e-mail hpc-requests@nrel.gov if you have any additional questions. CSC User and Applications Support A new Anaconda installation is in testing, and will be put into production shortly. To access the test installation, as always just enable the Test modules collection via module use /nopt/nrel/apps/modules/test/modulefiles and you should then see a conda/4.9.2 module. Unlike with previous Anaconda installations, we have enabled the conda activate and conda deactivate syntax without requiring conda init (which creates \"environmental\" problems, something we're all against). The \"source\"ing syntax still works, but you now have the option to use either. Our hope is that enabling the conda commands will permit greater interoperability with scripts developed elsewhere (where conda activate may have worked), perhaps prove slightly faster, and eliminate awkward error messages. This Conda deployment includes a new faster command for setting up new environments and installing packages. Instead of conda ... , you can try mamba ... , e.g., mamba install tensorflow . Other application upgrades have been or will be deployed shortly. If they are not already in production, you may access the installations via the module use statement above. App Version ANSYS 2021R1 CMake 3.18.2 COMSOL 5.6 CUDA 11, includes cudnn and development tools and libraries GAMS 34.3.0 MATLAB R2020b MPT 2.23 OpenMPI 4.1.0, including Java support AUP Renewals You may have received an email from DocuSign (dse_NA3@docusign.net) requesting that you renew your NREL HPC Appropriate Use Policy (AUP). We are required to maintain these agreements and, should you receive one, it will be necessary for you to complete it within 30 days in order to continue accessing HPC systems. We appreciate your cooperation.","title":"June 2021 Monthly Update"},{"location":"Announcements/2021-06-01-announcement/#fiscal-year-2022-hpc-annual-call-for-allocation-requests","text":"The deadline for requests for HPC requests is next Monday, June 7, at Midnight Mountain Time. The submission portal will remain open after this date. However, late requests will receive lower priority than on-time requests. Please submit a request if you are a researcher at any national laboratory, university, or other organization pursing EERE-funded research, or if you are an NREL-affiliated researcher performing research aligned with the EERE mission funded through other organizations. Requests are welcome for current projects, projects where a funding request has been submitted, and projects where a funding request for FY21 is in preparation. Additional information on the Eagle allocation process is available at our Resource Allocation Requests page. Please e-mail hpc-requests@nrel.gov if you have any additional questions.","title":"Fiscal Year 2022 HPC Annual Call for Allocation Requests"},{"location":"Announcements/2021-06-01-announcement/#csc-user-and-applications-support","text":"A new Anaconda installation is in testing, and will be put into production shortly. To access the test installation, as always just enable the Test modules collection via module use /nopt/nrel/apps/modules/test/modulefiles and you should then see a conda/4.9.2 module. Unlike with previous Anaconda installations, we have enabled the conda activate and conda deactivate syntax without requiring conda init (which creates \"environmental\" problems, something we're all against). The \"source\"ing syntax still works, but you now have the option to use either. Our hope is that enabling the conda commands will permit greater interoperability with scripts developed elsewhere (where conda activate may have worked), perhaps prove slightly faster, and eliminate awkward error messages. This Conda deployment includes a new faster command for setting up new environments and installing packages. Instead of conda ... , you can try mamba ... , e.g., mamba install tensorflow . Other application upgrades have been or will be deployed shortly. If they are not already in production, you may access the installations via the module use statement above. App Version ANSYS 2021R1 CMake 3.18.2 COMSOL 5.6 CUDA 11, includes cudnn and development tools and libraries GAMS 34.3.0 MATLAB R2020b MPT 2.23 OpenMPI 4.1.0, including Java support","title":"CSC User and Applications Support"},{"location":"Announcements/2021-06-01-announcement/#aup-renewals","text":"You may have received an email from DocuSign (dse_NA3@docusign.net) requesting that you renew your NREL HPC Appropriate Use Policy (AUP). We are required to maintain these agreements and, should you receive one, it will be necessary for you to complete it within 30 days in order to continue accessing HPC systems. We appreciate your cooperation.","title":"AUP Renewals"},{"location":"Announcements/2021-07-06-announcement/","text":"CSC User & Applications Support We will be making the conda/4.9.2 module the default module for loading (i.e., without a version number specified). By way of reminders, If you need to reference the existing default Anaconda installation, add the version number to your module load statement in job scripts, i.e., module load conda/mini_py37_4.8.3 rather than just module load conda . Custom environments should interoperate with either version, though. This module permits conda activate and conda deactivate functionality without conda init. Don't use conda init, as it breaks login shell setup. Consider trying mamba instead of conda when setting up environments. For example, mamba install to add a package to a custom environment. Julia modules are now available on Eagle. The module files are available at /nopt/nrel/ecom/modulefiles. Currently, versions 1.5.4 and 1.6.1 are available. If you plan on using these module files regularly, you may wish to add this directory to your module search path with the command: module use -a /nopt/nrel/ecom/modulefiles You can add this command to your .bash_profile or .bashrc file with the following command: echo 'module use -a /nopt/nrel/ecom/modulefiles' >> .bash_profile (or .bashrc in place of .bash_profile). Once your module path is updated, simply load the desired Julia version module: module load julia Questions or problems regarding Julia on Eagle can be sent to jonathan.maack@nrel.gov . Eagle Job Queue The number of running jobs on Eagle has been dipping on the weekends. Please think about submitting jobs to run over the weekend, especially long weekends, so we can keep the system full. Eagle System Time The next Eagle system time is scheduled for the week of August 2nd. This will be a multi day outage to do updates to the parallel file system as well as take care of some hardware issues affecting the compute nodes. Eagle and related file systems will be unavailable at during this system time.","title":"July 2021 Monthly Update"},{"location":"Announcements/2021-07-06-announcement/#csc-user-applications-support","text":"We will be making the conda/4.9.2 module the default module for loading (i.e., without a version number specified). By way of reminders, If you need to reference the existing default Anaconda installation, add the version number to your module load statement in job scripts, i.e., module load conda/mini_py37_4.8.3 rather than just module load conda . Custom environments should interoperate with either version, though. This module permits conda activate and conda deactivate functionality without conda init. Don't use conda init, as it breaks login shell setup. Consider trying mamba instead of conda when setting up environments. For example, mamba install to add a package to a custom environment. Julia modules are now available on Eagle. The module files are available at /nopt/nrel/ecom/modulefiles. Currently, versions 1.5.4 and 1.6.1 are available. If you plan on using these module files regularly, you may wish to add this directory to your module search path with the command: module use -a /nopt/nrel/ecom/modulefiles You can add this command to your .bash_profile or .bashrc file with the following command: echo 'module use -a /nopt/nrel/ecom/modulefiles' >> .bash_profile (or .bashrc in place of .bash_profile). Once your module path is updated, simply load the desired Julia version module: module load julia Questions or problems regarding Julia on Eagle can be sent to jonathan.maack@nrel.gov .","title":"CSC User &amp; Applications Support"},{"location":"Announcements/2021-07-06-announcement/#eagle-job-queue","text":"The number of running jobs on Eagle has been dipping on the weekends. Please think about submitting jobs to run over the weekend, especially long weekends, so we can keep the system full.","title":"Eagle Job Queue"},{"location":"Announcements/2021-07-06-announcement/#eagle-system-time","text":"The next Eagle system time is scheduled for the week of August 2nd. This will be a multi day outage to do updates to the parallel file system as well as take care of some hardware issues affecting the compute nodes. Eagle and related file systems will be unavailable at during this system time.","title":"Eagle System Time"},{"location":"Announcements/2021-08-06-announcement/","text":"August CSC User & Applications Support Q-Chem 5.4 will be made generally available. Look for the q-chem/5.4 module to appear in module avail output. Eagle Compute Node Local Storage and Limitations in /tmp The compute nodes on Eagle have local disks that may benefit your workload. Disk sizes start at 1TB, with a limited number of nodes available with more storage. The storage is available at /tmp/scratch. /tmp is in memory, and is limited in size. We occasionally see jobs filling /tmp, and recommend users adjust their environment to use /tmp/scratch.","title":"August 2021 Monthly Update"},{"location":"Announcements/2021-08-06-announcement/#august-csc-user-applications-support","text":"Q-Chem 5.4 will be made generally available. Look for the q-chem/5.4 module to appear in module avail output.","title":"August CSC User &amp; Applications Support"},{"location":"Announcements/2021-08-06-announcement/#eagle-compute-node-local-storage-and-limitations-in-tmp","text":"The compute nodes on Eagle have local disks that may benefit your workload. Disk sizes start at 1TB, with a limited number of nodes available with more storage. The storage is available at /tmp/scratch. /tmp is in memory, and is limited in size. We occasionally see jobs filling /tmp, and recommend users adjust their environment to use /tmp/scratch.","title":"Eagle Compute Node Local Storage and Limitations in /tmp"},{"location":"Announcements/2021-09-08-announcement/","text":"Reminder of Eagle Data Storage Policies for FY21 projects ending 9/30/2021 Eagle usage policies can be found at: HPC Policies Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource. In summary, data in /projects for allocations that end on 9/30/2021 will be purged on 12/31/2021. Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/ ) to another storage location. Users may continue to request MSS (Mass Storage System) files that have been archived, for a period of 15 months after the files have been initially archived. Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days. Please contact the HPC Help Desk with any questions at hpc-help@nrel.gov October is the start of the 2022 allocation year Scheduling will be paused on Eagle on October 1st to implement the new allocations.","title":"September 2021 Monthly Update"},{"location":"Announcements/2021-09-08-announcement/#reminder-of-eagle-data-storage-policies-for-fy21-projects-ending-9302021","text":"Eagle usage policies can be found at: HPC Policies Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource. In summary, data in /projects for allocations that end on 9/30/2021 will be purged on 12/31/2021. Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/ ) to another storage location. Users may continue to request MSS (Mass Storage System) files that have been archived, for a period of 15 months after the files have been initially archived. Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days. Please contact the HPC Help Desk with any questions at hpc-help@nrel.gov","title":"Reminder of Eagle Data Storage Policies for FY21 projects ending 9/30/2021"},{"location":"Announcements/2021-09-08-announcement/#october-is-the-start-of-the-2022-allocation-year","text":"Scheduling will be paused on Eagle on October 1st to implement the new allocations.","title":"October is the start of the 2022 allocation year"},{"location":"Announcements/2021-11-03-announcement/","text":"Reminder of Eagle Data Storage Policies for FY21 projects that ended on 9/30/2021 Eagle usage policies can be found at: https://www.nrel.gov/hpc/policies.html Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource. Eaglefs consists of /shared-projects, /datasets, /scratch and /projects. In summary, data in /projects for allocations that end on 9/30/2021 will be purged after 12/31/2021. Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/ ) to another storage location. The ability to write new data to /projects has been disabled for those projects that have expired. Instructions on how to archive data using AWS MSS (Mass Storage System) can be found here. Users may continue to request MSS files that have been archived, for a period of 15 months after the files have been initially archived. Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days. Fast Eagle Queue Times Eagle is now one month into the FY22 allocation cycle, and many projects are still in the process of preparing their software, data, and experiments. For those who are prepared to start computational work right away, though, now is a great opportunity to submit your jobs and beat the quarterly rush. Please also keep in mind that Allocation Units (AUs) are a function of how many hours there are in a year. In other words, an AU that goes unspent now cannot be saved for later, it is lost forever--so start your computational work on Eagle as early as possible in the allocation cycle.","title":"November 2021 Monthly Update"},{"location":"Announcements/2021-11-03-announcement/#reminder-of-eagle-data-storage-policies-for-fy21-projects-that-ended-on-9302021","text":"Eagle usage policies can be found at: https://www.nrel.gov/hpc/policies.html Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource. Eaglefs consists of /shared-projects, /datasets, /scratch and /projects. In summary, data in /projects for allocations that end on 9/30/2021 will be purged after 12/31/2021. Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/ ) to another storage location. The ability to write new data to /projects has been disabled for those projects that have expired. Instructions on how to archive data using AWS MSS (Mass Storage System) can be found here. Users may continue to request MSS files that have been archived, for a period of 15 months after the files have been initially archived. Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days.","title":"Reminder of Eagle Data Storage Policies for FY21 projects that ended on 9/30/2021"},{"location":"Announcements/2021-11-03-announcement/#fast-eagle-queue-times","text":"Eagle is now one month into the FY22 allocation cycle, and many projects are still in the process of preparing their software, data, and experiments. For those who are prepared to start computational work right away, though, now is a great opportunity to submit your jobs and beat the quarterly rush. Please also keep in mind that Allocation Units (AUs) are a function of how many hours there are in a year. In other words, an AU that goes unspent now cannot be saved for later, it is lost forever--so start your computational work on Eagle as early as possible in the allocation cycle.","title":"Fast Eagle Queue Times"},{"location":"Announcements/2021-12-08-announcement/","text":"Data Security Policy Reminder Eagle and systems in the ESIF data center are managed under a \"low\" Authority to Operate (ATO) per the FIPS 199 standard. End users should be familiar with NREL's HPC Data Security Policy for this class of systems. The potential impact rating of data is the responsibility of the data owner. The most common data security risk on Eagle is misconfiguration or misunderstanding of file permissions. This may involve accidentally setting UNIX ownership and/or permissions or ACLs on a directory that make files readable outside of the intended audience, or failing to remove permissions from users that should no longer have access to files. HPC Leads and PI\u2019s can mitigate any potential data exposure or leaks by checking file and directory ownerships and permissions, as well as updating their project entitlement in Lex as needed. Alternatively, HPC Leads or PI's could consider utilizing the AWS cloud environment \" Stratus \" as it has been classified for \"moderate impact\" data. File and folder permissions are an important way to you projects data from unintended access outside the project. Due to the number of people with privileged access to on ESIF data center systems, they are not a reasonable control for data rated outside of low. Data that is covered under a CRADA must be agreed upon by the legal entities that signed the CRADA as to what are the appropriate controls for that data. We understand that data classification can be challenging, and security requirements vary from project to project. We recommend project PI's contact the NREL Legal Department or their respective legal department with any questions regarding the classification of their data and where it may be safely stored. Eagle Second Quarter System Time The Eagle cluster will be offline for regular scheduled maintenance for the week beginning, January 10th at approximately 6:00am (Mountain), and will return to service on January 13th. Eagle login nodes, DAV/FastX nodes, all filesystems (including lustre and /home), Globus, and related support systems will be unavailable during this time. Network maintenance is planned during this time as well, and access to certain internet/external-facing HPC services (including eagle.nrel.gov, eagle-dav.nrel.gov, and the self-service password tool will be temporarily unavailable and all outbound traffic will have no access to the internet from the HPC datacenter. Reminder of Eagle Data Storage Policies for FY21 projects that ended on 9/30/2021 Eagle usage policies can be found on the Policies Page Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource. Eaglefs consists of /shared-projects, /datasets, /scratch and /projects. In summary, data in /projects for allocations that end on 9/30/2021 will be purged after 12/31/2021. Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/ ) to another storage location. The ability to write new data to /projects has been disabled for those projects that have expired. Instructions on how to archive data using AWS MSS (Mass Storage System) can be found here. Users may continue to request MSS files that have been archived, for a period of 15 months after the files have been initially archived. Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days.","title":"December 2021 Monthly Update"},{"location":"Announcements/2021-12-08-announcement/#data-security-policy-reminder","text":"Eagle and systems in the ESIF data center are managed under a \"low\" Authority to Operate (ATO) per the FIPS 199 standard. End users should be familiar with NREL's HPC Data Security Policy for this class of systems. The potential impact rating of data is the responsibility of the data owner. The most common data security risk on Eagle is misconfiguration or misunderstanding of file permissions. This may involve accidentally setting UNIX ownership and/or permissions or ACLs on a directory that make files readable outside of the intended audience, or failing to remove permissions from users that should no longer have access to files. HPC Leads and PI\u2019s can mitigate any potential data exposure or leaks by checking file and directory ownerships and permissions, as well as updating their project entitlement in Lex as needed. Alternatively, HPC Leads or PI's could consider utilizing the AWS cloud environment \" Stratus \" as it has been classified for \"moderate impact\" data. File and folder permissions are an important way to you projects data from unintended access outside the project. Due to the number of people with privileged access to on ESIF data center systems, they are not a reasonable control for data rated outside of low. Data that is covered under a CRADA must be agreed upon by the legal entities that signed the CRADA as to what are the appropriate controls for that data. We understand that data classification can be challenging, and security requirements vary from project to project. We recommend project PI's contact the NREL Legal Department or their respective legal department with any questions regarding the classification of their data and where it may be safely stored.","title":"Data Security Policy Reminder"},{"location":"Announcements/2021-12-08-announcement/#eagle-second-quarter-system-time","text":"The Eagle cluster will be offline for regular scheduled maintenance for the week beginning, January 10th at approximately 6:00am (Mountain), and will return to service on January 13th. Eagle login nodes, DAV/FastX nodes, all filesystems (including lustre and /home), Globus, and related support systems will be unavailable during this time. Network maintenance is planned during this time as well, and access to certain internet/external-facing HPC services (including eagle.nrel.gov, eagle-dav.nrel.gov, and the self-service password tool will be temporarily unavailable and all outbound traffic will have no access to the internet from the HPC datacenter.","title":"Eagle Second Quarter System Time"},{"location":"Announcements/2021-12-08-announcement/#reminder-of-eagle-data-storage-policies-for-fy21-projects-that-ended-on-9302021","text":"Eagle usage policies can be found on the Policies Page Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource. Eaglefs consists of /shared-projects, /datasets, /scratch and /projects. In summary, data in /projects for allocations that end on 9/30/2021 will be purged after 12/31/2021. Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/ ) to another storage location. The ability to write new data to /projects has been disabled for those projects that have expired. Instructions on how to archive data using AWS MSS (Mass Storage System) can be found here. Users may continue to request MSS files that have been archived, for a period of 15 months after the files have been initially archived. Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days.","title":"Reminder of Eagle Data Storage Policies for FY21 projects that ended on 9/30/2021"},{"location":"Announcements/2022-01-05-announcement/","text":"January 2022 System Time Reminder Eagle will be offline for regularly scheduled maintenance starting on Monday, January 10th, 2022 at 6:00am (Mountain Time.) The system is expected to return to service by January 13th, 2022. During this outage, the Eagle login nodes, DAV/FastX nodes, all filesystems (including lustre and /home), Globus, and other related support systems will be unavailable. Notable tasks for this system time include an upgrade to the Slurm scheduler, filesystem maintenance, cooling systems maintenance and repair, updates to FastX on ed7, and adjustments to the Arbiter2 resource monitor. Network maintenance is planned during this time as well. Access to certain internet/external-facing HPC services (including eagle.nrel.gov, eagle-dav.nrel.gov, and the self-service password tool at https://hpcusers.nrel.gov/) will be temporarily unavailable, and all outbound traffic will have no access to the internet from the HPC data center. Final Reminder of Eagle Data Storage Policies for FY21 projects that ended on 9/30/2021 Eagle usage policies can be found on the HPC website policy page . In summary, data in /projects for allocations that end on 9/30/2021 will be purged after 12/31/2021. Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/ ) to another storage location. The ability to write new data to /projects has been disabled for those projects that have expired. Instructions on how to archive data using AWS MSS (Mass Storage System) can be found on the Mass Storage page. Users may continue to request MSS files that have been archived, for a period of 15 months after the files have been initially archived. Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days. Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource. Eaglefs consists of /shared-projects, /datasets, /scratch and /projects. Thank you for participating in the annual survey We are extremely grateful to you for contributing your valuable time, your honest feedback, and your thoughtful suggestions. We are committed to utilizing the information to implement worthwhile improvements to the environment and our processes to make the cloud experience more efficient. We will share these implementations with you through our monthly newsletter. Applications COMSOL will provide NREL an overview of the new COMSOL 6.0 release on January 25, 2022, from 11:00 AM - 12:00 PM MST. To register, go to the COMSOL event registration page . Attend to learn about the latest features, and ask questions about COMSOL and NREL-HPC.","title":"January 2022 Monthly Update"},{"location":"Announcements/2022-01-05-announcement/#january-2022-system-time-reminder","text":"Eagle will be offline for regularly scheduled maintenance starting on Monday, January 10th, 2022 at 6:00am (Mountain Time.) The system is expected to return to service by January 13th, 2022. During this outage, the Eagle login nodes, DAV/FastX nodes, all filesystems (including lustre and /home), Globus, and other related support systems will be unavailable. Notable tasks for this system time include an upgrade to the Slurm scheduler, filesystem maintenance, cooling systems maintenance and repair, updates to FastX on ed7, and adjustments to the Arbiter2 resource monitor. Network maintenance is planned during this time as well. Access to certain internet/external-facing HPC services (including eagle.nrel.gov, eagle-dav.nrel.gov, and the self-service password tool at https://hpcusers.nrel.gov/) will be temporarily unavailable, and all outbound traffic will have no access to the internet from the HPC data center.","title":"January 2022 System Time Reminder"},{"location":"Announcements/2022-01-05-announcement/#final-reminder-of-eagle-data-storage-policies-for-fy21-projects-that-ended-on-9302021","text":"Eagle usage policies can be found on the HPC website policy page . In summary, data in /projects for allocations that end on 9/30/2021 will be purged after 12/31/2021. Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/ ) to another storage location. The ability to write new data to /projects has been disabled for those projects that have expired. Instructions on how to archive data using AWS MSS (Mass Storage System) can be found on the Mass Storage page. Users may continue to request MSS files that have been archived, for a period of 15 months after the files have been initially archived. Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days. Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource. Eaglefs consists of /shared-projects, /datasets, /scratch and /projects.","title":"Final Reminder of Eagle Data Storage Policies for FY21 projects that ended on 9/30/2021"},{"location":"Announcements/2022-01-05-announcement/#thank-you-for-participating-in-the-annual-survey","text":"We are extremely grateful to you for contributing your valuable time, your honest feedback, and your thoughtful suggestions. We are committed to utilizing the information to implement worthwhile improvements to the environment and our processes to make the cloud experience more efficient. We will share these implementations with you through our monthly newsletter.","title":"Thank you for participating in the annual survey"},{"location":"Announcements/2022-01-05-announcement/#applications","text":"COMSOL will provide NREL an overview of the new COMSOL 6.0 release on January 25, 2022, from 11:00 AM - 12:00 PM MST. To register, go to the COMSOL event registration page . Attend to learn about the latest features, and ask questions about COMSOL and NREL-HPC.","title":"Applications"},{"location":"Announcements/2022-02-02-announcement/","text":"Changes to Slurm \"srun\" for Interactive Jobs During the recent system time the Slurm job scheduler was upgraded. One of the side effects of this was a change in the way Slurm handles job steps internally in certain cases. This may affect the way some users run job steps with srun inside of interactive jobs (srun --pty $SHELL), so we wanted to provide some guidance as we work on updating our documentation to reflect this change. When running an interactive job with srun --pty $SHELL and then launching job steps on a node, a second srun is often used \"inside\" the first srun to launch certain software. For example, for users of Paraview, a Paraview server may be launched on an interactive node with \"srun -n 8 pvserver --force-offscreen-rendering\". (Certain GPU-enabled or MPI-enabled interactive software also functions in a similar manner.) This \"srun-inside-an-srun\" process will no longer function in the same way as in the past. Instead, the \"outer\" srun should be replaced with an salloc command. Salloc will accept the same arguments as srun, but \"--pty $SHELL\" will no longer be required. Salloc will automatically open a shell to the node once the job starts, and the \"inner\" srun can then be run successfully as normal. Other regular uses of srun and srun inside sbatch scripts should continue to behave as expected. For further technical details on this Slurm change, please see the Slurm 20.11 Release Notes regarding job steps, srun, and the new --overlap flag. Upcoming Outage There is a whole campus power outage planned for April 1 for NREL's South Table Mesa (STM) campus. All Computational Science Center systems will be affected. More details will follow as the date approaches.","title":"February 2022 Monthly Update"},{"location":"Announcements/2022-02-02-announcement/#changes-to-slurm-srun-for-interactive-jobs","text":"During the recent system time the Slurm job scheduler was upgraded. One of the side effects of this was a change in the way Slurm handles job steps internally in certain cases. This may affect the way some users run job steps with srun inside of interactive jobs (srun --pty $SHELL), so we wanted to provide some guidance as we work on updating our documentation to reflect this change. When running an interactive job with srun --pty $SHELL and then launching job steps on a node, a second srun is often used \"inside\" the first srun to launch certain software. For example, for users of Paraview, a Paraview server may be launched on an interactive node with \"srun -n 8 pvserver --force-offscreen-rendering\". (Certain GPU-enabled or MPI-enabled interactive software also functions in a similar manner.) This \"srun-inside-an-srun\" process will no longer function in the same way as in the past. Instead, the \"outer\" srun should be replaced with an salloc command. Salloc will accept the same arguments as srun, but \"--pty $SHELL\" will no longer be required. Salloc will automatically open a shell to the node once the job starts, and the \"inner\" srun can then be run successfully as normal. Other regular uses of srun and srun inside sbatch scripts should continue to behave as expected. For further technical details on this Slurm change, please see the Slurm 20.11 Release Notes regarding job steps, srun, and the new --overlap flag.","title":"Changes to Slurm \"srun\" for Interactive Jobs"},{"location":"Announcements/2022-02-02-announcement/#upcoming-outage","text":"There is a whole campus power outage planned for April 1 for NREL's South Table Mesa (STM) campus. All Computational Science Center systems will be affected. More details will follow as the date approaches.","title":"Upcoming Outage"},{"location":"Announcements/2022-03-02-announcement/","text":"March HPC Systems Power Outage Eagle, Swift, Vermillion, Meridian, and all other related HPC systems, services, and filesystems will be unavailable beginning on Thursday, March 31st, 2022 due to scheduled facilities maintenance. The Eagle Operations Team will also be performing firmware updates, security patches, and updates to the GPU node images during this time. The outage is anticipated to start at 7:00am on Thursday, March 31st and last at least through Friday, April 1st, 2022, but it may extend through to the following Monday, April 4th, 2022. We will provide updates as more information about the full outage period becomes available. Standby QOS now Available on Eagle & Swift Through the annual user survey and direct feedback, we have received requests to submit jobs to the standby queue on demand. We are now pleased to announce the capability to submit jobs to the standby Quality of Service (QoS) queue by using the --qos=standby flag. Please remember that standby jobs run with a very low priority, so these jobs may wait for a considerable period of time. However, the job will not incur any AU charges against your project's allocation. Eagle local I/O performance We\u2019ve received questions recently about disk types and performance on compute nodes. Eagle has two network file systems. Qumulo provides /home and /nopt. It is NFS and is not considered a performance file system. /home has snapshots for restoration of lost data, but should not be used as a replacement for a source code repository like Git. Lustre is our performance file system and provides /scratch, /projects, /shared-projects and /datasets. Eagle also has two options on nodes. /dev/shm, which is an in-memory space, which is fast but you need to balance its usage with your jobs memory usage. /tmp/scratch is physical storage. The type of storage and performance differ depending on node and that\u2019s what we hope to clarify. If we look under Compute Node Hardware Details on the central NREL HPC website , there are nodes listed as having SATA drives, and nodes listed as having SSDs. Our SATA drives are still spinning disks, while SAS (serial attached SCSI) is how the SSD\u2019s are connected to the node. We would generally expect the nodes with SSDs to perform better. Let\u2019s test that out with a simple test. This is a command we regularly use to verify Lustre OST (object storage target) performance. It\u2019s designed to write enough information so that you are seeing disk performance, and not just the performance of the storage controller of the disk: dd if=/dev/zero of=X bs=1M count=10k This is writing in file in chunks of 1M, 10k times, to X. It writes an 11GB file: Lustre: 1.6 GB/s per OST Node /dev/shm: 2.8 GB/s Node SATA spinning disk: 2.4 GB/s Node SAS SSD: 2.4 GB/s Surprising! There is not a difference between the two local disks. Let\u2019s do the same test, but instead of writing in 1M chunks, we will write in 10M chunks which will write a 107GB file. For this case, Lustre and /dev/shm maintain performance, but here\u2019s what we get for the two local disk types: Node SATA spinning disk: 146 MB/s Node SAS SSD: 1.9 GB/s That is a rather drastic drop off in performance for the SATA disk. So how your data writes to disk can drastically affect performance. A lot of tiny files will look the same between the two disk types, one large continuous write would differ.","title":"March 2022 Monthly Update"},{"location":"Announcements/2022-03-02-announcement/#march-hpc-systems-power-outage","text":"Eagle, Swift, Vermillion, Meridian, and all other related HPC systems, services, and filesystems will be unavailable beginning on Thursday, March 31st, 2022 due to scheduled facilities maintenance. The Eagle Operations Team will also be performing firmware updates, security patches, and updates to the GPU node images during this time. The outage is anticipated to start at 7:00am on Thursday, March 31st and last at least through Friday, April 1st, 2022, but it may extend through to the following Monday, April 4th, 2022. We will provide updates as more information about the full outage period becomes available.","title":"March HPC Systems Power Outage"},{"location":"Announcements/2022-03-02-announcement/#standby-qos-now-available-on-eagle-swift","text":"Through the annual user survey and direct feedback, we have received requests to submit jobs to the standby queue on demand. We are now pleased to announce the capability to submit jobs to the standby Quality of Service (QoS) queue by using the --qos=standby flag. Please remember that standby jobs run with a very low priority, so these jobs may wait for a considerable period of time. However, the job will not incur any AU charges against your project's allocation.","title":"Standby QOS now Available on Eagle &amp; Swift"},{"location":"Announcements/2022-03-02-announcement/#eagle-local-io-performance","text":"We\u2019ve received questions recently about disk types and performance on compute nodes. Eagle has two network file systems. Qumulo provides /home and /nopt. It is NFS and is not considered a performance file system. /home has snapshots for restoration of lost data, but should not be used as a replacement for a source code repository like Git. Lustre is our performance file system and provides /scratch, /projects, /shared-projects and /datasets. Eagle also has two options on nodes. /dev/shm, which is an in-memory space, which is fast but you need to balance its usage with your jobs memory usage. /tmp/scratch is physical storage. The type of storage and performance differ depending on node and that\u2019s what we hope to clarify. If we look under Compute Node Hardware Details on the central NREL HPC website , there are nodes listed as having SATA drives, and nodes listed as having SSDs. Our SATA drives are still spinning disks, while SAS (serial attached SCSI) is how the SSD\u2019s are connected to the node. We would generally expect the nodes with SSDs to perform better. Let\u2019s test that out with a simple test. This is a command we regularly use to verify Lustre OST (object storage target) performance. It\u2019s designed to write enough information so that you are seeing disk performance, and not just the performance of the storage controller of the disk: dd if=/dev/zero of=X bs=1M count=10k This is writing in file in chunks of 1M, 10k times, to X. It writes an 11GB file: Lustre: 1.6 GB/s per OST Node /dev/shm: 2.8 GB/s Node SATA spinning disk: 2.4 GB/s Node SAS SSD: 2.4 GB/s Surprising! There is not a difference between the two local disks. Let\u2019s do the same test, but instead of writing in 1M chunks, we will write in 10M chunks which will write a 107GB file. For this case, Lustre and /dev/shm maintain performance, but here\u2019s what we get for the two local disk types: Node SATA spinning disk: 146 MB/s Node SAS SSD: 1.9 GB/s That is a rather drastic drop off in performance for the SATA disk. So how your data writes to disk can drastically affect performance. A lot of tiny files will look the same between the two disk types, one large continuous write would differ.","title":"Eagle local I/O performance"},{"location":"Announcements/2022-04-06-announcement/","text":"FY23 HPC Allocation Process The Eagle allocation process for FY23 is scheduled to open up on May 11, with applications due June 8. The application process will be an update of the process used in FY23, with additional information requested to help manage the transition from Eagle to Kestrel. HPC Operations will host a webinar on May 17 to explain the application process. Watch for announcements. Documentation We would like to announce our user-contributed documentation repository and website for Eagle and other NREL HPC systems that is open to both NREL and non-NREL users. This repository serves as a collection of code examples, executables, and utilities to benefit the NREL HPC community. It also hosts a site that provides more verbose documentation and examples. If you would like to contribute or recommend a topic to be covered please open an issue or a pull request in the repository. Our contribution guidelines offer more detailed instructions on how to add content to the pages. Eagle login node etiquette Eagle logins are shared resources that are heavily utilized. We have some controls in place to limit per user process use of memory and CPU that will ramp down your processes usage over time. We recommend any sustained heavy usage of memory and CPU take place on compute nodes, where these limits aren't in place. If you only need a node for an hour, nodes in the debug partition are available. We permit compiles and file operations on the logins, but discourage multi-threaded operations or long, sustained operations against the file system. We cannot put the same limits on file system operations as memory and CPU, therefore if you slow the file system on the login node, you slow it for everyone on that login. Lastly, Fastx, the remote windowing package on the ED nodes, is a licensed product. When you are done using FastX, please log all the way out to ensure licenses are available for all users. CSC Tutorials Team - External Users Staff in the Computational Science Center host multiple tutorials and workshops on various computational science topics throughout the year, such as Visualization, Cloud, HPC, and others. In Microsoft Teams, a \u201c Computational Sciences Tutorials \u201d public team was just created to be the hub for all such tutorials and workshops. As an external user, you will be able to view discussion board posts, resource files, our SharePoint Calendar, and lists of the upcoming schedule and related repo links. Unfortunately, you will not be able to access recordings or the survey. The SharePoint Calendar provides a month view for upcoming tutorials, their descriptions, and links to join. If you miss the monthly announcements in our newsletters, you can access calendar events and find meeting information to join the tutorials in the Teams channel. The Upcoming Schedule provides a list view of the upcoming events and their tentative dates. For external users, you will receive an invite from the team. Should you decide to join the public team, there a few steps you will need to take. First, you need go through the steps to register a free Office365 account (or login if you already have an account). Next, you will need to download Microsoft Authenticator or another authenticator application. The process is straightforward, and you will be prompted during each step of the process. If you do not accept the invite or do not wish to go through the process of joining the public team, you can rely on the monthly newsletters or visit the Training Page on https://hpc.nrel.gov for meeting information. Instructions: You will receive a welcome email from the team owner (sometime this week), with information about the team. Click on accept. If you have never created a MS Office 365 account, you will prompted to create one. If you already have a MS Office 365 account, login. The first time you log in, you will be prompted to set up Microsoft Authenticator or other authenticator app. From your mobile device, Download and install the app from the Apple Store (for iOS) or the Google Play Store (for Android) and Open the app. On your mobile device, you will be prompted to allow notifications. Select Allow. On your mobile device, click OK on the screen for what information Microsoft gathers. Click Skip on the \"Add personal account\" page. Click Skip on the \"Add non-Microsoft account\" page. Click Add Work Account on the \"Add work account\" page. Click OK to allow access to the camera. Going forward, anytime you login, you will get a prompt on your phone to authenticate.","title":"April 2022 Monthly Update"},{"location":"Announcements/2022-04-06-announcement/#fy23-hpc-allocation-process","text":"The Eagle allocation process for FY23 is scheduled to open up on May 11, with applications due June 8. The application process will be an update of the process used in FY23, with additional information requested to help manage the transition from Eagle to Kestrel. HPC Operations will host a webinar on May 17 to explain the application process. Watch for announcements.","title":"FY23 HPC Allocation Process"},{"location":"Announcements/2022-04-06-announcement/#documentation","text":"We would like to announce our user-contributed documentation repository and website for Eagle and other NREL HPC systems that is open to both NREL and non-NREL users. This repository serves as a collection of code examples, executables, and utilities to benefit the NREL HPC community. It also hosts a site that provides more verbose documentation and examples. If you would like to contribute or recommend a topic to be covered please open an issue or a pull request in the repository. Our contribution guidelines offer more detailed instructions on how to add content to the pages.","title":"Documentation"},{"location":"Announcements/2022-04-06-announcement/#eagle-login-node-etiquette","text":"Eagle logins are shared resources that are heavily utilized. We have some controls in place to limit per user process use of memory and CPU that will ramp down your processes usage over time. We recommend any sustained heavy usage of memory and CPU take place on compute nodes, where these limits aren't in place. If you only need a node for an hour, nodes in the debug partition are available. We permit compiles and file operations on the logins, but discourage multi-threaded operations or long, sustained operations against the file system. We cannot put the same limits on file system operations as memory and CPU, therefore if you slow the file system on the login node, you slow it for everyone on that login. Lastly, Fastx, the remote windowing package on the ED nodes, is a licensed product. When you are done using FastX, please log all the way out to ensure licenses are available for all users.","title":"Eagle login node etiquette"},{"location":"Announcements/2022-04-06-announcement/#csc-tutorials-team-external-users","text":"Staff in the Computational Science Center host multiple tutorials and workshops on various computational science topics throughout the year, such as Visualization, Cloud, HPC, and others. In Microsoft Teams, a \u201c Computational Sciences Tutorials \u201d public team was just created to be the hub for all such tutorials and workshops. As an external user, you will be able to view discussion board posts, resource files, our SharePoint Calendar, and lists of the upcoming schedule and related repo links. Unfortunately, you will not be able to access recordings or the survey. The SharePoint Calendar provides a month view for upcoming tutorials, their descriptions, and links to join. If you miss the monthly announcements in our newsletters, you can access calendar events and find meeting information to join the tutorials in the Teams channel. The Upcoming Schedule provides a list view of the upcoming events and their tentative dates. For external users, you will receive an invite from the team. Should you decide to join the public team, there a few steps you will need to take. First, you need go through the steps to register a free Office365 account (or login if you already have an account). Next, you will need to download Microsoft Authenticator or another authenticator application. The process is straightforward, and you will be prompted during each step of the process. If you do not accept the invite or do not wish to go through the process of joining the public team, you can rely on the monthly newsletters or visit the Training Page on https://hpc.nrel.gov for meeting information.","title":"CSC Tutorials Team - External Users"},{"location":"Announcements/2022-04-06-announcement/#instructions","text":"You will receive a welcome email from the team owner (sometime this week), with information about the team. Click on accept. If you have never created a MS Office 365 account, you will prompted to create one. If you already have a MS Office 365 account, login. The first time you log in, you will be prompted to set up Microsoft Authenticator or other authenticator app. From your mobile device, Download and install the app from the Apple Store (for iOS) or the Google Play Store (for Android) and Open the app. On your mobile device, you will be prompted to allow notifications. Select Allow. On your mobile device, click OK on the screen for what information Microsoft gathers. Click Skip on the \"Add personal account\" page. Click Skip on the \"Add non-Microsoft account\" page. Click Add Work Account on the \"Add work account\" page. Click OK to allow access to the camera. Going forward, anytime you login, you will get a prompt on your phone to authenticate.","title":"Instructions:"},{"location":"Announcements/2022-05-04-announcement/","text":"FY23 HPC Allocation Process The Eagle allocation process for FY23 is scheduled to open up on May 11, with applications due June 8. The application process will be an update of the process used in FY22, with additional information requested to help manage the transition from Eagle to Kestrel. Be sure to sign-up for the webinar on May 17 from 11-12 a.m. MT. Michael Martin, Staff Scientist in the Computational Science Center at NREL, will be presenting on the allocation process, key dates, changes from last year, and how to submit a request. The registration site for the webinar is now available. NREL HPC Workshops - Intro to HPC Series (Save the Dates) NREL HPC Operations and Application Support teams will host an Intro to HPC workshop series this June every Wednesday from 12-1 p.m. Webinar information to follow. Topics/Schedule: Linux Fundamentals: Utilizing the Command Line Interface on June 1st 12-1 NREL HPC Systems on June 8th 12-1 Resource Management: Slurm on June 15th 12-1 Software Environments on June 22nd 12-1 JupyterHub on June 29th 12-1 Workaround for Windows SSH Users Some people who use Windows 10/11 computers to ssh to Eagle from a Windows command prompt, powershell, or via Visual Studio Code's SSH extension have received a new error message about a \"Corrupted MAC on input\" or \"message authentication code incorrect.\" This error is due to an outdated OpenSSL library included in Windows and a recent security-mandated change to ssh on Eagle. However, there is a functional workaround for this issue. (Note: If you are not experiencing the above error, you do not need and should not use the following workaround.) For command-line and Powershell ssh users, adding \"-m hmac-sha2-512\" to your ssh command will resolve the issue. For example: \"ssh -m hmac-sha2-512 @eagle.hpc.nrel.gov\" For VS Code SSH extension users, you will need to create an ssh config file on your local computer (~/.ssh/config), with a host entry for Eagle that specifies a new message authentication code: Host eagle HostName eagle.hpc.nrel.gov MACs hmac-sha2-512 The configuration file will also apply to command-line ssh in Windows, as well. This Visual Studio Blog post has further instructions on how to create the ssh configuration file for Windows and VS Code. Lustre Filesystem Usage Reminder The Lustre file systems that hosts /projects, /scratch, /shared-projects and /datasets works most efficiently when it is under 80% full. Please do your part to keep the file system under 80% by cleaning up your /projects, /scratch and /shared-projects spaces. Documentation We would like to announce our user-contributed documentation repository and website for Eagle and other NREL HPC systems that is open to both NREL and non-NREL users. This repository serves as a collection of code examples, executables, and utilities to benefit the NREL HPC community. It also hosts a site that provides more verbose documentation and examples. If you would like to contribute or recommend a topic to be covered please open an issue or a pull request in the repository. Our contribution guidelines offer more detailed instructions on how to add content to the pages.","title":"May 2022 Monthly Update"},{"location":"Announcements/2022-05-04-announcement/#fy23-hpc-allocation-process","text":"The Eagle allocation process for FY23 is scheduled to open up on May 11, with applications due June 8. The application process will be an update of the process used in FY22, with additional information requested to help manage the transition from Eagle to Kestrel. Be sure to sign-up for the webinar on May 17 from 11-12 a.m. MT. Michael Martin, Staff Scientist in the Computational Science Center at NREL, will be presenting on the allocation process, key dates, changes from last year, and how to submit a request. The registration site for the webinar is now available.","title":"FY23 HPC Allocation Process"},{"location":"Announcements/2022-05-04-announcement/#nrel-hpc-workshops-intro-to-hpc-series-save-the-dates","text":"NREL HPC Operations and Application Support teams will host an Intro to HPC workshop series this June every Wednesday from 12-1 p.m. Webinar information to follow. Topics/Schedule: Linux Fundamentals: Utilizing the Command Line Interface on June 1st 12-1 NREL HPC Systems on June 8th 12-1 Resource Management: Slurm on June 15th 12-1 Software Environments on June 22nd 12-1 JupyterHub on June 29th 12-1","title":"NREL HPC Workshops - Intro to HPC Series (Save the Dates)"},{"location":"Announcements/2022-05-04-announcement/#workaround-for-windows-ssh-users","text":"Some people who use Windows 10/11 computers to ssh to Eagle from a Windows command prompt, powershell, or via Visual Studio Code's SSH extension have received a new error message about a \"Corrupted MAC on input\" or \"message authentication code incorrect.\" This error is due to an outdated OpenSSL library included in Windows and a recent security-mandated change to ssh on Eagle. However, there is a functional workaround for this issue. (Note: If you are not experiencing the above error, you do not need and should not use the following workaround.) For command-line and Powershell ssh users, adding \"-m hmac-sha2-512\" to your ssh command will resolve the issue. For example: \"ssh -m hmac-sha2-512 @eagle.hpc.nrel.gov\" For VS Code SSH extension users, you will need to create an ssh config file on your local computer (~/.ssh/config), with a host entry for Eagle that specifies a new message authentication code: Host eagle HostName eagle.hpc.nrel.gov MACs hmac-sha2-512 The configuration file will also apply to command-line ssh in Windows, as well. This Visual Studio Blog post has further instructions on how to create the ssh configuration file for Windows and VS Code.","title":"Workaround for Windows SSH Users"},{"location":"Announcements/2022-05-04-announcement/#lustre-filesystem-usage-reminder","text":"The Lustre file systems that hosts /projects, /scratch, /shared-projects and /datasets works most efficiently when it is under 80% full. Please do your part to keep the file system under 80% by cleaning up your /projects, /scratch and /shared-projects spaces.","title":"Lustre Filesystem Usage Reminder"},{"location":"Announcements/2022-05-04-announcement/#documentation","text":"We would like to announce our user-contributed documentation repository and website for Eagle and other NREL HPC systems that is open to both NREL and non-NREL users. This repository serves as a collection of code examples, executables, and utilities to benefit the NREL HPC community. It also hosts a site that provides more verbose documentation and examples. If you would like to contribute or recommend a topic to be covered please open an issue or a pull request in the repository. Our contribution guidelines offer more detailed instructions on how to add content to the pages.","title":"Documentation"},{"location":"Announcements/2022-08-11-announcement/","text":"Annual Reports due August 31 FY22 HPC annual reports are due by August 31, 2022. If you have received an email prompting you to submit an annual report for any of your FY22 projects please follow the instructions provided. If you have any questions please contact hpc-reporting@nrel.gov . Notification of Full NSRDB Data Update Over the past few years, the NSRDB team has been hard at work improving their solar data product. There have been numerous improvements in software, data quality, cloud property algorithms, solar position algorithms, and surface albedo data. The current data and software version is v3.2.2, and you can see the NSRDB version history for more details. For the new and improved NSRDB data to proliferate, the NSRDB team will be manipulating the /datasets/NSRDB/ directory on Eagle and will eventually deprecate the data in the /datasets/NSRDB/v3/ directory. Here is an outline of the changes that will be made along with an estimated timeline: Before 8/13, the new NSRDB data will be copied to the /datasets/NSRDB/current/ directory (including data for 2021!) On 8/20, the old NSRDB data in /datasets/NSRDB/v3/ will be moved to /datasets/NSRDB/deprecated_v3/ On 9/3, the data in /datasets/NSRDB/conus/ and /datasets/NSRDB/full_disc/ will be replaced with the new data (this high-res data is too big to keep two copies). On 10/1, the old NSRDB data in /datasets/NSRDB/deprecated_v3/ will be removed permanently. You should be aware of one significant change if you have hard-coded your site location index values into any code: the NSRDB team is updating the meta data in the standard 4km 30min NSRDB product. The old meta data had several errors and inconsistencies which will be fixed in the new meta. A mapping of site index values from the \u201cv3\u201d meta to the new meta can be found here and is also copied at /datasets/NSRDB/nsrdb_v3_to_current_map.csv on Eagle. Minor differences in the data should be expected, but please reach out to Grant Buster and Manajit Sengupta with anything you see that looks like a true error. Thanks for your cooperation and thanks for using the NSRDB!","title":"August 2022 Monthly Update"},{"location":"Announcements/2022-08-11-announcement/#annual-reports-due-august-31","text":"FY22 HPC annual reports are due by August 31, 2022. If you have received an email prompting you to submit an annual report for any of your FY22 projects please follow the instructions provided. If you have any questions please contact hpc-reporting@nrel.gov .","title":"Annual Reports due August 31"},{"location":"Announcements/2022-08-11-announcement/#notification-of-full-nsrdb-data-update","text":"Over the past few years, the NSRDB team has been hard at work improving their solar data product. There have been numerous improvements in software, data quality, cloud property algorithms, solar position algorithms, and surface albedo data. The current data and software version is v3.2.2, and you can see the NSRDB version history for more details. For the new and improved NSRDB data to proliferate, the NSRDB team will be manipulating the /datasets/NSRDB/ directory on Eagle and will eventually deprecate the data in the /datasets/NSRDB/v3/ directory. Here is an outline of the changes that will be made along with an estimated timeline: Before 8/13, the new NSRDB data will be copied to the /datasets/NSRDB/current/ directory (including data for 2021!) On 8/20, the old NSRDB data in /datasets/NSRDB/v3/ will be moved to /datasets/NSRDB/deprecated_v3/ On 9/3, the data in /datasets/NSRDB/conus/ and /datasets/NSRDB/full_disc/ will be replaced with the new data (this high-res data is too big to keep two copies). On 10/1, the old NSRDB data in /datasets/NSRDB/deprecated_v3/ will be removed permanently. You should be aware of one significant change if you have hard-coded your site location index values into any code: the NSRDB team is updating the meta data in the standard 4km 30min NSRDB product. The old meta data had several errors and inconsistencies which will be fixed in the new meta. A mapping of site index values from the \u201cv3\u201d meta to the new meta can be found here and is also copied at /datasets/NSRDB/nsrdb_v3_to_current_map.csv on Eagle. Minor differences in the data should be expected, but please reach out to Grant Buster and Manajit Sengupta with anything you see that looks like a true error. Thanks for your cooperation and thanks for using the NSRDB!","title":"Notification of Full NSRDB Data Update"},{"location":"Documentation/","text":"Welcome to the central source of user-contributed documentation for Eagle and other NREL HPC systems. This repository is open to both NREL and non-NREL HPC users. You can browse the documentation here, or start contributing by visiting the repository in Git for more information. Where to Begin Please use the navigation bar on the left to explore the available documentation by category. Highlights Systems Guide to learn about our HPC systems Jupyterhub to get started with Jupyter Notebooks Conda environment howto and Eagle-specific information Other NREL Documentation Resources The NREL HPC Website is the home of Advanced Computing at NREL Our Github Repository for specific application examples, scripts, workshop content, the contributor guide, and more. The gh-pages branch (this site) is also open for contribution.","title":"Documentation Home"},{"location":"Documentation/#where-to-begin","text":"Please use the navigation bar on the left to explore the available documentation by category.","title":"Where to Begin"},{"location":"Documentation/#highlights","text":"Systems Guide to learn about our HPC systems Jupyterhub to get started with Jupyter Notebooks Conda environment howto and Eagle-specific information","title":"Highlights"},{"location":"Documentation/#other-nrel-documentation-resources","text":"The NREL HPC Website is the home of Advanced Computing at NREL Our Github Repository for specific application examples, scripts, workshop content, the contributor guide, and more. The gh-pages branch (this site) is also open for contribution.","title":"Other NREL Documentation Resources"},{"location":"Documentation/Applications/lammps/","text":"","title":"Lammps"},{"location":"Documentation/Applications/vasp/","text":"On Eagle Load VASP with Intel MPI: ml vasp script to run VASP on Eagle with Intel MPI Load VASP with Open MPI: source /nopt/nrel/apps/210830a/myenv.2108301742 ml vasp/6.1.1-l2mkbb2 script to run VASP on Eagle with Open MPI Load the GPU build of VASP: module use /nopt/nrel/apps/220511a/modules/lmod/linux-centos7-x86_64/gcc/12.1.0 ml fftw nvhpc export LD_LIBRARY_PATH=/nopt/nrel/apps/220511a/install/opt/spack/linux-centos7-skylake_avx512/gcc-12.1.0/nvhpc-22.3-c4qk6fly5hls3mjimoxg6vyuy5cc3vti/Linux_x86_64/22.3/compilers/extras/qd/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/nopt/nrel/apps/220511a/install/opt/spack/linux-centos7-skylake_avx512/gcc-12.1.0/nvhpc-22.3-c4qk6fly5hls3mjimoxg6vyuy5cc3vti/Linux_x86_64/22.3/compilers/extras/qd/lib:$LD_LIBRARY_PATH export PATH=/projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc:$PATH script to run VASP on Eagle on GPU nodes On Swift Load VASP with Intel MPI: ml vaspintel ml slurm/21-08-1-1-o2xw5ti ml gcc/9.4.0-v7mri5d ml intel-oneapi-compilers/2021.3.0-piz2usr ml intel-oneapi-mpi/2021.3.0-hcp2lkf ml intel-oneapi-mkl/2021.3.0-giz47h4 script to run VASP on Swift with Intel MPI Load VASP with Open MPI: ml vasp ml slurm/21-08-1-1-o2xw5ti ml openmpi/4.1.1-6vr2flz script to run VASP on Swift with Open MPI","title":"Vasp"},{"location":"Documentation/Applications/vasp/#on-eagle","text":"Load VASP with Intel MPI: ml vasp script to run VASP on Eagle with Intel MPI Load VASP with Open MPI: source /nopt/nrel/apps/210830a/myenv.2108301742 ml vasp/6.1.1-l2mkbb2 script to run VASP on Eagle with Open MPI Load the GPU build of VASP: module use /nopt/nrel/apps/220511a/modules/lmod/linux-centos7-x86_64/gcc/12.1.0 ml fftw nvhpc export LD_LIBRARY_PATH=/nopt/nrel/apps/220511a/install/opt/spack/linux-centos7-skylake_avx512/gcc-12.1.0/nvhpc-22.3-c4qk6fly5hls3mjimoxg6vyuy5cc3vti/Linux_x86_64/22.3/compilers/extras/qd/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/nopt/nrel/apps/220511a/install/opt/spack/linux-centos7-skylake_avx512/gcc-12.1.0/nvhpc-22.3-c4qk6fly5hls3mjimoxg6vyuy5cc3vti/Linux_x86_64/22.3/compilers/extras/qd/lib:$LD_LIBRARY_PATH export PATH=/projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc:$PATH script to run VASP on Eagle on GPU nodes","title":"On Eagle"},{"location":"Documentation/Applications/vasp/#on-swift","text":"Load VASP with Intel MPI: ml vaspintel ml slurm/21-08-1-1-o2xw5ti ml gcc/9.4.0-v7mri5d ml intel-oneapi-compilers/2021.3.0-piz2usr ml intel-oneapi-mpi/2021.3.0-hcp2lkf ml intel-oneapi-mkl/2021.3.0-giz47h4 script to run VASP on Swift with Intel MPI Load VASP with Open MPI: ml vasp ml slurm/21-08-1-1-o2xw5ti ml openmpi/4.1.1-6vr2flz script to run VASP on Swift with Open MPI","title":"On Swift"},{"location":"Documentation/Data-and-File-Systems/File-Systems/","text":"File systems Eagle has three primary file systems available for compute nodes. Understanding the usage of these is important for achieving the best performance. NREL file systems Home file system Quota of 50 GB Used to hold scripts, source code, executables Lustre parallel file system : Accessiblle across all nodes. When using this file system please familiarize yourself with the best practices section /scratch/username /projects /shared-projects /datasets Node file system : The local drive on each node, these are accessible only on a given node. /tmp/scratch 1 TB HDD (spinning disk, average performance) on compute nodes with 196GB or less RAM 1.6 TB SSD (higher performance) on 78 bigmem/GPU nodes 25.6 TB SSD (higher performance, maximum local storage) on 20 bigmem/GPU \"bigscratch\" nodes For more information on the file systems available on Eagle please see: Eagle System Configuration","title":"File Systems"},{"location":"Documentation/Data-and-File-Systems/File-Systems/#file-systems","text":"Eagle has three primary file systems available for compute nodes. Understanding the usage of these is important for achieving the best performance.","title":"File systems"},{"location":"Documentation/Data-and-File-Systems/File-Systems/#nrel-file-systems","text":"Home file system Quota of 50 GB Used to hold scripts, source code, executables Lustre parallel file system : Accessiblle across all nodes. When using this file system please familiarize yourself with the best practices section /scratch/username /projects /shared-projects /datasets Node file system : The local drive on each node, these are accessible only on a given node. /tmp/scratch 1 TB HDD (spinning disk, average performance) on compute nodes with 196GB or less RAM 1.6 TB SSD (higher performance) on 78 bigmem/GPU nodes 25.6 TB SSD (higher performance, maximum local storage) on 20 bigmem/GPU \"bigscratch\" nodes For more information on the file systems available on Eagle please see: Eagle System Configuration","title":"NREL file systems"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/","text":"Lustre best practices In some cases special care must be taken while using Lustre so as not to affect the performance of the filesystem for yourself and other users. The below Do's and Don'ts are provided as guidance. Do Use the lfs find e.g. lfs find /scratch/username -type f -name \"*.py\" Break up directories with many files into more directories if possible Store small files and directories of small files on a single OST (Object Storage Target) Limit the number of processes accessing a file. It may be better to read in a file once and then broadcast necessary information to other processes Change your stripecount based on the filesize Write many files to the node filesystem /tmp/scratch/ : this is local storage on each node, and is not a part of the Lustre filesystem. Once your work is complete, the files can then be added to a tar archive and transferred to the /project/project_name for later use, or deleted from /tmp/scratch if no longer needed Store data and run executables from /projects Storing your conda environments in /projects can ensure that your data and executables are on the same filesystem, improving performance Do Not Use ls -l Have a file accessed by multiple processes In Python, avoid using os.walk or os.scandir List files instead of using wildcards e.g. don't use cp * dir/ If you need to tar/rm/cp a large number of files use xargs or similar: lfs find /scratch/username/old_data/ -t f -print0 | xargs -0 rm Have many small files in a single directory Store important files in /scratch e.g. don't keep data, libraries or programs in /scratch/username , as /scratch directories are subject to automated purging based on the Data Retention Policy Useful Lustre commands Check your storage usage: lfs quota -h -u <username> /scratch See which MDT a directory is located on lfs getstripe --mdt-index /scratch/<username> This will return an index 0-2 indicating the MDT Create a folder on a specific MDT (admin only) lfs mkdir \u2013i <mdt_index> /dir_path Striping Lustre provides a way to stripe files, this spreads them across multiple OSTs. Striping a large file being accessed by many processes can greatly improve the performace. See Lustre file striping for more details. lfs setstripe <file> -c <count> -s <size> * The stripecount determines how many OST the data is spread across * The stripe size is how large each of the stripes are in KB, MB, GB References Lustre manual CU Boulder - Lustre Do's and Don'ts NASA - Lustre Best Practices NASA - Lustre basics UMBC - Lustre Best Practices NICS - I/O and Lustre Usage NERSC - Lustre","title":"Lustre Best Practices"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#lustre-best-practices","text":"In some cases special care must be taken while using Lustre so as not to affect the performance of the filesystem for yourself and other users. The below Do's and Don'ts are provided as guidance. Do Use the lfs find e.g. lfs find /scratch/username -type f -name \"*.py\" Break up directories with many files into more directories if possible Store small files and directories of small files on a single OST (Object Storage Target) Limit the number of processes accessing a file. It may be better to read in a file once and then broadcast necessary information to other processes Change your stripecount based on the filesize Write many files to the node filesystem /tmp/scratch/ : this is local storage on each node, and is not a part of the Lustre filesystem. Once your work is complete, the files can then be added to a tar archive and transferred to the /project/project_name for later use, or deleted from /tmp/scratch if no longer needed Store data and run executables from /projects Storing your conda environments in /projects can ensure that your data and executables are on the same filesystem, improving performance Do Not Use ls -l Have a file accessed by multiple processes In Python, avoid using os.walk or os.scandir List files instead of using wildcards e.g. don't use cp * dir/ If you need to tar/rm/cp a large number of files use xargs or similar: lfs find /scratch/username/old_data/ -t f -print0 | xargs -0 rm Have many small files in a single directory Store important files in /scratch e.g. don't keep data, libraries or programs in /scratch/username , as /scratch directories are subject to automated purging based on the Data Retention Policy","title":"Lustre best practices"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#useful-lustre-commands","text":"Check your storage usage: lfs quota -h -u <username> /scratch See which MDT a directory is located on lfs getstripe --mdt-index /scratch/<username> This will return an index 0-2 indicating the MDT Create a folder on a specific MDT (admin only) lfs mkdir \u2013i <mdt_index> /dir_path","title":"Useful Lustre commands"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#striping","text":"Lustre provides a way to stripe files, this spreads them across multiple OSTs. Striping a large file being accessed by many processes can greatly improve the performace. See Lustre file striping for more details. lfs setstripe <file> -c <count> -s <size> * The stripecount determines how many OST the data is spread across * The stripe size is how large each of the stripes are in KB, MB, GB","title":"Striping"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#references","text":"Lustre manual CU Boulder - Lustre Do's and Don'ts NASA - Lustre Best Practices NASA - Lustre basics UMBC - Lustre Best Practices NICS - I/O and Lustre Usage NERSC - Lustre","title":"References"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/","text":"Transferring files using FileZilla FileZilla can be used to securely transfer files between your local computer running Windows, Linux or MacOS to a remote computer running Linux. Setting Up FileZilla Download and install FileZilla . Connecting to a Host Decide which host you wish to connect to such as, eagle.hpc.nrel.gov Enter your username in the Username field. Enter your password or Password+OTP Token in the Password field. Use 22 as the Port. Click the 'Quickconnect' button. Transferring Files You may use FileZilla to transfer individual files or directories from the Local Directory to the Remote Directory or vice versa. Transfer files by dragging them from the Local Directory (left pane) to the Remote Directory (right pane) or vice versa. Once the transfer is complete the selected file will be visible in the pane it was transferred to.","title":"FileZilla"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/#transferring-files-using-filezilla","text":"FileZilla can be used to securely transfer files between your local computer running Windows, Linux or MacOS to a remote computer running Linux.","title":"Transferring files using FileZilla"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/#setting-up-filezilla","text":"Download and install FileZilla .","title":"Setting Up FileZilla"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/#connecting-to-a-host","text":"Decide which host you wish to connect to such as, eagle.hpc.nrel.gov Enter your username in the Username field. Enter your password or Password+OTP Token in the Password field. Use 22 as the Port. Click the 'Quickconnect' button.","title":"Connecting to a Host"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/#transferring-files","text":"You may use FileZilla to transfer individual files or directories from the Local Directory to the Remote Directory or vice versa. Transfer files by dragging them from the Local Directory (left pane) to the Remote Directory (right pane) or vice versa. Once the transfer is complete the selected file will be visible in the pane it was transferred to.","title":"Transferring Files"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/","text":"Transferring files Learn how to transfer data within, to and from NREL's high-performance computing (HPC) systems. A supported set of instructions for data transfer using NREL HPC systems is provided on the HPC NREL Website . Checking Usage and Quota The below command is used to check your quota from an Eagle login node. hours_report will display your usage and quota for each filesystem. $ hours_report Best Practices for Transferring Files File Transfers Between Filesystems on the NREL network rsync is the recommended tool for transferring data between NREL systems. It allows you to easily restart transfers if they fail, and also provides more consistency when dealing with symbolic links, hard links, and sparse files than either scp or cp. It is recommended you do not use compression for transfers within NREL systems. An example command is: $ rsync -aP --no-g /scratch/username/dataset1/ /mss/users/username/dataset1/ Mass Storage has quotas that limit the number of individual files you can store. If you are copying hundreds of thousands of files then it is best to archive these files prior to copying to Mass Storage. See the guide on how to archive files . Mass Storage quotas rely on the group of the file and not the directory path. It is best to use the --no-g option when rsyncing to MSS so you use the destination group rather than the group permissions of your source. You can also chgrp your files to the appropriate group prior to rsyncing to MSS. Small Transfers (<100GB) outside of the NREL network rsync , scp , and curl will be your best option for small transfers (<100GB) outside of the NREL network. If your rsync/scp/curl transfers are taking hours to complete then you should consider using Globus . If you're transferring many files then you should use rsync: $ rsync -azP --no-g /mss/users/username/dataset1/ user@desthost:/home/username/dataset1/ If you're transferring an individual file then use scp: $ scp /home/username/example.tar.gz user@desthost:/home/username/ You can use curl or wget to download individual files: $ curl -O https://URL $ wget https://URL Large Transfers (>100GB) outside of the NREL network Globus is optimized for file transfers between data centers and anything outside of the NREL network. It will be several times faster than any other tools you will have available. Documentation about requesting a HPC Globus account is available on the Globus Services page on the HPC website . See Transfering files using Globus for instructions on transfering files with Globus. Transfering files using Windows For Windows you will need to download WinSCP to transfer files to and from HPC systems over SCP. See Transfering using WinSCP . Archiving files and directories Learn various techniques to combine and compress multiple files or directories into a single file to reduce storage footprint or simplify sharing. tar tar , along with zip , is one of the basic commands to combine multiple individual files into a single file (called a \"tarball\"). tar requires at least one command line option. A typical usage would be: $ tar -cf newArchiveName.tar file1 file2 file3 # or $ tar -cf newArchiveName.tar /path/to/folder/ The -c flag denotes c reating an archive, and -f denotes that the next argument given will be the archive name\u2014in this case it means the name you would prefer for the resulting archive file. To extract files from a tar, it's recommended to use: $ tar -xvf existingArchiveName.tar -x is for ex tracting, -v uses v erbose mode which will print the name of each file as it is extracted from the archive. Compressing tar can also generate compressed tarballs which reduce the size of the resulting archive. This can be done with the -z flag (which just calls gzip on the resulting archive automatically, resulting in a .tar.gz extension) or -j (which uses bzip2 , creating a .tar.bz2 ). For example: # gzip $ tar -czvf newArchive.tar.gz file1 file2 file3 $ tar -xvzf newArchive.tar.gz # bzip2 $ tar -czjf newArchive.tar.bz2 file1 file2 file3 $ tar -xvjf newArchive.tar.bz2","title":"File Transfers"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#transferring-files","text":"Learn how to transfer data within, to and from NREL's high-performance computing (HPC) systems. A supported set of instructions for data transfer using NREL HPC systems is provided on the HPC NREL Website .","title":"Transferring files"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#checking-usage-and-quota","text":"The below command is used to check your quota from an Eagle login node. hours_report will display your usage and quota for each filesystem. $ hours_report","title":"Checking Usage and Quota"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#best-practices-for-transferring-files","text":"","title":"Best Practices for Transferring Files"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#file-transfers-between-filesystems-on-the-nrel-network","text":"rsync is the recommended tool for transferring data between NREL systems. It allows you to easily restart transfers if they fail, and also provides more consistency when dealing with symbolic links, hard links, and sparse files than either scp or cp. It is recommended you do not use compression for transfers within NREL systems. An example command is: $ rsync -aP --no-g /scratch/username/dataset1/ /mss/users/username/dataset1/ Mass Storage has quotas that limit the number of individual files you can store. If you are copying hundreds of thousands of files then it is best to archive these files prior to copying to Mass Storage. See the guide on how to archive files . Mass Storage quotas rely on the group of the file and not the directory path. It is best to use the --no-g option when rsyncing to MSS so you use the destination group rather than the group permissions of your source. You can also chgrp your files to the appropriate group prior to rsyncing to MSS.","title":"File Transfers Between Filesystems on the NREL network"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#small-transfers-100gb-outside-of-the-nrel-network","text":"rsync , scp , and curl will be your best option for small transfers (<100GB) outside of the NREL network. If your rsync/scp/curl transfers are taking hours to complete then you should consider using Globus . If you're transferring many files then you should use rsync: $ rsync -azP --no-g /mss/users/username/dataset1/ user@desthost:/home/username/dataset1/ If you're transferring an individual file then use scp: $ scp /home/username/example.tar.gz user@desthost:/home/username/ You can use curl or wget to download individual files: $ curl -O https://URL $ wget https://URL","title":"Small Transfers (&lt;100GB) outside of the NREL network"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#large-transfers-100gb-outside-of-the-nrel-network","text":"Globus is optimized for file transfers between data centers and anything outside of the NREL network. It will be several times faster than any other tools you will have available. Documentation about requesting a HPC Globus account is available on the Globus Services page on the HPC website . See Transfering files using Globus for instructions on transfering files with Globus.","title":"Large Transfers (&gt;100GB) outside of the NREL network"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#transfering-files-using-windows","text":"For Windows you will need to download WinSCP to transfer files to and from HPC systems over SCP. See Transfering using WinSCP .","title":"Transfering files using Windows"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#archiving-files-and-directories","text":"Learn various techniques to combine and compress multiple files or directories into a single file to reduce storage footprint or simplify sharing.","title":"Archiving files and directories"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#tar","text":"tar , along with zip , is one of the basic commands to combine multiple individual files into a single file (called a \"tarball\"). tar requires at least one command line option. A typical usage would be: $ tar -cf newArchiveName.tar file1 file2 file3 # or $ tar -cf newArchiveName.tar /path/to/folder/ The -c flag denotes c reating an archive, and -f denotes that the next argument given will be the archive name\u2014in this case it means the name you would prefer for the resulting archive file. To extract files from a tar, it's recommended to use: $ tar -xvf existingArchiveName.tar -x is for ex tracting, -v uses v erbose mode which will print the name of each file as it is extracted from the archive.","title":"tar"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#compressing","text":"tar can also generate compressed tarballs which reduce the size of the resulting archive. This can be done with the -z flag (which just calls gzip on the resulting archive automatically, resulting in a .tar.gz extension) or -j (which uses bzip2 , creating a .tar.bz2 ). For example: # gzip $ tar -czvf newArchive.tar.gz file1 file2 file3 $ tar -xvzf newArchive.tar.gz # bzip2 $ tar -czjf newArchive.tar.bz2 file1 file2 file3 $ tar -xvjf newArchive.tar.bz2","title":"Compressing"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/","text":"Transfering Files with Globus For large data transfers between NREL\u2019s high-performance computing (HPC) systems and another data center, or even a laptop off-site, we recommend using Globus. A supported set of instructions for requesting a HPC Globus account and data transfer using Globus is available on the HPC NREL Website What Is Globus? Globus provides services for research data management, including file transfer. It enables you to quickly, securely and reliably move your data to and from locations you have access to . Globus transfers files using GridFTP . GridFTP is a high-performance data transfer protocol which is optimized for high-bandwidth wide-area networks. It provides more reliable high performance file transfer and synchronization than scp or rsync. It automatically tunes parameters to maximize bandwidth while providing automatic fault recovery and notification of completion or problems. Globus Personal endpoints You can set up a \"Globus Connect Personal EndPoint\", which turns your personal computer into an endpoint, by downloading and installing the Globus Connect Personal application on your system. We use a personal endpoint to demonstrate how to transfer files to and from Eagle. Set Up a Personal EndPoint Login to the Globus website. From the Manage Data drop down menu, select Transfer Files. Then click Get Globus Connect Personal. Pick a name for your personal endpoint and select Generate Startup Key. Follow the instructions on the web page to save your key. Download and install the Globus Connect Personal software on your personal system. Copy the startup key from the Globus web page to this application. Configure Permissions on a Personal EndPoint Once Globus Connect Personal is installed on your system, set up the permissions for reading or writing files to your local system. If you are using a Mac, click on the \"g\" icon on the upper right portion of your screen to access the Globus Connect Personal application. Select Preferences. To allow Globus to copy files to your local system, make sure that the directory (folder) they will go in is Writable. Transferring Files You can transfer files with Globus through the Globus Online website or via a CLI (command line interface). Globus Online Globus Online is a hosted service that allows you to use a browser to transfer files between trusted sites called \"endpoints\". To use it, the Globus software must be installed on the systems at both ends of the data transfer. The NREL endpoint is nrel#eglobus. Click Login on the Globus web site . On the login page select \"Globus ID\" as the login method and click continue. Use the Globus credentials you used to register your Globus.org account. Go to the Transfer Files page, the link is located under the Manage Data tab at the top of the page. Select nrel#eglobus as the endpoint on one right side. In the box asking for authentication, enter your Eagle (NREL HPC) username and password . Do not use your globus.org username and password when authenticating with the nrel#eglobus endpoint. Select another Globus endpoint, such as a personal endpoint or an endpoint at another institution that you have access to. To use your personal endpoint, first start the Globus Connect Personal application. Then enter \"USERNAME#ENDPOINT\" on the left side or use the drop down menu to find it. Click \"go\". To transfer files Select the files you want to transfer someplace else from the system from the dialog box on the left. Select the destination location (a folder or directory) from the dialog box on right right. Click the large blue button at the top of the screen to begin to transfer the files. When your transfer is complete, you will be notified by email. Globus CLI (Command line Interface) Configuring your Globus.org account to allow ssh CLI access To use the CLI you must have a Globus account with ssh access enabled. To enable your account for ssh access you must add your ssh public key to your Globus account by visiting the Manage Identities page and clicking \"manage SSH and X.509 keys\" and then \"Add a New Key\". If you do not have an ssh key, follow the directions here to create one. Globus.org CLI examples $ ssh <globus username>@cli.globusonline.org <command> <options> <params> $ ssh <globus username>@cli.globusonline.org help A one-liner can be used to integrate globus.org CLI commands into shell scripts $ ssh <globus username>@cli.globusonline.org scp nrel#eglobus:/globusro/file1.txt myuser#laptop:/tmp/myfile.txt The globus.org CLI can be used interactively $ ssh <globus username>@cli.globusonline.org Welcome to globusonline.org, <globus username>. Type 'help' for help. $ help $ scp nrel#globus:/globusro/file1.txt myuser#laptop:/tmp/myfile.txt $ exit You can find more information on the Globus CLI from the official Globus CLI documentation .","title":"Globus"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#transfering-files-with-globus","text":"For large data transfers between NREL\u2019s high-performance computing (HPC) systems and another data center, or even a laptop off-site, we recommend using Globus. A supported set of instructions for requesting a HPC Globus account and data transfer using Globus is available on the HPC NREL Website","title":"Transfering Files with Globus"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#what-is-globus","text":"Globus provides services for research data management, including file transfer. It enables you to quickly, securely and reliably move your data to and from locations you have access to . Globus transfers files using GridFTP . GridFTP is a high-performance data transfer protocol which is optimized for high-bandwidth wide-area networks. It provides more reliable high performance file transfer and synchronization than scp or rsync. It automatically tunes parameters to maximize bandwidth while providing automatic fault recovery and notification of completion or problems.","title":"What Is Globus?"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#globus-personal-endpoints","text":"You can set up a \"Globus Connect Personal EndPoint\", which turns your personal computer into an endpoint, by downloading and installing the Globus Connect Personal application on your system. We use a personal endpoint to demonstrate how to transfer files to and from Eagle.","title":"Globus Personal endpoints"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#set-up-a-personal-endpoint","text":"Login to the Globus website. From the Manage Data drop down menu, select Transfer Files. Then click Get Globus Connect Personal. Pick a name for your personal endpoint and select Generate Startup Key. Follow the instructions on the web page to save your key. Download and install the Globus Connect Personal software on your personal system. Copy the startup key from the Globus web page to this application.","title":"Set Up a Personal EndPoint"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#configure-permissions-on-a-personal-endpoint","text":"Once Globus Connect Personal is installed on your system, set up the permissions for reading or writing files to your local system. If you are using a Mac, click on the \"g\" icon on the upper right portion of your screen to access the Globus Connect Personal application. Select Preferences. To allow Globus to copy files to your local system, make sure that the directory (folder) they will go in is Writable.","title":"Configure Permissions on a Personal EndPoint"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#transferring-files","text":"You can transfer files with Globus through the Globus Online website or via a CLI (command line interface).","title":"Transferring Files"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#globus-online","text":"Globus Online is a hosted service that allows you to use a browser to transfer files between trusted sites called \"endpoints\". To use it, the Globus software must be installed on the systems at both ends of the data transfer. The NREL endpoint is nrel#eglobus. Click Login on the Globus web site . On the login page select \"Globus ID\" as the login method and click continue. Use the Globus credentials you used to register your Globus.org account. Go to the Transfer Files page, the link is located under the Manage Data tab at the top of the page. Select nrel#eglobus as the endpoint on one right side. In the box asking for authentication, enter your Eagle (NREL HPC) username and password . Do not use your globus.org username and password when authenticating with the nrel#eglobus endpoint. Select another Globus endpoint, such as a personal endpoint or an endpoint at another institution that you have access to. To use your personal endpoint, first start the Globus Connect Personal application. Then enter \"USERNAME#ENDPOINT\" on the left side or use the drop down menu to find it. Click \"go\". To transfer files Select the files you want to transfer someplace else from the system from the dialog box on the left. Select the destination location (a folder or directory) from the dialog box on right right. Click the large blue button at the top of the screen to begin to transfer the files. When your transfer is complete, you will be notified by email.","title":"Globus Online"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#globus-cli-command-line-interface","text":"Configuring your Globus.org account to allow ssh CLI access To use the CLI you must have a Globus account with ssh access enabled. To enable your account for ssh access you must add your ssh public key to your Globus account by visiting the Manage Identities page and clicking \"manage SSH and X.509 keys\" and then \"Add a New Key\". If you do not have an ssh key, follow the directions here to create one. Globus.org CLI examples $ ssh <globus username>@cli.globusonline.org <command> <options> <params> $ ssh <globus username>@cli.globusonline.org help A one-liner can be used to integrate globus.org CLI commands into shell scripts $ ssh <globus username>@cli.globusonline.org scp nrel#eglobus:/globusro/file1.txt myuser#laptop:/tmp/myfile.txt The globus.org CLI can be used interactively $ ssh <globus username>@cli.globusonline.org Welcome to globusonline.org, <globus username>. Type 'help' for help. $ help $ scp nrel#globus:/globusro/file1.txt myuser#laptop:/tmp/myfile.txt $ exit You can find more information on the Globus CLI from the official Globus CLI documentation .","title":"Globus CLI (Command line Interface)"},{"location":"Documentation/MachineLearning/tensorflow/","text":"TensorFlow with GPU support singularity container This Singularity container supplies TensorFlow 2.3.0 optimized for use with GPU nodes. It also has opencv, numpy, pandas, seaborn, scikit-learn python libraries. For more information on Singularity on please see: Containers Quickstart # Get allocation salloc --gres = gpu:2 -N 1 -A hpcapps -t 0 :10:00 -p debug # Run singularity in srun environment module load singularity-container unset LD_PRELOAD srun --gpus = 2 --pty singularity shell --nv /nopt/nrel/apps/singularity/images/tensorflow_gpu_extras_2.3.0.sif Building a custom image based on TensorFlow In order to build a custom Singularity image based on this one, docker must be installed on your local computer. Docker documentation shows how to install docker. Update Dockerfile shown below to represent the changes desired and save to working directory. FROM tensorflow/tensorflow:2.3.0-gpu-jupyter RUN apt-get update RUN DEBIAN_FRONTEND=\"noninteractive\" apt-get -y install python3-opencv RUN mkdir /custom_env COPY requirements.txt /custom_env RUN pip install -r /custom_env/requirements.txt Update requirements.txt shown below for changing the python library list and save to working directory. seaborn pandas numpy scikit-learn git+https://github.com/tensorflow/docs Build new docker image docker build -t tensorflow-custom-tag-name . Create Singularity image file. Note the ./images directory must be created before running this command. docker run -v /var/run/docker.sock:/var/run/docker.sock \\ -v $( PWD ) /images:/output \\ --privileged -t --rm \\ quay.io/singularity/docker2singularity --name tensorflow_custom.sif \\ tensorflow-custom-tag-name Transfer image file to Eagle. For this example I created a directory named /scratch/$(USER)/tensorflow on eagle rsync -v images/tensorflow_custom.sif eagle.hpc.nrel.gov:/scratch/ $( USER ) /tensorflow/","title":"Containerized TensorFlow"},{"location":"Documentation/MachineLearning/tensorflow/#tensorflow-with-gpu-support-singularity-container","text":"This Singularity container supplies TensorFlow 2.3.0 optimized for use with GPU nodes. It also has opencv, numpy, pandas, seaborn, scikit-learn python libraries. For more information on Singularity on please see: Containers","title":"TensorFlow with GPU support singularity container"},{"location":"Documentation/MachineLearning/tensorflow/#quickstart","text":"# Get allocation salloc --gres = gpu:2 -N 1 -A hpcapps -t 0 :10:00 -p debug # Run singularity in srun environment module load singularity-container unset LD_PRELOAD srun --gpus = 2 --pty singularity shell --nv /nopt/nrel/apps/singularity/images/tensorflow_gpu_extras_2.3.0.sif","title":"Quickstart"},{"location":"Documentation/MachineLearning/tensorflow/#building-a-custom-image-based-on-tensorflow","text":"In order to build a custom Singularity image based on this one, docker must be installed on your local computer. Docker documentation shows how to install docker. Update Dockerfile shown below to represent the changes desired and save to working directory. FROM tensorflow/tensorflow:2.3.0-gpu-jupyter RUN apt-get update RUN DEBIAN_FRONTEND=\"noninteractive\" apt-get -y install python3-opencv RUN mkdir /custom_env COPY requirements.txt /custom_env RUN pip install -r /custom_env/requirements.txt Update requirements.txt shown below for changing the python library list and save to working directory. seaborn pandas numpy scikit-learn git+https://github.com/tensorflow/docs Build new docker image docker build -t tensorflow-custom-tag-name . Create Singularity image file. Note the ./images directory must be created before running this command. docker run -v /var/run/docker.sock:/var/run/docker.sock \\ -v $( PWD ) /images:/output \\ --privileged -t --rm \\ quay.io/singularity/docker2singularity --name tensorflow_custom.sif \\ tensorflow-custom-tag-name Transfer image file to Eagle. For this example I created a directory named /scratch/$(USER)/tensorflow on eagle rsync -v images/tensorflow_custom.sif eagle.hpc.nrel.gov:/scratch/ $( USER ) /tensorflow/","title":"Building a custom image based on TensorFlow"},{"location":"Documentation/MachineLearning/ReinforcementLearning/","text":"Reinforcement Learning on Eagle Welcome to the first NREL HPC tutorial for Reinforcement Learning (RL)! This tutorial covers an extended, albeit simplified, introduction of OpenAI Gym and Ray/RLlib which you can use to effortlessly design, create, and run your own RL experiments on Eagle. You can find the full material of this tutorial in the NREL/HPC GitHub repo . The tutorial covers the following: Brief introduction to RL and Ray Agent training with Ray/RLlib: Experimenting with Ray Tune Single node/Single core. Single node/Multiple cores. Multiple nodes. Run experiments using GPUs for policy learning (helpful for large-scale observation and/or action spaces) Run OpenAI Gym on a single node/single core Login on your Eagle account, create a new Anaconda environment as described in the tutorial repo , and test your installation by running a small example using one of the standard Gym environments (e.g. CartPole-v0 ). Activate the Anaconda enironment and start a Python session module purge conda activate /scratch/$USER/conda-envs/myenv python Then, run the following: import gym env = gym . ens . make ( \"CartPole-v0\" ) env . reset () done = False while not done : action = env . action_space . sample () obs , rew , done , _ = env . step ( action ) print ( action , obs , rew , done ) If everything works correctly, you will see an output similar to: 0 [-0.04506794 -0.22440939 -0.00831435 0.26149667] 1.0 False 1 [-0.04955613 -0.02916975 -0.00308441 -0.03379707] 1.0 False 0 [-0.05013952 -0.22424733 -0.00376036 0.2579111 ] 1.0 False 0 [-0.05462447 -0.4193154 0.00139787 0.54940559] 1.0 False 0 [-0.06301078 -0.61445696 0.01238598 0.84252861] 1.0 False 1 [-0.07529992 -0.41950623 0.02923655 0.55376634] 1.0 False 0 [-0.08369004 -0.61502627 0.04031188 0.85551538] 1.0 False 0 [-0.09599057 -0.8106737 0.05742218 1.16059658] 1.0 False 0 [-0.11220404 -1.00649474 0.08063412 1.47071687] 1.0 False 1 [-0.13233393 -0.81244634 0.11004845 1.20427076] 1.0 False 1 [-0.14858286 -0.61890536 0.13413387 0.94800442] 1.0 False 0 [-0.16096097 -0.8155534 0.15309396 1.27964413] 1.0 False 1 [-0.17727204 -0.62267747 0.17868684 1.03854806] 1.0 False 0 [-0.18972559 -0.81966549 0.1994578 1.38158021] 1.0 False 0 [-0.2061189 -1.0166379 0.22708941 1.72943365] 1.0 True Note that the above process does not involve any training. Agent training with Ray/RLlib RL algorithms are notorious for the amount of data they need to collect in order to learn policies. The more data collected, the better the training will (usually) be. The best way to do it is to run many Gym instances in parallel and collecting experience, and this is where RLlib assists. RLlib is an open-source library for reinforcement learning that offers both high scalability and a unified API for a variety of applications. It supports all known deep learning frameworks such as Tensorflow, Pytorch, although most parts are framework-agnostic and can be used by either one. The RL policy learning examples provided in this tutorial demonstrate the RLlib abilities. For convenience, the CartPole-v0 OpenAI Gym environment will be used. The most straightforward way is to create a Python \"trainer\" script. It will call the necessary packages, setup flags, and run the experiments, all nicely put in a few lines of Python code. Import packages Begin trainer by importing the ray package: import ray from ray import tune Ray consists of an API readily available for building distributed applications . On top of it, there are several problem-solving libraries, one of which is RLlib. Tune is also one of Ray 's libraries for scalable hyperparameter tuning. All RLlib trainers (scripts for RL agent training) are compatible with Tune API, making experimenting easy and streamlined. Import also the argparse package and setup some flags. Although that step is not mandatory, these flags will allow controlling of certain hyperparameters, such as: * RL algorithm utilized (e.g. PPO, DQN) * Number of CPUs/GPUs * ...and others import argparse Create flags Begin by defining the following flags: parser . add_argument ( \"--num-cpus\" , type = int , default = 0 ) parser . add_argument ( \"--num-gpus\" , type = int , default = 0 ) parser . add_argument ( \"--name-env\" , type = str , default = \"CartPole-v0\" ) parser . add_argument ( \"--run\" , type = str , default = \"DQN\" ) parser . add_argument ( \"--local-mode\" , action = \"store_true\" ) All of them are self-explanatory, however let's see each one separately. 1. --num-cpus : Defines the number of CPU cores used for experience collection (Default value 0 means allocation of a single CPU core). 2. --num-gpus : Allocates a GPU node for policy learning (works only for Tensorflow-GPU). Except whole values (1,2,etc.), it also accepts partial values, in case 100% of the GPU is not necessary. 3. --name-env : The name of the OpenAI Gym environment. 4. --run : Specifies the RL algorithm for agent training. 5. --local-mode : Helps defining whether experiments running on a single core or multiple cores. Initialize Ray Ray is able to run either on a local mode (e.g. laptop, personal computer), or on a cluster. For the first experiment, only a single core is needed, therefore, setup ray to run on a local mode. Then, set the number of CPU cores to be used. Run experiments with Tune This is the final step in this basic trainer. Tune's tune.run function initiates the agent training process. There are three main arguments in this function: * RL algorithm (string): It is defined in the --run flag (PPO, DQN, etc.). * stop (dictionary): Provides a criterion to stop training (in this example is the number of training iterations; stop training when iterations reach 10,000). * config (dictionary): Basic information for training, contains the OpenAI Gym environment name, number of CPUs/GPUs, and others. tune . run ( args . run , name = args . name_env , stop = { \"training_iteration\" : 10000 }, config = { \"env\" : args . name_env , \"num_workers\" : args . num_cpus , \"num_gpus\" : args . num_gpus , \"ignore_worker_failures\" : True } ) The RLlib trainer is ready! Except the aforementioned default hyperparameters, every RL algorithm provided by RLlib has its own hyperparameters and their default values that can be tuned in advance. The code of the trainer in this example can be found in the tutorial repo . Run experiments on Eagle Follow the steps in the tutorial repo carefully. Run multi-core experiments The previous example is designed to run on a single CPU core. However, as explained above, RL training is highly benefited from running multiple concurrent OpenAI Gym rollouts. A single node on Eagle has 36 CPU cores, therefore use any number of those in order to speed up your agent training. For all 36 cores, adjust the --num-cpus hyperparameter to reflect to all CPUs on the node: python simple_trainer.py --num-cpus 35 Again, RLlib by default utilizes a single CPU core, therefore by putting --num-cpus equal to 35 means that all 36 cores are requested. Such is not the case with the num_gpus key, where zero means no GPU allocation is permitted. This is because GPUs are used for policy training and not running the OpenAI Gym environment instances, thus they are not mandatory (although having a GPU node can assist the agent training by reducing training time). Run experiments on multiple nodes Let's focus now on cases where the problem under consideration is highly complex and requires vast amounts of training data for training the policy network in a reasonable amount of time. It could be then, that you will require more than one nodes to run your experiments. In this case, it is better to use a slurm script file that will include all the necessary commands for agent train using multiple CPUs and multiple nodes. Example: CartPole-v0 As explained above, CartPole is a rather simple environment and solving it using multiple cores on a single node feels like an overkill, let alone multiple nodes! However, it is a good example for giving you an experience on running RL experiments using RLlib. For multiple nodes it is more convenient to use a slurm script instead of an interactive node. Slurm files are submitted as sbatch <name_of_your_batch_script> , and the results are exported in an slurm-<job_id>.out file. The .out file can be interactively accessed during training using the tail -f slurm-<job_id>.out command. Otherwise, after training, open it using a standard text editor (e.g. nano ). Next, the basic parts of the slurm script file are given. The repo also provides the complete script . The slurm file begins with defining some basic SBATCH options, including the desired training time, number of nodes, tasks per node, etc. #!/bin/bash --login #SBATCH --job-name=cartpole-multiple-nodes #SBATCH --time=00:10:00 #SBATCH --nodes=3 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=36 #SBATCH --account=A<account> env Allocating multiple nodes means creating a Ray cluster. A Ray cluster consists of a head node and a set of worker nodes. The head node needs to be started first, and the worker nodes are given the address of the head node to form the cluster. The agent training will run for 20 minutes ( SBATCH --time=00:20:00 ), and on three Eagle CPU nodes ( SBATCH --nodes=3 ). Every node will execute a single task ( SBATCH --tasks-per-node=1 ), which will be executed on all 36 cores ( SBATCH --cpus-per-task=36 ). Then, define the project account. Other options are also available, such as whether to prioritize the experiment ( --qos=high ). Use the commands to activate the Anaconda environment. Do not forget to unset LD_PRELOAD . module purge conda activate /scratch/$USER/conda-envs/env_example unset LD_PRELOAD Set up the Redis server that will allow all the nodes you requested to communicate with each other. For that, set a Redis password: ip_prefix=$(srun --nodes=1 --ntasks=1 -w $node1 hostname --ip-address) port=6379 ip_head=$ip_prefix:$port redis_password=$(uuidgen) Submit the jobs one at a time at the workers, starting with the head node and moving on to the rest of them. srun --nodes=1 --ntasks=1 -w $node1 ray start --block --head \\ --node-ip-address= \"$ip_prefix\" --port=$port --redis-password=$redis_password & sleep 10 echo \"starting workers\" for (( i=1; i < =$worker_num; i++ )) do node2=${nodes_array[$i]} echo \"i=${i}, node2=${node2}\" srun --nodes=1 --ntasks=1 -w $node2 ray start --block --address \"$ip_head\" --redis-password=$redis_password & sleep 5 done Set the Python script to run. Since this experiment will run on a cluster, Ray will be initialized as: ray . init ( _redis_password = args . redis_password , address = os . environ [ \"ip_head\" ]) num_cpus = args . num_cpus - 1 The --redis-password option must be active, along with the total number of CPUs: python -u simple_trainer.py --redis-password $redis_password --num-cpus $total_cpus The experiment is ready to begin, simply run: sbatch <your_slurm_file> If the trainer script is on a different directory, make sure to cd to this directory in the slurm script before executing it. ### Example where the trainer is on scratch: cd /scratch/$USER/path_to_specific_directory python -u simple_trainer.py --redis-password $redis_password --num-cpus $total_cpus Experimenting using GPUs It is now time to learn running experiments using GPU nodes on Eagle that can boost training times considerably. GPU nodes however is better to be utilized only in cases of environments with very large observation and/or action spaces. CartPole will be used again for establishing a template. Allocate GPU node The following instructions are the same for both regular and Optimized TF versions of the Anaconda environments Running experiments with combined CPU and GPU nodes is not so straightforward as running them using only CPU nodes (either single or multiple nodes). Particularly, heterogenous jobs using slurm have to be submitted. Begin at first by specifying some basic options, similarly to previous section: #!/bin/bash --login #SBATCH --account=A < account> #SBATCH --job-name=cartpole-gpus #SBATCH --time=00:10:00 The slurm script will clearly define the various jobs. These jobs include the CPU nodes that will carry the environment rollouts, and the GPU node for policy learning. Eagle has 44 GPU nodes and each node has 2 GPUs. Either request one GPU per node ( --gres=gpu:1 ), or both of them ( --gres=gpu:2 ). For the purposes of this tutorial, one GPU core on a single node is utilized. In total, slurm nodes can be categorized as: * A head node, and multiple rollout nodes (as before) * A policy training node (GPU) Include the hetjob header for both the rollout nodes and the policy training node. Three CPU nodes are requested to be used for rollouts and a single GPU node is requested for policy learning: # Ray head node #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 # Rollout nodes - Nodes with multiple runs of OpenAI Gym #SBATCH hetjob #SBATCH --nodes=3 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=36 # Policy training node - This is the GPU node #SBATCH hetjob #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --partition=debug #SBATCH --gres=gpu:1 Of course, any number of CPU/GPU nodes can be requested, depending on problem complexity. As an example, a single node and perhaps just a single CPU core may be requested. Now, it is more reasonable to request GPUs for an OpenAI Gym environment that utilizes high-dimensional observation and/or action spaces. Hence, the first priority would be to start with multiple CPU nodes, and request GPUs only if they are needed. For the three types of nodes (head, rollouts, training), define three separate groups: head_node=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_0) rollout_nodes=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_1) rollout_nodes_array=( $rollout_nodes ) learner_node=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_2) echo \"head node : \" $head_node echo \"rollout nodes: \" $rollout_nodes echo \"learner node : \" $learner_node Each group of nodes requires its separate srun command so that they will run independently of each other. echo \"starting head node at $head_node\" srun --pack-group=0 --nodes=1 --ntasks=1 -w $head_node ray start --block --head \\ --node-ip-address= \"$ip_prefix\" --port=$port --redis-password=$redis_password & # Starting the head sleep 10 echo \"starting rollout workers\" for (( i=0; i < $rollout_node_num; i++ )) do rollout_node=${rollout_nodes_array[$i]} echo \"i=${i}, rollout_node=${rollout_node}\" srun --pack-group=1 --nodes=1 --ntasks=1 -w $rollout_node \\ ray start --block --address \"$ip_head\" --redis-password=$redis_password & # Starting the workers sleep 5 done echo \"starting learning on GPU\" srun --pack-group=2 --nodes=1 --gres=gpu:1 -w $learner_node ray start --block --address \"$ip_head\" --redis-password=$redis_password & The slurm commands for the head and rollout nodes are identical to those from the previous section. A third command is also added for engaging the GPU node. Finally, call python -u simple_trainer.py --redis-password $redis_password --num-cpus $rollout_num_cpus --num-gpus 1 to begin training. Add the ---num-gpus argument to include the requested GPU node (or nodes in case of --gres=gpu:2 ) for policy training. There is no need to manually declare the GPU for policy training in the simple_trainer.py , RLlib will automatically recognize the available GPU and use it accordingly. The repo contains the complete slurm file versions for both env_example_gpu and env_gpu_optimized_tf , and they can be used as templates for future projects. Create Gym environments from scratch So far, only benchmark Gym environments were used in order to demonstrate the processes for running experiments. It is time now to see how one can create their own Gym environment, carefully tailor-made to one's needs. OpenAI Gym functionality allows the creation of custom-made environments using the same structure as the benchmark ones. Custom-made environments can become extremely complex due to the mechanics involved and may require many subscripts that perform parts of the simulation. Nevertheless, the basis of all environments is simply a Python class that inherits the gym.Env class, where the user can implement the three main Gym functions and define any hyperapameters necessary: * def __init__(self) : Initializes the environment. It defines initial values for variables/hyperparameters and may contain other necessary information. It also defines the dimensionality of the problem. Dimensionality is expressed at the sizes of the observation and action spaces, which are given using the parameters self.observation_space and self.action_space , respectively. Depending on their nature, they can take discrete, continuous, or a combination of values. OpenAI provides detailed examples of each one of these types of spaces. * def reset(self) : When called, it resets the environment on a previous state (hence the name). This state can either be a user-defined initial state or it may be a random initial position. The latter can be found on environments that describe locomotion like CartPole , where the initial state can be any possible position of the pole on the cart. * def step(self, action) : The heart of the class. It defines the inner mechanics of the environment, hence it can be seen as some kind of simulator. Its main input is the sampled action, which when acted upon moves the environment into a new state and calculates the new reward. The new state and reward are two of the function's output and they are necessary for policy training since they are also inputs to the policy network. Other outputs include a boolean variable done that is True when the environment reaches its final state (if it exists), and False otherwise * , as well as a dictionary ( info ) with user-defined key-value objects that contain further information from the inner workings of the environment. Many environments do not consider a final state, since it might not make sense (e.g. a traffic simulator for fleets of autonomous ridesharing vehicles that reposition themselves based on a certain criterion. In this case the reward will get better every time, but there is no notion of a final vehicle position).* Directions of how to create and register a custom-made OpenAI Gym environment are given below. Create an environment class As stated above, the basis of any Gym environment is a Python class that inherits the gym.Env class. After importing the gym package, define the class as: import gym class BasicEnv ( gym . Env ):( ... ) The example environment is very simple and is represented by two possible states (0, 1) and 5 possible actions (0-4). For the purposes of this tutorial, consider state 0 as the initial state, and state 1 as the final state. Define the dimensions of observation and action spaces in the def __init__(self) function: def __init__ ( self ): self . action_space = gym . spaces . Discrete ( 5 ) # --> Actions take values in the 0-4 interval self . observation_space = gym . spaces . Discrete ( 2 ) # --> Two possible states [0,1] Both spaces take discrete values, therefore they are defined using Gym's Discrete function. Other possible functions are Box for continuous single- or multi-dimensional observations and states, MultiDiscrete for vectors of discrete values, etc. OpenAi provides detailed explanation for all different space forms. Next, define the def reset(self) function: def reset ( self ): state = 0 return state In this example, the reset function simply returns the environment to the initial state. Finally, define the def step(self, action) function, which takes as input the sampled action. Here the step function takes the environment at state 1 and based on the action, returns a reward of 1 or -1: def step ( self , action ): state = 1 if action == 2 : reward = 1 else : reward = - 1 done = True info = {} return state , reward , done , info That's it, the new Gym environment is ready! Make note that there is one more function usually found on Gym environments. This is the def render(self) function, and is called in random intervals throughout training returning a \"snapshot\" of the environment at that time. While this is helpful for evaluating the agent training process, it is not necessary for the actual training process. OpenAI documentation provides details for every one of these functions. You can find the full script of this environment in the repo. Run experiments on RLlib Let's now train the agent with RLlib. The full trainer script is given at the repo. The trainer is almost identical to the one used before , with few additions that are necessary to register the new environment. At first, along with ray and tune , import: from ray.tune.registry import register_env from custom_env import BasicEnv The register_env function is used to register the new environment, which is imported from the custom_env.py . Function register_env takes two arguments: * Training name of the environment, chosen by the developer. * Actual name of the environment ( BasicEnv ) in a lambda config: function. env_name = \"custom-env\" register_env ( env_name , lambda config : BasicEnv ()) Once again, RLlib provides detailed explanation of how register_env works. The tune.run function, instead of args.name_env , it uses the env_name defined above. That's all! Proceed with agent training using any of the slurm scripts provided by the repo. As a final note, creating custom-made OpenAI Gym environment is more like an art than science. The main issue is to really clarify what the environment represents and how it works, and then define this functionality in Python. Validating results using Tensorboard Another way of visualizing the performance of agent training is with Tensorboard . Navigate to the ray_results directory: cd ~/ray_results/ Every RL experiment generates a subdirectory named from the OpenAI Gym environment used in the experiment. E.g., after running all the examples previously shown in this tutorial, ray_results will have a subdirectory named CartPole-v0 . Within, every experiment using CartPole generates a new subdirectory. For the purpose of this tutorial, cd to the CartPole-v0 subdirectory and activate one of the environments: module purge conda activate <your_environment> Initialize Tensorboard following the steps in this tutorial . Open the localhost url in a browser, and all plots for rewards, iterations and other metrics will be demonstrated as: The tune/episode_reward_mean plot is essentialy the same as the figure plotted from data in the progress.csv file. The difference in the x-axis scale has a simple explanation. The episode_reward_mean column on the progress.csv file shows the reward progress on every training iteration, while the tune/episode_reward_mean plot on Tensorboard shows reward progress on every training episode (a single RLlib training iteration consists of thousands of episodes).","title":"Reinforcement Learning"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#reinforcement-learning-on-eagle","text":"Welcome to the first NREL HPC tutorial for Reinforcement Learning (RL)! This tutorial covers an extended, albeit simplified, introduction of OpenAI Gym and Ray/RLlib which you can use to effortlessly design, create, and run your own RL experiments on Eagle. You can find the full material of this tutorial in the NREL/HPC GitHub repo . The tutorial covers the following: Brief introduction to RL and Ray Agent training with Ray/RLlib: Experimenting with Ray Tune Single node/Single core. Single node/Multiple cores. Multiple nodes. Run experiments using GPUs for policy learning (helpful for large-scale observation and/or action spaces)","title":"Reinforcement Learning on Eagle"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-openai-gym-on-a-single-nodesingle-core","text":"Login on your Eagle account, create a new Anaconda environment as described in the tutorial repo , and test your installation by running a small example using one of the standard Gym environments (e.g. CartPole-v0 ). Activate the Anaconda enironment and start a Python session module purge conda activate /scratch/$USER/conda-envs/myenv python Then, run the following: import gym env = gym . ens . make ( \"CartPole-v0\" ) env . reset () done = False while not done : action = env . action_space . sample () obs , rew , done , _ = env . step ( action ) print ( action , obs , rew , done ) If everything works correctly, you will see an output similar to: 0 [-0.04506794 -0.22440939 -0.00831435 0.26149667] 1.0 False 1 [-0.04955613 -0.02916975 -0.00308441 -0.03379707] 1.0 False 0 [-0.05013952 -0.22424733 -0.00376036 0.2579111 ] 1.0 False 0 [-0.05462447 -0.4193154 0.00139787 0.54940559] 1.0 False 0 [-0.06301078 -0.61445696 0.01238598 0.84252861] 1.0 False 1 [-0.07529992 -0.41950623 0.02923655 0.55376634] 1.0 False 0 [-0.08369004 -0.61502627 0.04031188 0.85551538] 1.0 False 0 [-0.09599057 -0.8106737 0.05742218 1.16059658] 1.0 False 0 [-0.11220404 -1.00649474 0.08063412 1.47071687] 1.0 False 1 [-0.13233393 -0.81244634 0.11004845 1.20427076] 1.0 False 1 [-0.14858286 -0.61890536 0.13413387 0.94800442] 1.0 False 0 [-0.16096097 -0.8155534 0.15309396 1.27964413] 1.0 False 1 [-0.17727204 -0.62267747 0.17868684 1.03854806] 1.0 False 0 [-0.18972559 -0.81966549 0.1994578 1.38158021] 1.0 False 0 [-0.2061189 -1.0166379 0.22708941 1.72943365] 1.0 True Note that the above process does not involve any training.","title":"Run OpenAI Gym on a single node/single core"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#agent-training-with-rayrllib","text":"RL algorithms are notorious for the amount of data they need to collect in order to learn policies. The more data collected, the better the training will (usually) be. The best way to do it is to run many Gym instances in parallel and collecting experience, and this is where RLlib assists. RLlib is an open-source library for reinforcement learning that offers both high scalability and a unified API for a variety of applications. It supports all known deep learning frameworks such as Tensorflow, Pytorch, although most parts are framework-agnostic and can be used by either one. The RL policy learning examples provided in this tutorial demonstrate the RLlib abilities. For convenience, the CartPole-v0 OpenAI Gym environment will be used. The most straightforward way is to create a Python \"trainer\" script. It will call the necessary packages, setup flags, and run the experiments, all nicely put in a few lines of Python code.","title":"Agent training with Ray/RLlib"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#import-packages","text":"Begin trainer by importing the ray package: import ray from ray import tune Ray consists of an API readily available for building distributed applications . On top of it, there are several problem-solving libraries, one of which is RLlib. Tune is also one of Ray 's libraries for scalable hyperparameter tuning. All RLlib trainers (scripts for RL agent training) are compatible with Tune API, making experimenting easy and streamlined. Import also the argparse package and setup some flags. Although that step is not mandatory, these flags will allow controlling of certain hyperparameters, such as: * RL algorithm utilized (e.g. PPO, DQN) * Number of CPUs/GPUs * ...and others import argparse","title":"Import packages"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#create-flags","text":"Begin by defining the following flags: parser . add_argument ( \"--num-cpus\" , type = int , default = 0 ) parser . add_argument ( \"--num-gpus\" , type = int , default = 0 ) parser . add_argument ( \"--name-env\" , type = str , default = \"CartPole-v0\" ) parser . add_argument ( \"--run\" , type = str , default = \"DQN\" ) parser . add_argument ( \"--local-mode\" , action = \"store_true\" ) All of them are self-explanatory, however let's see each one separately. 1. --num-cpus : Defines the number of CPU cores used for experience collection (Default value 0 means allocation of a single CPU core). 2. --num-gpus : Allocates a GPU node for policy learning (works only for Tensorflow-GPU). Except whole values (1,2,etc.), it also accepts partial values, in case 100% of the GPU is not necessary. 3. --name-env : The name of the OpenAI Gym environment. 4. --run : Specifies the RL algorithm for agent training. 5. --local-mode : Helps defining whether experiments running on a single core or multiple cores.","title":"Create flags"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#initialize-ray","text":"Ray is able to run either on a local mode (e.g. laptop, personal computer), or on a cluster. For the first experiment, only a single core is needed, therefore, setup ray to run on a local mode. Then, set the number of CPU cores to be used.","title":"Initialize Ray"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-experiments-with-tune","text":"This is the final step in this basic trainer. Tune's tune.run function initiates the agent training process. There are three main arguments in this function: * RL algorithm (string): It is defined in the --run flag (PPO, DQN, etc.). * stop (dictionary): Provides a criterion to stop training (in this example is the number of training iterations; stop training when iterations reach 10,000). * config (dictionary): Basic information for training, contains the OpenAI Gym environment name, number of CPUs/GPUs, and others. tune . run ( args . run , name = args . name_env , stop = { \"training_iteration\" : 10000 }, config = { \"env\" : args . name_env , \"num_workers\" : args . num_cpus , \"num_gpus\" : args . num_gpus , \"ignore_worker_failures\" : True } ) The RLlib trainer is ready! Except the aforementioned default hyperparameters, every RL algorithm provided by RLlib has its own hyperparameters and their default values that can be tuned in advance. The code of the trainer in this example can be found in the tutorial repo .","title":"Run experiments with Tune"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-experiments-on-eagle","text":"Follow the steps in the tutorial repo carefully.","title":"Run experiments on Eagle"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-multi-core-experiments","text":"The previous example is designed to run on a single CPU core. However, as explained above, RL training is highly benefited from running multiple concurrent OpenAI Gym rollouts. A single node on Eagle has 36 CPU cores, therefore use any number of those in order to speed up your agent training. For all 36 cores, adjust the --num-cpus hyperparameter to reflect to all CPUs on the node: python simple_trainer.py --num-cpus 35 Again, RLlib by default utilizes a single CPU core, therefore by putting --num-cpus equal to 35 means that all 36 cores are requested. Such is not the case with the num_gpus key, where zero means no GPU allocation is permitted. This is because GPUs are used for policy training and not running the OpenAI Gym environment instances, thus they are not mandatory (although having a GPU node can assist the agent training by reducing training time).","title":"Run multi-core experiments"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-experiments-on-multiple-nodes","text":"Let's focus now on cases where the problem under consideration is highly complex and requires vast amounts of training data for training the policy network in a reasonable amount of time. It could be then, that you will require more than one nodes to run your experiments. In this case, it is better to use a slurm script file that will include all the necessary commands for agent train using multiple CPUs and multiple nodes.","title":"Run experiments on multiple nodes"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#example-cartpole-v0","text":"As explained above, CartPole is a rather simple environment and solving it using multiple cores on a single node feels like an overkill, let alone multiple nodes! However, it is a good example for giving you an experience on running RL experiments using RLlib. For multiple nodes it is more convenient to use a slurm script instead of an interactive node. Slurm files are submitted as sbatch <name_of_your_batch_script> , and the results are exported in an slurm-<job_id>.out file. The .out file can be interactively accessed during training using the tail -f slurm-<job_id>.out command. Otherwise, after training, open it using a standard text editor (e.g. nano ). Next, the basic parts of the slurm script file are given. The repo also provides the complete script . The slurm file begins with defining some basic SBATCH options, including the desired training time, number of nodes, tasks per node, etc. #!/bin/bash --login #SBATCH --job-name=cartpole-multiple-nodes #SBATCH --time=00:10:00 #SBATCH --nodes=3 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=36 #SBATCH --account=A<account> env Allocating multiple nodes means creating a Ray cluster. A Ray cluster consists of a head node and a set of worker nodes. The head node needs to be started first, and the worker nodes are given the address of the head node to form the cluster. The agent training will run for 20 minutes ( SBATCH --time=00:20:00 ), and on three Eagle CPU nodes ( SBATCH --nodes=3 ). Every node will execute a single task ( SBATCH --tasks-per-node=1 ), which will be executed on all 36 cores ( SBATCH --cpus-per-task=36 ). Then, define the project account. Other options are also available, such as whether to prioritize the experiment ( --qos=high ). Use the commands to activate the Anaconda environment. Do not forget to unset LD_PRELOAD . module purge conda activate /scratch/$USER/conda-envs/env_example unset LD_PRELOAD Set up the Redis server that will allow all the nodes you requested to communicate with each other. For that, set a Redis password: ip_prefix=$(srun --nodes=1 --ntasks=1 -w $node1 hostname --ip-address) port=6379 ip_head=$ip_prefix:$port redis_password=$(uuidgen) Submit the jobs one at a time at the workers, starting with the head node and moving on to the rest of them. srun --nodes=1 --ntasks=1 -w $node1 ray start --block --head \\ --node-ip-address= \"$ip_prefix\" --port=$port --redis-password=$redis_password & sleep 10 echo \"starting workers\" for (( i=1; i < =$worker_num; i++ )) do node2=${nodes_array[$i]} echo \"i=${i}, node2=${node2}\" srun --nodes=1 --ntasks=1 -w $node2 ray start --block --address \"$ip_head\" --redis-password=$redis_password & sleep 5 done Set the Python script to run. Since this experiment will run on a cluster, Ray will be initialized as: ray . init ( _redis_password = args . redis_password , address = os . environ [ \"ip_head\" ]) num_cpus = args . num_cpus - 1 The --redis-password option must be active, along with the total number of CPUs: python -u simple_trainer.py --redis-password $redis_password --num-cpus $total_cpus The experiment is ready to begin, simply run: sbatch <your_slurm_file> If the trainer script is on a different directory, make sure to cd to this directory in the slurm script before executing it. ### Example where the trainer is on scratch: cd /scratch/$USER/path_to_specific_directory python -u simple_trainer.py --redis-password $redis_password --num-cpus $total_cpus","title":"Example: CartPole-v0"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#experimenting-using-gpus","text":"It is now time to learn running experiments using GPU nodes on Eagle that can boost training times considerably. GPU nodes however is better to be utilized only in cases of environments with very large observation and/or action spaces. CartPole will be used again for establishing a template.","title":"Experimenting using GPUs"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#allocate-gpu-node","text":"The following instructions are the same for both regular and Optimized TF versions of the Anaconda environments Running experiments with combined CPU and GPU nodes is not so straightforward as running them using only CPU nodes (either single or multiple nodes). Particularly, heterogenous jobs using slurm have to be submitted. Begin at first by specifying some basic options, similarly to previous section: #!/bin/bash --login #SBATCH --account=A < account> #SBATCH --job-name=cartpole-gpus #SBATCH --time=00:10:00 The slurm script will clearly define the various jobs. These jobs include the CPU nodes that will carry the environment rollouts, and the GPU node for policy learning. Eagle has 44 GPU nodes and each node has 2 GPUs. Either request one GPU per node ( --gres=gpu:1 ), or both of them ( --gres=gpu:2 ). For the purposes of this tutorial, one GPU core on a single node is utilized. In total, slurm nodes can be categorized as: * A head node, and multiple rollout nodes (as before) * A policy training node (GPU) Include the hetjob header for both the rollout nodes and the policy training node. Three CPU nodes are requested to be used for rollouts and a single GPU node is requested for policy learning: # Ray head node #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 # Rollout nodes - Nodes with multiple runs of OpenAI Gym #SBATCH hetjob #SBATCH --nodes=3 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=36 # Policy training node - This is the GPU node #SBATCH hetjob #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --partition=debug #SBATCH --gres=gpu:1 Of course, any number of CPU/GPU nodes can be requested, depending on problem complexity. As an example, a single node and perhaps just a single CPU core may be requested. Now, it is more reasonable to request GPUs for an OpenAI Gym environment that utilizes high-dimensional observation and/or action spaces. Hence, the first priority would be to start with multiple CPU nodes, and request GPUs only if they are needed. For the three types of nodes (head, rollouts, training), define three separate groups: head_node=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_0) rollout_nodes=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_1) rollout_nodes_array=( $rollout_nodes ) learner_node=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_2) echo \"head node : \" $head_node echo \"rollout nodes: \" $rollout_nodes echo \"learner node : \" $learner_node Each group of nodes requires its separate srun command so that they will run independently of each other. echo \"starting head node at $head_node\" srun --pack-group=0 --nodes=1 --ntasks=1 -w $head_node ray start --block --head \\ --node-ip-address= \"$ip_prefix\" --port=$port --redis-password=$redis_password & # Starting the head sleep 10 echo \"starting rollout workers\" for (( i=0; i < $rollout_node_num; i++ )) do rollout_node=${rollout_nodes_array[$i]} echo \"i=${i}, rollout_node=${rollout_node}\" srun --pack-group=1 --nodes=1 --ntasks=1 -w $rollout_node \\ ray start --block --address \"$ip_head\" --redis-password=$redis_password & # Starting the workers sleep 5 done echo \"starting learning on GPU\" srun --pack-group=2 --nodes=1 --gres=gpu:1 -w $learner_node ray start --block --address \"$ip_head\" --redis-password=$redis_password & The slurm commands for the head and rollout nodes are identical to those from the previous section. A third command is also added for engaging the GPU node. Finally, call python -u simple_trainer.py --redis-password $redis_password --num-cpus $rollout_num_cpus --num-gpus 1 to begin training. Add the ---num-gpus argument to include the requested GPU node (or nodes in case of --gres=gpu:2 ) for policy training. There is no need to manually declare the GPU for policy training in the simple_trainer.py , RLlib will automatically recognize the available GPU and use it accordingly. The repo contains the complete slurm file versions for both env_example_gpu and env_gpu_optimized_tf , and they can be used as templates for future projects.","title":"Allocate GPU node"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#create-gym-environments-from-scratch","text":"So far, only benchmark Gym environments were used in order to demonstrate the processes for running experiments. It is time now to see how one can create their own Gym environment, carefully tailor-made to one's needs. OpenAI Gym functionality allows the creation of custom-made environments using the same structure as the benchmark ones. Custom-made environments can become extremely complex due to the mechanics involved and may require many subscripts that perform parts of the simulation. Nevertheless, the basis of all environments is simply a Python class that inherits the gym.Env class, where the user can implement the three main Gym functions and define any hyperapameters necessary: * def __init__(self) : Initializes the environment. It defines initial values for variables/hyperparameters and may contain other necessary information. It also defines the dimensionality of the problem. Dimensionality is expressed at the sizes of the observation and action spaces, which are given using the parameters self.observation_space and self.action_space , respectively. Depending on their nature, they can take discrete, continuous, or a combination of values. OpenAI provides detailed examples of each one of these types of spaces. * def reset(self) : When called, it resets the environment on a previous state (hence the name). This state can either be a user-defined initial state or it may be a random initial position. The latter can be found on environments that describe locomotion like CartPole , where the initial state can be any possible position of the pole on the cart. * def step(self, action) : The heart of the class. It defines the inner mechanics of the environment, hence it can be seen as some kind of simulator. Its main input is the sampled action, which when acted upon moves the environment into a new state and calculates the new reward. The new state and reward are two of the function's output and they are necessary for policy training since they are also inputs to the policy network. Other outputs include a boolean variable done that is True when the environment reaches its final state (if it exists), and False otherwise * , as well as a dictionary ( info ) with user-defined key-value objects that contain further information from the inner workings of the environment. Many environments do not consider a final state, since it might not make sense (e.g. a traffic simulator for fleets of autonomous ridesharing vehicles that reposition themselves based on a certain criterion. In this case the reward will get better every time, but there is no notion of a final vehicle position).* Directions of how to create and register a custom-made OpenAI Gym environment are given below.","title":"Create Gym environments from scratch"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#create-an-environment-class","text":"As stated above, the basis of any Gym environment is a Python class that inherits the gym.Env class. After importing the gym package, define the class as: import gym class BasicEnv ( gym . Env ):( ... ) The example environment is very simple and is represented by two possible states (0, 1) and 5 possible actions (0-4). For the purposes of this tutorial, consider state 0 as the initial state, and state 1 as the final state. Define the dimensions of observation and action spaces in the def __init__(self) function: def __init__ ( self ): self . action_space = gym . spaces . Discrete ( 5 ) # --> Actions take values in the 0-4 interval self . observation_space = gym . spaces . Discrete ( 2 ) # --> Two possible states [0,1] Both spaces take discrete values, therefore they are defined using Gym's Discrete function. Other possible functions are Box for continuous single- or multi-dimensional observations and states, MultiDiscrete for vectors of discrete values, etc. OpenAi provides detailed explanation for all different space forms. Next, define the def reset(self) function: def reset ( self ): state = 0 return state In this example, the reset function simply returns the environment to the initial state. Finally, define the def step(self, action) function, which takes as input the sampled action. Here the step function takes the environment at state 1 and based on the action, returns a reward of 1 or -1: def step ( self , action ): state = 1 if action == 2 : reward = 1 else : reward = - 1 done = True info = {} return state , reward , done , info That's it, the new Gym environment is ready! Make note that there is one more function usually found on Gym environments. This is the def render(self) function, and is called in random intervals throughout training returning a \"snapshot\" of the environment at that time. While this is helpful for evaluating the agent training process, it is not necessary for the actual training process. OpenAI documentation provides details for every one of these functions. You can find the full script of this environment in the repo.","title":"Create an environment class"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-experiments-on-rllib","text":"Let's now train the agent with RLlib. The full trainer script is given at the repo. The trainer is almost identical to the one used before , with few additions that are necessary to register the new environment. At first, along with ray and tune , import: from ray.tune.registry import register_env from custom_env import BasicEnv The register_env function is used to register the new environment, which is imported from the custom_env.py . Function register_env takes two arguments: * Training name of the environment, chosen by the developer. * Actual name of the environment ( BasicEnv ) in a lambda config: function. env_name = \"custom-env\" register_env ( env_name , lambda config : BasicEnv ()) Once again, RLlib provides detailed explanation of how register_env works. The tune.run function, instead of args.name_env , it uses the env_name defined above. That's all! Proceed with agent training using any of the slurm scripts provided by the repo. As a final note, creating custom-made OpenAI Gym environment is more like an art than science. The main issue is to really clarify what the environment represents and how it works, and then define this functionality in Python.","title":"Run experiments on RLlib"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#validating-results-using-tensorboard","text":"Another way of visualizing the performance of agent training is with Tensorboard . Navigate to the ray_results directory: cd ~/ray_results/ Every RL experiment generates a subdirectory named from the OpenAI Gym environment used in the experiment. E.g., after running all the examples previously shown in this tutorial, ray_results will have a subdirectory named CartPole-v0 . Within, every experiment using CartPole generates a new subdirectory. For the purpose of this tutorial, cd to the CartPole-v0 subdirectory and activate one of the environments: module purge conda activate <your_environment> Initialize Tensorboard following the steps in this tutorial . Open the localhost url in a browser, and all plots for rewards, iterations and other metrics will be demonstrated as: The tune/episode_reward_mean plot is essentialy the same as the figure plotted from data in the progress.csv file. The difference in the x-axis scale has a simple explanation. The episode_reward_mean column on the progress.csv file shows the reward progress on every training iteration, while the tune/episode_reward_mean plot on Tensorboard shows reward progress on every training episode (a single RLlib training iteration consists of thousands of episodes).","title":"Validating results using Tensorboard"},{"location":"Documentation/MachineLearning/TensorBoard/","text":"Validating ML results using Tensorboard Tensorboard provides visualization and tooling needed for machine learning, deep learning, and reinforcement learning experimentation: * Tracking and visualizing metrics such as loss and accuracy. * Visualizing the model graph (ops and layers). * Viewing histograms of weights, biases, or other tensors as they change over time. * Projecting embeddings to a lower dimensional space. * Displaying images, text, and audio data. * Profiling TensorFlow programs. For RL it is useful to visualize metrics such as: * Mean, min, and max reward values. * Episodes/iteration. * Estimated Q-values. * Algorithm-specific metrics (e.g. entropy for PPO). To visualize results from Tensorboard, first cd to the directory where your results reside. E.g., if you ran experiments using ray , then do the following: cd ~/ray_results/ There are three main methods for activating Tensorboard: * If you included Tensorboard installation in an Anaconda environment, simply activate it: module purge conda activate <your_environment> * You can also install Tensorboard in userspace using pip install : pip install tensorboard --user * Or, install using container images: ml singularity-container singularity pull docker://tensorflow/tensorflow singularity run tensorflow_latest.sif Then, initialize Tensorboard using a pre-specified port number of your choosing (e.g. 6006, 8008): tensorboard --logdir=. --port 6006 --bind_all If everything works properly, terminal will show: Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all TensorBoard 2.5.0 at http://localhost:6006/ (Press CTRL+C to quit) Open a new Terminal tab and create a tunnel: ssh -NfL 6006:localhost:6006 $USER@el1.hpc.nrel.gov Finally, open the above localhost url ( http://localhost:6006/ ) in a browser, where all the aforementioned plots will be shown.","title":"TensorBoard"},{"location":"Documentation/MachineLearning/TensorBoard/#validating-ml-results-using-tensorboard","text":"Tensorboard provides visualization and tooling needed for machine learning, deep learning, and reinforcement learning experimentation: * Tracking and visualizing metrics such as loss and accuracy. * Visualizing the model graph (ops and layers). * Viewing histograms of weights, biases, or other tensors as they change over time. * Projecting embeddings to a lower dimensional space. * Displaying images, text, and audio data. * Profiling TensorFlow programs. For RL it is useful to visualize metrics such as: * Mean, min, and max reward values. * Episodes/iteration. * Estimated Q-values. * Algorithm-specific metrics (e.g. entropy for PPO). To visualize results from Tensorboard, first cd to the directory where your results reside. E.g., if you ran experiments using ray , then do the following: cd ~/ray_results/ There are three main methods for activating Tensorboard: * If you included Tensorboard installation in an Anaconda environment, simply activate it: module purge conda activate <your_environment> * You can also install Tensorboard in userspace using pip install : pip install tensorboard --user * Or, install using container images: ml singularity-container singularity pull docker://tensorflow/tensorflow singularity run tensorflow_latest.sif Then, initialize Tensorboard using a pre-specified port number of your choosing (e.g. 6006, 8008): tensorboard --logdir=. --port 6006 --bind_all If everything works properly, terminal will show: Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all TensorBoard 2.5.0 at http://localhost:6006/ (Press CTRL+C to quit) Open a new Terminal tab and create a tunnel: ssh -NfL 6006:localhost:6006 $USER@el1.hpc.nrel.gov Finally, open the above localhost url ( http://localhost:6006/ ) in a browser, where all the aforementioned plots will be shown.","title":"Validating ML results using Tensorboard"},{"location":"Documentation/Software_Tools/conda/","text":"Why Conda? Conda is a package manager which allows you to easily create and switch betwen different software environments in different languages for different purposes. With Conda, it's easy to: Manage different (potentially conflicting) versions of the same software without complication Quickly stand up even complicated dependencies for stacks of software Share your specific programming environment with others for reproducible results Creating Environments by Name To create a basic Conda environment, we'll start by running conda create --name mypy python where the --name option (or the shortened -n ) means the environment will be specified by name and myenv will be the name of the created environment. Any arguments following the environment name are the packages to be installed. To specify a specific version of a package, simply add the version number after the \"=\" sign conda create --name mypy37 python=3.7 You can specify multiple packages for installation during environment creation conda create --name mynumpy python=3.7 numpy Conda ensures dependencies are satisfied when installing packages, so the version of the numpy package installed will be consistent with Python 3.7 (and any other packages specified). Tip: It\u2019s recommended to install all the packages you want to include in an environment at the same time to help avoid dependency conflicts. Environment Navigation To see a list of all existing environments (useful to confirm the successful creation of a new environment): conda env list To activate your new environment: conda activate mypy Your usual command prompt should now be prefixed with (mypy) , which helps keep track of which environment is currently activated. To see which packages are installed from within a currently active environment : conda list When finished with this programming session, deactivate your environment with: conda deactivate Creating Environments by Location Creating environments by location is especially helpful when working on the Eagle HPC, as the default location is your /home/<username>/ directory, which is limited to 50 GB. To create a Conda environment somewhere besides the default location, use the --prefix flag (or the shortened -p ) instead of --name when creating. conda create --prefix /path/to/mypy python=3.7 numpy This re-creates the python+numpy environment from earlier, but with all downloaded packages stored in the specified location. Warning: Keep in mind that scratch on Eagle is temporary in that files are purged after 28 days of inactivity. Unfortunately, placing environments outside of the default /env folder means that it needs to be activated with the full path ( conda activate /path/to/mypy ) and will show the full path rather than the environment name at the command prompt. To fix the cumbersome command prompt, simply modify the env_prompt setting in your .condarc file: conda config --set env_prompt '({name}) ' Note that '({name})' is not a placeholder for your desired environment name but text to be copied literally. This will edit your .condarc file if you already have one or create a .condarc file if you do not. For more on modifying your .condarc file, check out the User Guide . Once you've completed this step, the command prompt will show the shortened name (mypy, in the previous example). Managing Conda Environments Over time, it may become necessary to add additional packages to your environments. New packages can be installed in the currently active environment with: conda install pandas Conda will ensure that all dependencies are satisfied which may include upgrades to existing packages in this repository. To install packages from other sources, specify the channel option: conda install --channel conda-forge fenics To add a pip-installable package to your environment: conda install pip pip <pip_subcommand> A note on mixing Conda and Pip: Issues may arise when using pip and conda together. When combining conda and pip, it is best to use an isolated conda environment. Only after conda has been used to install as many packages as possible should pip be used to install any remaining software . If modifications are needed to the environment, it is best to create a new environment rather than running conda after pip. When appropriate, conda and pip requirements should be stored in text files. For more information on this point, check out the User Guide We can use conda list to see which packages are currently installed, but for a more version-control-flavored approach: conda list --revisions which shows changes to the environment over time. To revert back to a previous environemnt conda install --revision 1 To remove packages from the currently activated environment: conda remove pkg1 To completely remove an environment and all installed packages: conda remove --name mypy --all Conda environments can become large quickly due to the liberal creation of cached files. To remove these files and free up space you can use conda clean --all or to simply preview the potential changes before doing any actual deletion conda clean --all --dry-run Sharing Conda Environments To create a file with the the exact \"recipe\" used to create the current environment: conda env export > environment.yaml In practice, this recipe may be overly-specific to the point of creating problems on different hardware. To save an abbreviated version of the recipe with only the packages you explicitly requested : conda env export --from-history > environment.yaml To create a new environment with the recipe specified in the .yaml file: conda env create --name mypyeagle --file environment.yaml If a name or prefix isn't specified, the environment will be given the same name as the original environment the recipe was exported from (which may be desirable if you're moving to a different computer). Speed up dependency solving To speed up dependency solving, substitute the mamba command for conda. Mamba is a dependency solver written in C++ designed to speed up the conda environment solve. mamba create --prefix /path/to/mypy python=3.7 numpy Reduce home directory usage By default, the conda module uses the home directory for package caches and named environments. This results in a lot of the home directory quota used. Some ways to reduce home directory usage include: Use the -p PATH_NAME switch when creating or updating your environment. Make sure PATH_NAME isn't in the home directory. Keep in mind files in /scratch are deleted after about a month of inactivity. Change the directory used for caching. This location is set by the module file to ~/.conda-pkgs. Calling export CONDA_PKGS_DIRS=PATH_NAME to somewhere to store downloads and cached files such as /scratch/$USER/.conda-pkgs will reduce home directory usage. Eagle Considerations Interacting with your Conda environments on Eagle should feel exactly the same as working on your desktop. An example desktop-to-HPC workflow might go: Create the environment locally Verify that environment works on a minimal working example Export local environment file and copy to Eagle Duplicate local environment on Eagle Execute production-level runs on Eagle #!/bin/bash #SBATCH --ntasks=4 #SBATCH --nodes=1 #SBATCH --time=5 #SBATCH --account=<project_handle> module purge module load conda conda activate mypy srun -n 8 python my_main.py Cheat Sheet of Common Commands Task ... outside environment ... inside environment Create by name conda create -n mypy pkg1 pkg2 N/A Create by path conda create -p path/to/mypy pkg1 pkg2 N/A Create by file conda env create -f environment.yml N/A Show environments conda env list N/A Activate conda activate mypy N/A Deactivate N/A conda deactivate Install New Package conda install -n mypy pkg1 pkg2 conda install pkg1 pkg2 List All Packages conda list -n mypy conda list Revision Listing conda list --revisions -n mypy conda list --revisions Export Environment conda env export -n mypy > environment.yaml conda env export > environment.yaml Remove Package conda remove -n mypy pkg1 pkg2 conda remove pkg1 pkg2","title":"Conda"},{"location":"Documentation/Software_Tools/conda/#why-conda","text":"Conda is a package manager which allows you to easily create and switch betwen different software environments in different languages for different purposes. With Conda, it's easy to: Manage different (potentially conflicting) versions of the same software without complication Quickly stand up even complicated dependencies for stacks of software Share your specific programming environment with others for reproducible results","title":"Why Conda?"},{"location":"Documentation/Software_Tools/conda/#creating-environments-by-name","text":"To create a basic Conda environment, we'll start by running conda create --name mypy python where the --name option (or the shortened -n ) means the environment will be specified by name and myenv will be the name of the created environment. Any arguments following the environment name are the packages to be installed. To specify a specific version of a package, simply add the version number after the \"=\" sign conda create --name mypy37 python=3.7 You can specify multiple packages for installation during environment creation conda create --name mynumpy python=3.7 numpy Conda ensures dependencies are satisfied when installing packages, so the version of the numpy package installed will be consistent with Python 3.7 (and any other packages specified). Tip: It\u2019s recommended to install all the packages you want to include in an environment at the same time to help avoid dependency conflicts.","title":"Creating Environments by Name"},{"location":"Documentation/Software_Tools/conda/#environment-navigation","text":"To see a list of all existing environments (useful to confirm the successful creation of a new environment): conda env list To activate your new environment: conda activate mypy Your usual command prompt should now be prefixed with (mypy) , which helps keep track of which environment is currently activated. To see which packages are installed from within a currently active environment : conda list When finished with this programming session, deactivate your environment with: conda deactivate","title":"Environment Navigation"},{"location":"Documentation/Software_Tools/conda/#creating-environments-by-location","text":"Creating environments by location is especially helpful when working on the Eagle HPC, as the default location is your /home/<username>/ directory, which is limited to 50 GB. To create a Conda environment somewhere besides the default location, use the --prefix flag (or the shortened -p ) instead of --name when creating. conda create --prefix /path/to/mypy python=3.7 numpy This re-creates the python+numpy environment from earlier, but with all downloaded packages stored in the specified location. Warning: Keep in mind that scratch on Eagle is temporary in that files are purged after 28 days of inactivity. Unfortunately, placing environments outside of the default /env folder means that it needs to be activated with the full path ( conda activate /path/to/mypy ) and will show the full path rather than the environment name at the command prompt. To fix the cumbersome command prompt, simply modify the env_prompt setting in your .condarc file: conda config --set env_prompt '({name}) ' Note that '({name})' is not a placeholder for your desired environment name but text to be copied literally. This will edit your .condarc file if you already have one or create a .condarc file if you do not. For more on modifying your .condarc file, check out the User Guide . Once you've completed this step, the command prompt will show the shortened name (mypy, in the previous example).","title":"Creating Environments by Location"},{"location":"Documentation/Software_Tools/conda/#managing-conda-environments","text":"Over time, it may become necessary to add additional packages to your environments. New packages can be installed in the currently active environment with: conda install pandas Conda will ensure that all dependencies are satisfied which may include upgrades to existing packages in this repository. To install packages from other sources, specify the channel option: conda install --channel conda-forge fenics To add a pip-installable package to your environment: conda install pip pip <pip_subcommand> A note on mixing Conda and Pip: Issues may arise when using pip and conda together. When combining conda and pip, it is best to use an isolated conda environment. Only after conda has been used to install as many packages as possible should pip be used to install any remaining software . If modifications are needed to the environment, it is best to create a new environment rather than running conda after pip. When appropriate, conda and pip requirements should be stored in text files. For more information on this point, check out the User Guide We can use conda list to see which packages are currently installed, but for a more version-control-flavored approach: conda list --revisions which shows changes to the environment over time. To revert back to a previous environemnt conda install --revision 1 To remove packages from the currently activated environment: conda remove pkg1 To completely remove an environment and all installed packages: conda remove --name mypy --all Conda environments can become large quickly due to the liberal creation of cached files. To remove these files and free up space you can use conda clean --all or to simply preview the potential changes before doing any actual deletion conda clean --all --dry-run","title":"Managing Conda Environments"},{"location":"Documentation/Software_Tools/conda/#sharing-conda-environments","text":"To create a file with the the exact \"recipe\" used to create the current environment: conda env export > environment.yaml In practice, this recipe may be overly-specific to the point of creating problems on different hardware. To save an abbreviated version of the recipe with only the packages you explicitly requested : conda env export --from-history > environment.yaml To create a new environment with the recipe specified in the .yaml file: conda env create --name mypyeagle --file environment.yaml If a name or prefix isn't specified, the environment will be given the same name as the original environment the recipe was exported from (which may be desirable if you're moving to a different computer).","title":"Sharing Conda Environments"},{"location":"Documentation/Software_Tools/conda/#speed-up-dependency-solving","text":"To speed up dependency solving, substitute the mamba command for conda. Mamba is a dependency solver written in C++ designed to speed up the conda environment solve. mamba create --prefix /path/to/mypy python=3.7 numpy","title":"Speed up dependency solving"},{"location":"Documentation/Software_Tools/conda/#reduce-home-directory-usage","text":"By default, the conda module uses the home directory for package caches and named environments. This results in a lot of the home directory quota used. Some ways to reduce home directory usage include: Use the -p PATH_NAME switch when creating or updating your environment. Make sure PATH_NAME isn't in the home directory. Keep in mind files in /scratch are deleted after about a month of inactivity. Change the directory used for caching. This location is set by the module file to ~/.conda-pkgs. Calling export CONDA_PKGS_DIRS=PATH_NAME to somewhere to store downloads and cached files such as /scratch/$USER/.conda-pkgs will reduce home directory usage.","title":"Reduce home directory usage"},{"location":"Documentation/Software_Tools/conda/#eagle-considerations","text":"Interacting with your Conda environments on Eagle should feel exactly the same as working on your desktop. An example desktop-to-HPC workflow might go: Create the environment locally Verify that environment works on a minimal working example Export local environment file and copy to Eagle Duplicate local environment on Eagle Execute production-level runs on Eagle #!/bin/bash #SBATCH --ntasks=4 #SBATCH --nodes=1 #SBATCH --time=5 #SBATCH --account=<project_handle> module purge module load conda conda activate mypy srun -n 8 python my_main.py","title":"Eagle Considerations"},{"location":"Documentation/Software_Tools/conda/#cheat-sheet-of-common-commands","text":"Task ... outside environment ... inside environment Create by name conda create -n mypy pkg1 pkg2 N/A Create by path conda create -p path/to/mypy pkg1 pkg2 N/A Create by file conda env create -f environment.yml N/A Show environments conda env list N/A Activate conda activate mypy N/A Deactivate N/A conda deactivate Install New Package conda install -n mypy pkg1 pkg2 conda install pkg1 pkg2 List All Packages conda list -n mypy conda list Revision Listing conda list --revisions -n mypy conda list --revisions Export Environment conda env export -n mypy > environment.yaml conda env export > environment.yaml Remove Package conda remove -n mypy pkg1 pkg2 conda remove pkg1 pkg2","title":"Cheat Sheet of Common Commands"},{"location":"Documentation/Software_Tools/spack/","text":"Introduction Spack is an HPC-centric package manager for acquiring, building, and managing HPC applications as well as all their dependencies, down to the compilers themselves. Like frameworks such as Anaconda, it is associated with a repository of both source-code and binary packages. Builds are fully configurable through a DSL at the command line as well as in YAML files. Maintaining many build-time permutations of packages is simple through an automatic and user-transparent hashing mechanism. The Spack system also automatically creates (customizable) environment modulefiles for each built package. Installation Multiple installations of Spack can easily be kept, and each is separate from the others by virtue of the environment variable SPACK_ROOT . All package, build, and modulefile content is kept inside the SPACK_ROOT path, so working with different package collections is as simple as setting SPACK_ROOT to the appropriate location. The only exception to this orthogonality are YAML files in $HOME/.spack/<platform> . Installing a Spack instance is as easy as git clone https://github.com/spack/spack.git Once the initial Spack instance is set up, it is easy to create new ones from it through spack clone <new_path> SPACK_ROOT will need to point to <new_path> in order to be consistent. Spack environment setup can be done by sourcing $SPACK_ROOT/share/spack/setup-env.sh , or by simply adding $SPACK_ROOT/bin to your PATH. source $SPACK_ROOT/share/spack/setup-env.sh or export PATH=$SPACK_ROOT/bin:$PATH Setting Up Compilers Spack is able to find certain compilers on its own, and will add them to your environment as it does. In order to obtain the list of available compilers on Eagle the user can run module avail , the user can then load the compiler of interest using module use <compiler> . To see which compilers your Spack collections know about, type spack compilers To add an existing compiler installation to your collection, point Spack to its location through spack add compiler <path to Spack-installed compiler directory with hash in name> The command will add to $HOME/.spack/linux/compilers.yaml . To configure more generally, move changes to one of the lower-precedence compilers.yaml files (paths described below in Configuration section). Spack has enough facility with standard compilers (e.g., GCC, Intel, PGI, Clang) that this should be all that\u2019s required to use the added compiler successfully. Available Packages in Repo Command Description spack list all available packages by name. Dumps repo content, so if use local repo, this should dump local package load. spack list <pattern> all available packages that have <pattern> somewhere in their name. <pattern> is simple, not regex. spack info <package_name> available versions classified as safe, preferred, or variants, as well as dependencies. Variants are important for selecting certain build features, e.g., with/without Infiniband support. spack versions <package_name> see which versions are available Installed packages Command Description spack find list all locally installed packages spack find --deps <package> list dependencies of <package> spack find --explicit list packages that were explicitly requested via spack install spack find --implicit list packages that were installed as a dependency to an explicitly installed package spack find --long include partial hash in package listing. Useful to see distinct builds spack find --paths show installation paths Finding how an installed package was built does not seem as straightforward as it should be. Probably the best way is to examine <install_path>/.spack/build.env , where <install_path> is the Spack-created directory with the hash for the package being queried. The environment variable SPACK_SHORT_SPEC in build.env contains the Spack command that can be used to recreate the package (including any implicitly defined variables, e.g., arch). The 7-character short hash is also included, and should be excluded from any spack install command. Symbols Description @ package versions. Can use range operator \u201c:\u201d, e.g., X@1.2:1.4 . Range is inclusive and open-ended, e.g., \u201cX@1.4:\u201d matches any version of package X 1.4 or higher. % compiler spec. Can include versioning, e.g., X%gcc@4.8.5 +,-,~ build options. +opt, -opt, \u201c~\u201d is equivalent to \u201c-\u201c name=value build options for non-Boolean flags. Special names are cflags, cxxflags, fflags, cppflags, ldflags, and ldlibs target=value for defined CPU architectures, e.g., target=haswell os=value for defined operating systems ^ dependency specification, using above specs as appropriate ^/<hash> specify dependency where <hash> is of sufficient length to resolve uniquely External Packages Sometimes dependencies are expected to be resolved through a package that is installed as part of the host system, or otherwise outside of the Spack database. One example is Slurm integration into MPI builds. If you were to try to add a dependency on one of the listed Slurms in the Spack database, you might see, e.g., [ $user @el2 ~ ] $ spack spec openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-3-2 Input spec -------------------------------- openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-3-2 Concretized -------------------------------- == > Error: The spec 'slurm' is configured as not buildable, and no matching external installs were found Given that something like Slurm is integrated deeply into the runtime infrastructure of our local environment, we really want to point to the local installation. The way to do that is with a packages.yaml file, which can reside in the standard Spack locations (see Configuration below). See the Spack docs on external packages for more detail. In the above example at time of writing, we would like to build OpenMPI against our installed Slurm 19.05.2 . So, you can create file ~/.spack/linux/packages.yaml with the contents packages : slurm : paths : slurm@18-08-0-3 : /nopt/slurm/18.08.3 slurm@19-05-0-2 : /nopt/slurm/19.05.2 that will enable builds against both installed Slurm versions. Then you should see [ $user @el2 ~ ] $ spack spec openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-0-2 Input spec -------------------------------- openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-0-2 Concretized -------------------------------- openmpi@3.1.3%gcc@7.3.0 cflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" cxxflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" fflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" +cuda+cxx_exceptions fabrics = verbs ~java~legacylaunchers~memchecker+pmi schedulers = slurm ~sqlite3~thread_multiple+vt arch = linux-centos7-x86_64 - ^slurm@19-05-0-2%gcc@7.3.0 cflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" cxxflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" fflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" ~gtk~hdf5~hwloc~mariadb+readline arch = linux-centos7-x86_64 where the Slurm dependency will be satisfied with the installed Slurm (cflags, cxxflags, and arch are coming from site-wide configuration in /nopt/nrel/apps/base/2018-12-02/spack/etc/spack/compilers.yaml ; the variants string is likely coming from the configuration in the Spack database, and should be ignored). Virtual Packages It is possible to specify some packages for which multiple options are available at a higher level. For example, mpi is a virtual package specifier that can resolve to mpich, openmpi, Intel MPI, etc. If a package's dependencies are spec'd in terms of a virtual package, Spack will choose a specific package at build time according to site preferences. Choices can be constrained by spec, e.g., spack install X ^mpich@3 would satisfy package X\u2019s mpi dependency with some version 3 of MPICH. You can see available providers of a virtual package with spack providers <vpackage> Extensions In many cases, frameworks have sub-package installations in standard locations within their own installations. A familiar example of this is Python and its usual module location in lib(64)/python<version>/site-packages , and pointed to via the environment variable PYTHONPATH . To find available extensions spack extensions <package> Extensions are just packages, but they are not enabled for use out of the box. To do so (e.g., so that you could load the Python module after installing), you can either load the extension package\u2019s environment module, or spack use <extension package> This only lasts for the current session, and is not of general interest. A more persistent option is to activate the extension: spack activate <extension package> This takes care of dependencies as well. The inverse operation is deactivation. Command Description spack deactivate <extension package> deactivates extension alone. Will not deactivate if dependents exist spack deactivate --force <extension package> deactivates regardless of dependents spack deactivate --all <extension package> deactivates extension and all dependencies spack deactivate --all <parent> deactivates all extensions of parent (e.g., <python> ) Modules Spack can auto-create environment modulefiles for the packages that it builds, both in Tcl for \u201cenvironment modules\u201d per se, and in Lua for Lmod. Auto-creation includes each dependency and option permutation, which can lead to excessive quantities of modulefiles. Spack also uses the package hash as part of the modulefile name, which can be somewhat disconcerting to users. These default behaviors can be treated in the active modules.yaml file, as well as practices used for support. Tcl modulefiles are created in $SPACK_ROOT/share/spack/modules by default, and the equivalent Lmod location is $SPACK_ROOT/share/spack/lmod . Only Tcl modules are created by default. You can modify the active modules.yaml file in the following ways to affect some example behaviors: To turn Lmod module creation on: modules: enable: - tcl - lmod To change the modulefile naming pattern: modules: tcl: naming_scheme: \u2018{name}/{version}/{compiler.name}-{compiler.version} would achieve the Eagle naming scheme. To remove default variable settings in the modulefile, e.g., CPATH: modules: tcl: all: filter: environment_blacklist: [\u2018CPATH\u2019] Note that this would affect Tcl modulefiles only; if Spack also creates Lmod files, those would still contain default CPATH modification behavior. To prevent certain modulefiles from being built, you can whitelist and blacklist: modules: tcl: whitelist: [\u2018gcc\u2019] blacklist: [\u2018%gcc@4.8.5\u2019] This would create modules for all versions of GCC built using the system compiler, but not for the system compiler itself. There are a great many further behaviors that can be changed, see https://spack.readthedocs.io/en/latest/module_file_support.html#modules for more. For general user support, it is not a bad idea to keep the modules that are publicly visible separate from the collection that Spack auto-generates. This involves some manual copying, but is generally not onerous as all rpaths are included in Spack-built binaries (i.e., you don\u2019t have to worry about satisfying library dependencies for Spack applications with an auto-built module, since library paths are hard-coded into the application binaries). This separation also frees one from accepting Spack\u2019s verbose coding formats within modulefiles, should you decide to maintain certain modulefiles another way. Configuration Spack uses hierarchical customization files. Every package is a Python class, and inherits from the top-level class Package. Depending on the degree of site customization, you may want to fork the Spack repo to create your own customized Spack package. There are 4 levels of configuration. In order of increasing precedence, Default: $SPACK_ROOT/etc/spack/default System-wide: /etc/spack Site-wide: $SPACK_ROOT/etc/spack User-specific: $HOME/.spack Spack configuration uses YAML files, a subset of JSON native to Python. There are 5 main configuration files. compilers.yaml . Customizations to the Spack-known compilers for all builds i. Use full path to compilers ii. Additional rpaths beyond the Spack repo iii. Additional modules necessary when invoking compilers iv. Mixing toolchains v. Optimization flags vi. Environment modifications config.yaml . Base functionality of Spack itself i. install_tree: where to install packages ii. build_stage: where to do compiles. For performance, can specify a local SSD or a RAMFS. iii. modules_roots: where to install modulefiles modules.yaml . How to create modulefiles i. whitelist/blacklist packages from having their own modulefiles created ii. adjust hierarchies packages.yaml . Specific optimizations, such as multiple hardware targets. i. dependencies, e.g., don\u2019t build OpenSSL (usually want sysadmins to handle updates, etc.) ii. mark specific packages as non-buildable, e.g., vendor MPIs iii. preferences, e.g., BLAS -> MKL, LAPACK -> MKL repos.yaml i. Directory-housed, not remote ii. Specify other package locations iii. Can then spec build in other configs (e.g., binary, don\u2019t build) iv. Precedence in YAML file order, but follows Spack precedence order (user > site > system > default) Variants: standard adjustments to package build spack edit \u2026 -- opens Python file for package, can easily write new variants Providers spack providers -- virtual packages, e.g., blas, mpi, etc. Standards, not implementations. Abstraction of an implementation (blas/mkl, mpi/mpich, etc.) Mirrors mirrors.yaml: where packages are kept A repo is where build information is kept; a mirror is where code lives MirrorTopLevel package_a package_a-version1.tar.gz package_a-version2.tar.gz package_b \u22ee spack mirror to manage mirrors Repos Can take precedence from, e.g., a site repo Can namespace packages repo.yaml alpha hotfix-patch-ABC.patch package.py package.pyc beta theta","title":"Spack"},{"location":"Documentation/Software_Tools/spack/#introduction","text":"Spack is an HPC-centric package manager for acquiring, building, and managing HPC applications as well as all their dependencies, down to the compilers themselves. Like frameworks such as Anaconda, it is associated with a repository of both source-code and binary packages. Builds are fully configurable through a DSL at the command line as well as in YAML files. Maintaining many build-time permutations of packages is simple through an automatic and user-transparent hashing mechanism. The Spack system also automatically creates (customizable) environment modulefiles for each built package.","title":"Introduction"},{"location":"Documentation/Software_Tools/spack/#installation","text":"Multiple installations of Spack can easily be kept, and each is separate from the others by virtue of the environment variable SPACK_ROOT . All package, build, and modulefile content is kept inside the SPACK_ROOT path, so working with different package collections is as simple as setting SPACK_ROOT to the appropriate location. The only exception to this orthogonality are YAML files in $HOME/.spack/<platform> . Installing a Spack instance is as easy as git clone https://github.com/spack/spack.git Once the initial Spack instance is set up, it is easy to create new ones from it through spack clone <new_path> SPACK_ROOT will need to point to <new_path> in order to be consistent. Spack environment setup can be done by sourcing $SPACK_ROOT/share/spack/setup-env.sh , or by simply adding $SPACK_ROOT/bin to your PATH. source $SPACK_ROOT/share/spack/setup-env.sh or export PATH=$SPACK_ROOT/bin:$PATH","title":"Installation"},{"location":"Documentation/Software_Tools/spack/#setting-up-compilers","text":"Spack is able to find certain compilers on its own, and will add them to your environment as it does. In order to obtain the list of available compilers on Eagle the user can run module avail , the user can then load the compiler of interest using module use <compiler> . To see which compilers your Spack collections know about, type spack compilers To add an existing compiler installation to your collection, point Spack to its location through spack add compiler <path to Spack-installed compiler directory with hash in name> The command will add to $HOME/.spack/linux/compilers.yaml . To configure more generally, move changes to one of the lower-precedence compilers.yaml files (paths described below in Configuration section). Spack has enough facility with standard compilers (e.g., GCC, Intel, PGI, Clang) that this should be all that\u2019s required to use the added compiler successfully.","title":"Setting Up Compilers"},{"location":"Documentation/Software_Tools/spack/#available-packages-in-repo","text":"Command Description spack list all available packages by name. Dumps repo content, so if use local repo, this should dump local package load. spack list <pattern> all available packages that have <pattern> somewhere in their name. <pattern> is simple, not regex. spack info <package_name> available versions classified as safe, preferred, or variants, as well as dependencies. Variants are important for selecting certain build features, e.g., with/without Infiniband support. spack versions <package_name> see which versions are available","title":"Available Packages in Repo"},{"location":"Documentation/Software_Tools/spack/#installed-packages","text":"Command Description spack find list all locally installed packages spack find --deps <package> list dependencies of <package> spack find --explicit list packages that were explicitly requested via spack install spack find --implicit list packages that were installed as a dependency to an explicitly installed package spack find --long include partial hash in package listing. Useful to see distinct builds spack find --paths show installation paths Finding how an installed package was built does not seem as straightforward as it should be. Probably the best way is to examine <install_path>/.spack/build.env , where <install_path> is the Spack-created directory with the hash for the package being queried. The environment variable SPACK_SHORT_SPEC in build.env contains the Spack command that can be used to recreate the package (including any implicitly defined variables, e.g., arch). The 7-character short hash is also included, and should be excluded from any spack install command. Symbols Description @ package versions. Can use range operator \u201c:\u201d, e.g., X@1.2:1.4 . Range is inclusive and open-ended, e.g., \u201cX@1.4:\u201d matches any version of package X 1.4 or higher. % compiler spec. Can include versioning, e.g., X%gcc@4.8.5 +,-,~ build options. +opt, -opt, \u201c~\u201d is equivalent to \u201c-\u201c name=value build options for non-Boolean flags. Special names are cflags, cxxflags, fflags, cppflags, ldflags, and ldlibs target=value for defined CPU architectures, e.g., target=haswell os=value for defined operating systems ^ dependency specification, using above specs as appropriate ^/<hash> specify dependency where <hash> is of sufficient length to resolve uniquely","title":"Installed packages"},{"location":"Documentation/Software_Tools/spack/#external-packages","text":"Sometimes dependencies are expected to be resolved through a package that is installed as part of the host system, or otherwise outside of the Spack database. One example is Slurm integration into MPI builds. If you were to try to add a dependency on one of the listed Slurms in the Spack database, you might see, e.g., [ $user @el2 ~ ] $ spack spec openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-3-2 Input spec -------------------------------- openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-3-2 Concretized -------------------------------- == > Error: The spec 'slurm' is configured as not buildable, and no matching external installs were found Given that something like Slurm is integrated deeply into the runtime infrastructure of our local environment, we really want to point to the local installation. The way to do that is with a packages.yaml file, which can reside in the standard Spack locations (see Configuration below). See the Spack docs on external packages for more detail. In the above example at time of writing, we would like to build OpenMPI against our installed Slurm 19.05.2 . So, you can create file ~/.spack/linux/packages.yaml with the contents packages : slurm : paths : slurm@18-08-0-3 : /nopt/slurm/18.08.3 slurm@19-05-0-2 : /nopt/slurm/19.05.2 that will enable builds against both installed Slurm versions. Then you should see [ $user @el2 ~ ] $ spack spec openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-0-2 Input spec -------------------------------- openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-0-2 Concretized -------------------------------- openmpi@3.1.3%gcc@7.3.0 cflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" cxxflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" fflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" +cuda+cxx_exceptions fabrics = verbs ~java~legacylaunchers~memchecker+pmi schedulers = slurm ~sqlite3~thread_multiple+vt arch = linux-centos7-x86_64 - ^slurm@19-05-0-2%gcc@7.3.0 cflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" cxxflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" fflags = \"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" ~gtk~hdf5~hwloc~mariadb+readline arch = linux-centos7-x86_64 where the Slurm dependency will be satisfied with the installed Slurm (cflags, cxxflags, and arch are coming from site-wide configuration in /nopt/nrel/apps/base/2018-12-02/spack/etc/spack/compilers.yaml ; the variants string is likely coming from the configuration in the Spack database, and should be ignored).","title":"External Packages"},{"location":"Documentation/Software_Tools/spack/#virtual-packages","text":"It is possible to specify some packages for which multiple options are available at a higher level. For example, mpi is a virtual package specifier that can resolve to mpich, openmpi, Intel MPI, etc. If a package's dependencies are spec'd in terms of a virtual package, Spack will choose a specific package at build time according to site preferences. Choices can be constrained by spec, e.g., spack install X ^mpich@3 would satisfy package X\u2019s mpi dependency with some version 3 of MPICH. You can see available providers of a virtual package with spack providers <vpackage>","title":"Virtual Packages"},{"location":"Documentation/Software_Tools/spack/#extensions","text":"In many cases, frameworks have sub-package installations in standard locations within their own installations. A familiar example of this is Python and its usual module location in lib(64)/python<version>/site-packages , and pointed to via the environment variable PYTHONPATH . To find available extensions spack extensions <package> Extensions are just packages, but they are not enabled for use out of the box. To do so (e.g., so that you could load the Python module after installing), you can either load the extension package\u2019s environment module, or spack use <extension package> This only lasts for the current session, and is not of general interest. A more persistent option is to activate the extension: spack activate <extension package> This takes care of dependencies as well. The inverse operation is deactivation. Command Description spack deactivate <extension package> deactivates extension alone. Will not deactivate if dependents exist spack deactivate --force <extension package> deactivates regardless of dependents spack deactivate --all <extension package> deactivates extension and all dependencies spack deactivate --all <parent> deactivates all extensions of parent (e.g., <python> )","title":"Extensions"},{"location":"Documentation/Software_Tools/spack/#modules","text":"Spack can auto-create environment modulefiles for the packages that it builds, both in Tcl for \u201cenvironment modules\u201d per se, and in Lua for Lmod. Auto-creation includes each dependency and option permutation, which can lead to excessive quantities of modulefiles. Spack also uses the package hash as part of the modulefile name, which can be somewhat disconcerting to users. These default behaviors can be treated in the active modules.yaml file, as well as practices used for support. Tcl modulefiles are created in $SPACK_ROOT/share/spack/modules by default, and the equivalent Lmod location is $SPACK_ROOT/share/spack/lmod . Only Tcl modules are created by default. You can modify the active modules.yaml file in the following ways to affect some example behaviors:","title":"Modules"},{"location":"Documentation/Software_Tools/spack/#to-turn-lmod-module-creation-on","text":"modules: enable: - tcl - lmod","title":"To turn Lmod module creation on:"},{"location":"Documentation/Software_Tools/spack/#to-change-the-modulefile-naming-pattern","text":"modules: tcl: naming_scheme: \u2018{name}/{version}/{compiler.name}-{compiler.version} would achieve the Eagle naming scheme.","title":"To change the modulefile naming pattern:"},{"location":"Documentation/Software_Tools/spack/#to-remove-default-variable-settings-in-the-modulefile-eg-cpath","text":"modules: tcl: all: filter: environment_blacklist: [\u2018CPATH\u2019] Note that this would affect Tcl modulefiles only; if Spack also creates Lmod files, those would still contain default CPATH modification behavior.","title":"To remove default variable settings in the modulefile, e.g., CPATH:"},{"location":"Documentation/Software_Tools/spack/#to-prevent-certain-modulefiles-from-being-built-you-can-whitelist-and-blacklist","text":"modules: tcl: whitelist: [\u2018gcc\u2019] blacklist: [\u2018%gcc@4.8.5\u2019] This would create modules for all versions of GCC built using the system compiler, but not for the system compiler itself. There are a great many further behaviors that can be changed, see https://spack.readthedocs.io/en/latest/module_file_support.html#modules for more. For general user support, it is not a bad idea to keep the modules that are publicly visible separate from the collection that Spack auto-generates. This involves some manual copying, but is generally not onerous as all rpaths are included in Spack-built binaries (i.e., you don\u2019t have to worry about satisfying library dependencies for Spack applications with an auto-built module, since library paths are hard-coded into the application binaries). This separation also frees one from accepting Spack\u2019s verbose coding formats within modulefiles, should you decide to maintain certain modulefiles another way.","title":"To prevent certain modulefiles from being built, you can whitelist and blacklist:"},{"location":"Documentation/Software_Tools/spack/#configuration","text":"Spack uses hierarchical customization files. Every package is a Python class, and inherits from the top-level class Package. Depending on the degree of site customization, you may want to fork the Spack repo to create your own customized Spack package. There are 4 levels of configuration. In order of increasing precedence, Default: $SPACK_ROOT/etc/spack/default System-wide: /etc/spack Site-wide: $SPACK_ROOT/etc/spack User-specific: $HOME/.spack Spack configuration uses YAML files, a subset of JSON native to Python. There are 5 main configuration files. compilers.yaml . Customizations to the Spack-known compilers for all builds i. Use full path to compilers ii. Additional rpaths beyond the Spack repo iii. Additional modules necessary when invoking compilers iv. Mixing toolchains v. Optimization flags vi. Environment modifications config.yaml . Base functionality of Spack itself i. install_tree: where to install packages ii. build_stage: where to do compiles. For performance, can specify a local SSD or a RAMFS. iii. modules_roots: where to install modulefiles modules.yaml . How to create modulefiles i. whitelist/blacklist packages from having their own modulefiles created ii. adjust hierarchies packages.yaml . Specific optimizations, such as multiple hardware targets. i. dependencies, e.g., don\u2019t build OpenSSL (usually want sysadmins to handle updates, etc.) ii. mark specific packages as non-buildable, e.g., vendor MPIs iii. preferences, e.g., BLAS -> MKL, LAPACK -> MKL repos.yaml i. Directory-housed, not remote ii. Specify other package locations iii. Can then spec build in other configs (e.g., binary, don\u2019t build) iv. Precedence in YAML file order, but follows Spack precedence order (user > site > system > default)","title":"Configuration"},{"location":"Documentation/Software_Tools/spack/#variants-standard-adjustments-to-package-build","text":"spack edit \u2026 -- opens Python file for package, can easily write new variants","title":"Variants: standard adjustments to package build"},{"location":"Documentation/Software_Tools/spack/#providers","text":"spack providers -- virtual packages, e.g., blas, mpi, etc. Standards, not implementations. Abstraction of an implementation (blas/mkl, mpi/mpich, etc.)","title":"Providers"},{"location":"Documentation/Software_Tools/spack/#mirrors","text":"mirrors.yaml: where packages are kept A repo is where build information is kept; a mirror is where code lives MirrorTopLevel package_a package_a-version1.tar.gz package_a-version2.tar.gz package_b \u22ee spack mirror to manage mirrors","title":"Mirrors"},{"location":"Documentation/Software_Tools/spack/#repos","text":"Can take precedence from, e.g., a site repo Can namespace packages repo.yaml alpha hotfix-patch-ABC.patch package.py package.pyc beta theta","title":"Repos"},{"location":"Documentation/Software_Tools/Containers/","text":"Introduction to containers What are containers? Containers provide a method of packaging your code so that it can be run anywhere you have a container runtime. This enables you to create a container on your local laptop and then run it on Eagle or other computing resources. Containers provide an alternative way of isolating and packaging your code from solutions such as Conda environments. Docker vs. Singularity The most common container runtime outside of HPC is Docker. Docker is not suited for the HPC environment on Eagle and is therefore not available on the system currently. Singularity is an alternative container tool which is provided. Compatibility Singularity is able to run most Docker images, but Docker is unable to run Singularity images. A key consideration when deciding to containerize an application is which container engine to build with. A suggested best practice is to build images with Docker when possible, as this provides more flexibility. Sometimes this is not possible though, and you may have to build with Singularity or maintain separate builds for each container engine. Container advantages Portability : containers can be run on HPC, locally, and on cloud infrastructure used at NREL. Reproducibility : Containers are one option to ensure reproducible research by packaging all necessary software to reproduce an analysis. Containers are also easily versioned using a hash. Workflow integration : Workflow management systems such as Airflow, Nextflow, Luigi, and others provide built in integration with container engines. HPC hardware Both Singularity and Docker provide the ability to use hardware based features of Eagle. A common usage for containers is packaging of GPU enabled tools such as TensorFlow. Singularity enables access to the GPU and driver on the host. Likewise the MPI installations available on Eagle can be accessed from correctly configured containers. Building Containers are built from a container specification file, Dockerfiles for Docker or Singularity Definition File in Singularity. These files specify the steps necessary to create the desired package and the additional software packages to install and configure in this environment. FROM ubuntu:20.04 RUN apt-get -y update && apt-get install -y python3 The above Dockerfile illustrates the build steps to create a simple image. Images are normally built from a base image indicated by FROM , in this case Ubuntu. The ability to use a different base image provides a way to use packages which may work more easily on one Linux Distribution. For example the Linux distribution on Eagle is CentOS, building the above image would allow the user to install packages from Ubuntu repositories. The RUN portion of the above Dockerfile indicates the command to run, in this example it installs the Python 3 package. Additional commands such as COPY , ENV , and others enable the customization of your image to suit your compute environment requirements.","title":"Containers Intro"},{"location":"Documentation/Software_Tools/Containers/#introduction-to-containers","text":"","title":"Introduction to containers"},{"location":"Documentation/Software_Tools/Containers/#what-are-containers","text":"Containers provide a method of packaging your code so that it can be run anywhere you have a container runtime. This enables you to create a container on your local laptop and then run it on Eagle or other computing resources. Containers provide an alternative way of isolating and packaging your code from solutions such as Conda environments.","title":"What are containers?"},{"location":"Documentation/Software_Tools/Containers/#docker-vs-singularity","text":"The most common container runtime outside of HPC is Docker. Docker is not suited for the HPC environment on Eagle and is therefore not available on the system currently. Singularity is an alternative container tool which is provided.","title":"Docker vs. Singularity"},{"location":"Documentation/Software_Tools/Containers/#compatibility","text":"Singularity is able to run most Docker images, but Docker is unable to run Singularity images. A key consideration when deciding to containerize an application is which container engine to build with. A suggested best practice is to build images with Docker when possible, as this provides more flexibility. Sometimes this is not possible though, and you may have to build with Singularity or maintain separate builds for each container engine.","title":"Compatibility"},{"location":"Documentation/Software_Tools/Containers/#container-advantages","text":"Portability : containers can be run on HPC, locally, and on cloud infrastructure used at NREL. Reproducibility : Containers are one option to ensure reproducible research by packaging all necessary software to reproduce an analysis. Containers are also easily versioned using a hash. Workflow integration : Workflow management systems such as Airflow, Nextflow, Luigi, and others provide built in integration with container engines.","title":"Container advantages"},{"location":"Documentation/Software_Tools/Containers/#hpc-hardware","text":"Both Singularity and Docker provide the ability to use hardware based features of Eagle. A common usage for containers is packaging of GPU enabled tools such as TensorFlow. Singularity enables access to the GPU and driver on the host. Likewise the MPI installations available on Eagle can be accessed from correctly configured containers.","title":"HPC hardware"},{"location":"Documentation/Software_Tools/Containers/#building","text":"Containers are built from a container specification file, Dockerfiles for Docker or Singularity Definition File in Singularity. These files specify the steps necessary to create the desired package and the additional software packages to install and configure in this environment. FROM ubuntu:20.04 RUN apt-get -y update && apt-get install -y python3 The above Dockerfile illustrates the build steps to create a simple image. Images are normally built from a base image indicated by FROM , in this case Ubuntu. The ability to use a different base image provides a way to use packages which may work more easily on one Linux Distribution. For example the Linux distribution on Eagle is CentOS, building the above image would allow the user to install packages from Ubuntu repositories. The RUN portion of the above Dockerfile indicates the command to run, in this example it installs the Python 3 package. Additional commands such as COPY , ENV , and others enable the customization of your image to suit your compute environment requirements.","title":"Building"},{"location":"Documentation/Software_Tools/Containers/registres/","text":"Container registries at NREL Introduction Container registries enable users to store container images. An overview of the steps to use each fo the main container registries available to NREL users is provided below. Registries can enable reproducibility by storing tagged versions of containers, and also facilitate transferring images easily between different computational resources. Create Docker images Docker is not supported on NREL's HPC systems including Eagle. Instead Singularity is the container engine provided as a module. Singularity is able to pull Docker images and convert them to Singularity images. Although not always possible, we suggest creating Docker images when possible to ensure portability between compute resources and using Singularity to convert the image if it is to be run on an HPC system. Accessibility Registry Eagle Access AWS Access Docker Support Singularity Support Harbor Yes No Yes Yes AWS ECR Yes Yes Yes No* DockerHub Yes Yes Yes No* *for DockerHub and AWS ECR it may be possible to push images using ORAS, but this was not found to be a streamlined process in testing. AWS ECR AWS ECR can be utilized by projects with a cloud allocation to host containers. ECR primarily can be used with Docker containers, although Singularity should also be possible. Harbor NREL's Harbor is a registry hosted by ITS that supports both Docker and Singularity containers. **NREL ITS is currently evaluating a replacement to internally hosted Harbor (likely moving to Enterprise DockerHub ) Docker Login On your local machine to push a container to the registry. docker login harbor.nrel.gov Prepare image for push docker tag SOURCE_IMAGE[:TAG] harbor.nrel.gov/REPO/IMAGE[:TAG] docker push harbor.nrel.gov/REPO/IMAGE[:TAG] Pull Docker image on Eagle Pull and convert container to Singularity on Eagle. Note: --nohttps is not optimal but need to add certs for NREL otherwise there is a cert error. singularity pull --nohttps --docker-login docker://harbor.nrel.gov/REPO/IMAGE[:TAG] The container should now be downloaded and usable as usual Singularity Login information Under your User Profile in Harbor obtain and export the following information export SINGULARITY_DOCKER_USERNAME=<harbor username> export SINGULARITY_DOCKER_PASSWORD=<harbor CLI secret> Push a Singularity image singularity push <image>.sif oras://harbor.nrel.gov/<PROJECT>/<IMAGE>:<TAG> Pull a Singularity image singularity pull oras://harbor.nrel.gov/<PROJECT>/<IMAGE>:<TAG> Dockerhub Currently under testing, and not generally available Credentials To get the needed credentials for NREL Dockerhub, select your username in the top right -> Account -> Security -> Create a new access token. The dialog box will describe how to use the security token with docker login to enable pulling and pushing containers.","title":"Container registries at NREL"},{"location":"Documentation/Software_Tools/Containers/registres/#container-registries-at-nrel","text":"","title":"Container registries at NREL"},{"location":"Documentation/Software_Tools/Containers/registres/#introduction","text":"Container registries enable users to store container images. An overview of the steps to use each fo the main container registries available to NREL users is provided below. Registries can enable reproducibility by storing tagged versions of containers, and also facilitate transferring images easily between different computational resources.","title":"Introduction"},{"location":"Documentation/Software_Tools/Containers/registres/#create-docker-images","text":"Docker is not supported on NREL's HPC systems including Eagle. Instead Singularity is the container engine provided as a module. Singularity is able to pull Docker images and convert them to Singularity images. Although not always possible, we suggest creating Docker images when possible to ensure portability between compute resources and using Singularity to convert the image if it is to be run on an HPC system.","title":"Create Docker images"},{"location":"Documentation/Software_Tools/Containers/registres/#accessibility","text":"Registry Eagle Access AWS Access Docker Support Singularity Support Harbor Yes No Yes Yes AWS ECR Yes Yes Yes No* DockerHub Yes Yes Yes No* *for DockerHub and AWS ECR it may be possible to push images using ORAS, but this was not found to be a streamlined process in testing.","title":"Accessibility"},{"location":"Documentation/Software_Tools/Containers/registres/#aws-ecr","text":"AWS ECR can be utilized by projects with a cloud allocation to host containers. ECR primarily can be used with Docker containers, although Singularity should also be possible.","title":"AWS ECR"},{"location":"Documentation/Software_Tools/Containers/registres/#harbor","text":"NREL's Harbor is a registry hosted by ITS that supports both Docker and Singularity containers. **NREL ITS is currently evaluating a replacement to internally hosted Harbor (likely moving to Enterprise DockerHub )","title":"Harbor"},{"location":"Documentation/Software_Tools/Containers/registres/#docker","text":"","title":"Docker"},{"location":"Documentation/Software_Tools/Containers/registres/#login","text":"On your local machine to push a container to the registry. docker login harbor.nrel.gov","title":"Login"},{"location":"Documentation/Software_Tools/Containers/registres/#prepare-image-for-push","text":"docker tag SOURCE_IMAGE[:TAG] harbor.nrel.gov/REPO/IMAGE[:TAG] docker push harbor.nrel.gov/REPO/IMAGE[:TAG]","title":"Prepare image for push"},{"location":"Documentation/Software_Tools/Containers/registres/#pull-docker-image-on-eagle","text":"Pull and convert container to Singularity on Eagle. Note: --nohttps is not optimal but need to add certs for NREL otherwise there is a cert error. singularity pull --nohttps --docker-login docker://harbor.nrel.gov/REPO/IMAGE[:TAG] The container should now be downloaded and usable as usual","title":"Pull Docker image on Eagle"},{"location":"Documentation/Software_Tools/Containers/registres/#singularity","text":"","title":"Singularity"},{"location":"Documentation/Software_Tools/Containers/registres/#login-information","text":"Under your User Profile in Harbor obtain and export the following information export SINGULARITY_DOCKER_USERNAME=<harbor username> export SINGULARITY_DOCKER_PASSWORD=<harbor CLI secret>","title":"Login information"},{"location":"Documentation/Software_Tools/Containers/registres/#push-a-singularity-image","text":"singularity push <image>.sif oras://harbor.nrel.gov/<PROJECT>/<IMAGE>:<TAG>","title":"Push a Singularity image"},{"location":"Documentation/Software_Tools/Containers/registres/#pull-a-singularity-image","text":"singularity pull oras://harbor.nrel.gov/<PROJECT>/<IMAGE>:<TAG>","title":"Pull a Singularity image"},{"location":"Documentation/Software_Tools/Containers/registres/#dockerhub","text":"Currently under testing, and not generally available","title":"Dockerhub"},{"location":"Documentation/Software_Tools/Containers/registres/#credentials","text":"To get the needed credentials for NREL Dockerhub, select your username in the top right -> Account -> Security -> Create a new access token. The dialog box will describe how to use the security token with docker login to enable pulling and pushing containers.","title":"Credentials"},{"location":"Documentation/Software_Tools/Containers/singularity/","text":"Singularity is installed on Eagle's compute nodes as a module named singularity-container. Images can be copied to eagle and run, or can be generated from a recipe (definition file) . Note: Input commands in the following examples are preceded by a $ . Run hello-world ubuntu image Step 1 : Log into compute node, checking it is running CentOS 7 $ ssh eagle.hpc.nrel.gov [ $USER @el1 ~ ] $ srun -A MYALLOCATION -t 60 -N 1 --pty $SHELL [ $USER @r1i3n18 ~ ] $ cat /etc/redhat-release CentOS Linux release 7 .7.1908 ( Core ) Step 2 : Load the singularity-container module [ $USER @r1i3n18 ~ ] $ module purge [ $USER @r1i3n18 ~ ] $ module load singularity-container Step 3 : Retrieve hello-world image. Be sure to use /scratch as images are typically large [ $USER @r1i3n18 ~ ] $ cd /scratch/ $USER [ $USER @r1i3n18 $USER ] $ mkdir -p singularity-images [ $USER @r1i3n18 $USER ] $ cd singularity-images [ $USER @r1i3n18 singularity-images ] $ singularity pull --name hello-world.simg shub://vsoch/hello-world Progress | =================================== | 100 .0% Done. Container is at: /lustre/eaglefs/scratch/ $USER /singularity-images/hello-world.simg Step 4 : Explore image details [ $USER @r1i3n18 singularity-images ] $ singularity inspect hello-world.simg # Shows labels { \"org.label-schema.usage.singularity.deffile.bootstrap\" : \"docker\" , \"MAINTAINER\" : \"vanessasaur\" , \"org.label-schema.usage.singularity.deffile\" : \"Singularity\" , \"org.label-schema.schema-version\" : \"1.0\" , \"WHATAMI\" : \"dinosaur\" , \"org.label-schema.usage.singularity.deffile.from\" : \"ubuntu:14.04\" , \"org.label-schema.build-date\" : \"2017-10-15T12:52:56+00:00\" , \"org.label-schema.usage.singularity.version\" : \"2.4-feature-squashbuild-secbuild.g780c84d\" , \"org.label-schema.build-size\" : \"333MB\" } [ $USER @r1i3n18 singularity-images ] $ singularity inspect -r hello-world.simg # Shows the script run #!/bin/sh exec /bin/bash /rawr.sh Step 5 : Run image default script [ $USER @r1i3n18 singularity-images ] $ singularity run hello-world.simg RaawwWWWWWRRRR!! Avocado. Step 6 : Run in singularity bash shell [ $USER @r1i3n18 singularity-images ] $ cat /etc/redhat-release CentOS Linux release 7 .7.1908 ( Core ) [ $USER @r1i3n18 singularity-images ] $ cat /etc/lsb-release cat: /etc/lsb-release: No such file or directory [ $USER @r1i3n18 singularity-images ] $ singularity shell hello-world.simg Singularity: Invoking an interactive shell within container... Singularity hello-world.simg:~> cat /etc/lsb-release DISTRIB_ID = Ubuntu DISTRIB_RELEASE = 14 .04 DISTRIB_CODENAME = trusty DISTRIB_DESCRIPTION = \"Ubuntu 14.04.5 LTS\" Singularity hello-world.simg:~> cat /etc/redhat-release cat: /etc/redhat-release: No such file or directory Create a CentOS 7 EPEL image with MPI support This example shows how to create a CentOS 7 singularity image with openmpi installed. It requires root/admin privileges to create the image so must be run on a user's computer with singularity installed. After creation, the image can be copied to Eagle and run. Step 1 : Create a new recipe based on singularityhub/centos:latest echo \"Bootstrap: shub From: singularityhub/centos:latest \" > centos-mpi.recipe Step 2 : Install development tools and enable epel repository after bootstrap is created echo \"%post yum -y groupinstall \" Development Tools \" yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm \" >> centos-mpi.recipe Step 3 : Download, compile and install openmpi 2.1 echo \" curl -O https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.2.tar.bz2 tar jxf openmpi-2.1.2.tar.bz2 cd openmpi-2.1.2 ./configure --prefix=/usr/local make make install \" >> centos-mpi.recipe Step 4 : Compile and install example mpi application echo \" mpicc examples/ring_c.c -o ring cp ring /usr/bin/ \" >> centos-mpi.recipe Step 5 : Install a package found in EPEL, in this example R echo \" yum -y install R \" >> centos-mpi.recipe Step 6 : Set default script to run ring echo \"%runscript /usr/bin/ring \" >> centos-mpi.recipe Step 7 : Build image sudo $( type -p singularity ) build centos-mpi.simg centos-mpi.recipe Step 8 : Test image $ mpirun -np 20 singularity exec centos-mpi.simg /usr/bin/ring $ singularity run centos-epel-r.simg --version R version 3 .4.4 ( 2018 -03-15 ) -- \"Someone to Lean On\" Copyright ( C ) 2018 The R Foundation for Statistical Computing Platform: x86_64-redhat-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3 . For more information about these matters see http://www.gnu.org/licenses/. $ singularity exec centos-mpi.simg Rscript -e \"a <- 42; A <- a*2; print(A)\" [ 1 ] 84","title":"Singularity on Eagle"},{"location":"Documentation/Software_Tools/Containers/singularity/#run-hello-world-ubuntu-image","text":"Step 1 : Log into compute node, checking it is running CentOS 7 $ ssh eagle.hpc.nrel.gov [ $USER @el1 ~ ] $ srun -A MYALLOCATION -t 60 -N 1 --pty $SHELL [ $USER @r1i3n18 ~ ] $ cat /etc/redhat-release CentOS Linux release 7 .7.1908 ( Core ) Step 2 : Load the singularity-container module [ $USER @r1i3n18 ~ ] $ module purge [ $USER @r1i3n18 ~ ] $ module load singularity-container Step 3 : Retrieve hello-world image. Be sure to use /scratch as images are typically large [ $USER @r1i3n18 ~ ] $ cd /scratch/ $USER [ $USER @r1i3n18 $USER ] $ mkdir -p singularity-images [ $USER @r1i3n18 $USER ] $ cd singularity-images [ $USER @r1i3n18 singularity-images ] $ singularity pull --name hello-world.simg shub://vsoch/hello-world Progress | =================================== | 100 .0% Done. Container is at: /lustre/eaglefs/scratch/ $USER /singularity-images/hello-world.simg Step 4 : Explore image details [ $USER @r1i3n18 singularity-images ] $ singularity inspect hello-world.simg # Shows labels { \"org.label-schema.usage.singularity.deffile.bootstrap\" : \"docker\" , \"MAINTAINER\" : \"vanessasaur\" , \"org.label-schema.usage.singularity.deffile\" : \"Singularity\" , \"org.label-schema.schema-version\" : \"1.0\" , \"WHATAMI\" : \"dinosaur\" , \"org.label-schema.usage.singularity.deffile.from\" : \"ubuntu:14.04\" , \"org.label-schema.build-date\" : \"2017-10-15T12:52:56+00:00\" , \"org.label-schema.usage.singularity.version\" : \"2.4-feature-squashbuild-secbuild.g780c84d\" , \"org.label-schema.build-size\" : \"333MB\" } [ $USER @r1i3n18 singularity-images ] $ singularity inspect -r hello-world.simg # Shows the script run #!/bin/sh exec /bin/bash /rawr.sh Step 5 : Run image default script [ $USER @r1i3n18 singularity-images ] $ singularity run hello-world.simg RaawwWWWWWRRRR!! Avocado. Step 6 : Run in singularity bash shell [ $USER @r1i3n18 singularity-images ] $ cat /etc/redhat-release CentOS Linux release 7 .7.1908 ( Core ) [ $USER @r1i3n18 singularity-images ] $ cat /etc/lsb-release cat: /etc/lsb-release: No such file or directory [ $USER @r1i3n18 singularity-images ] $ singularity shell hello-world.simg Singularity: Invoking an interactive shell within container... Singularity hello-world.simg:~> cat /etc/lsb-release DISTRIB_ID = Ubuntu DISTRIB_RELEASE = 14 .04 DISTRIB_CODENAME = trusty DISTRIB_DESCRIPTION = \"Ubuntu 14.04.5 LTS\" Singularity hello-world.simg:~> cat /etc/redhat-release cat: /etc/redhat-release: No such file or directory","title":"Run hello-world ubuntu image"},{"location":"Documentation/Software_Tools/Containers/singularity/#create-a-centos-7-epel-image-with-mpi-support","text":"This example shows how to create a CentOS 7 singularity image with openmpi installed. It requires root/admin privileges to create the image so must be run on a user's computer with singularity installed. After creation, the image can be copied to Eagle and run. Step 1 : Create a new recipe based on singularityhub/centos:latest echo \"Bootstrap: shub From: singularityhub/centos:latest \" > centos-mpi.recipe Step 2 : Install development tools and enable epel repository after bootstrap is created echo \"%post yum -y groupinstall \" Development Tools \" yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm \" >> centos-mpi.recipe Step 3 : Download, compile and install openmpi 2.1 echo \" curl -O https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.2.tar.bz2 tar jxf openmpi-2.1.2.tar.bz2 cd openmpi-2.1.2 ./configure --prefix=/usr/local make make install \" >> centos-mpi.recipe Step 4 : Compile and install example mpi application echo \" mpicc examples/ring_c.c -o ring cp ring /usr/bin/ \" >> centos-mpi.recipe Step 5 : Install a package found in EPEL, in this example R echo \" yum -y install R \" >> centos-mpi.recipe Step 6 : Set default script to run ring echo \"%runscript /usr/bin/ring \" >> centos-mpi.recipe Step 7 : Build image sudo $( type -p singularity ) build centos-mpi.simg centos-mpi.recipe Step 8 : Test image $ mpirun -np 20 singularity exec centos-mpi.simg /usr/bin/ring $ singularity run centos-epel-r.simg --version R version 3 .4.4 ( 2018 -03-15 ) -- \"Someone to Lean On\" Copyright ( C ) 2018 The R Foundation for Statistical Computing Platform: x86_64-redhat-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3 . For more information about these matters see http://www.gnu.org/licenses/. $ singularity exec centos-mpi.simg Rscript -e \"a <- 42; A <- a*2; print(A)\" [ 1 ] 84","title":"Create a CentOS 7 EPEL image with MPI support"},{"location":"Documentation/Software_Tools/FastX/fastx/","text":"FastX on Eagle DAV nodes In addition to four standard ssh-only login nodes, Eagle is also equipped with several specialized Data Analysis and Visualization (DAV) login nodes, intended for HPC applications on Eagle that require a graphical user interface. It is not a general-purpose remote desktop, so we ask that you restrict your usage to only HPC or visualization software that requires Eagle. There are five internal DAV nodes available only to internal NREL users (or via the HPC VPN ), and one node that is externally accessible. All DAV nodes have 36 CPU cores (Intel Xeon Gold 6150), 768GB RAM, one 32GB NVIDIA Quadro GV100 GPU, and offer a Linux desktop (via FastX) with visualization capabilities, optional VirtualGL, and standard Linux terminal applications. DAV nodes are shared resources that support multiple simultaneous users. CPU and RAM usage is monitored by automated software, and high usage may result in temporary throttling by Arbiter. Users who exceed 8 CPUs and 128GB RAM will receive an email notice when limits have been exceeded, and another when usage returns to normal and restrictions are removed. Getting Started with FastX Information on how to log into a DAV node with a FastX remote desktop can be found in the FastX Documentation at https://www.nrel.gov/hpc/eagle-software-fastx.html . NREL users may use the web browser or the FastX desktop client. External users must use the FastX desktop client, or connect to the HPC VPN for the web client. Multiple FastX Sessions FastX sessions may be closed without terminating the session and resumed at a later time. However, since there is a license-based limit to the number of concurrent users, please fully log out/terminate your remote desktop session when you are done working and no longer need to leave processes running. Avoid having remote desktop sessions open on multiple nodes that you are not using, or your sessions may be terminated by system administrators to make licenses available for active users. Reattaching FastX Sessions Connections to the DAV nodes via eagle-dav.hpc.nrel.gov will connect you to a random node. To resume a session that you have suspended, take note of the node your session is running on (ed1, ed2, ed3, ed5, or ed6) before you close the FastX client or browser window, and you may directly access that node when you are ready to reconnect at ed#.hpc.nrel.gov in the FastX client or through your web browser at https://ed#.hpc.nrel.gov . Troubleshooting Could not connect to session bus: Failed to connect to socket /tmp/dbus-XXX: Connection refused This error is usually the result of a change to the default login environment, often by an alteration to ~/.bashrc by altering your $PATH, or by configuring Conda to launch into a (base) or other environment immediately upon login. For changes to your $PATH , be sure to prepend any changes with $PATH so that the default system paths are included before any custom changes that you make. For example: $PATH=$PATH:/home/username/bin instead of $PATH=/home/username/bin/:$PATH . For conda users, the command conda config --set auto_activate_base false will prevent conda from launching into a base environment upon login. How to Get Help Please contact the HPC Helpdesk at hpc-help@nrel.gov if you have any questions, technical issues, or receive a \"no free licenses\" error.","title":"FastX"},{"location":"Documentation/Software_Tools/FastX/fastx/#fastx-on-eagle-dav-nodes","text":"In addition to four standard ssh-only login nodes, Eagle is also equipped with several specialized Data Analysis and Visualization (DAV) login nodes, intended for HPC applications on Eagle that require a graphical user interface. It is not a general-purpose remote desktop, so we ask that you restrict your usage to only HPC or visualization software that requires Eagle. There are five internal DAV nodes available only to internal NREL users (or via the HPC VPN ), and one node that is externally accessible. All DAV nodes have 36 CPU cores (Intel Xeon Gold 6150), 768GB RAM, one 32GB NVIDIA Quadro GV100 GPU, and offer a Linux desktop (via FastX) with visualization capabilities, optional VirtualGL, and standard Linux terminal applications. DAV nodes are shared resources that support multiple simultaneous users. CPU and RAM usage is monitored by automated software, and high usage may result in temporary throttling by Arbiter. Users who exceed 8 CPUs and 128GB RAM will receive an email notice when limits have been exceeded, and another when usage returns to normal and restrictions are removed.","title":"FastX on Eagle DAV nodes"},{"location":"Documentation/Software_Tools/FastX/fastx/#getting-started-with-fastx","text":"Information on how to log into a DAV node with a FastX remote desktop can be found in the FastX Documentation at https://www.nrel.gov/hpc/eagle-software-fastx.html . NREL users may use the web browser or the FastX desktop client. External users must use the FastX desktop client, or connect to the HPC VPN for the web client.","title":"Getting Started with FastX"},{"location":"Documentation/Software_Tools/FastX/fastx/#multiple-fastx-sessions","text":"FastX sessions may be closed without terminating the session and resumed at a later time. However, since there is a license-based limit to the number of concurrent users, please fully log out/terminate your remote desktop session when you are done working and no longer need to leave processes running. Avoid having remote desktop sessions open on multiple nodes that you are not using, or your sessions may be terminated by system administrators to make licenses available for active users.","title":"Multiple FastX Sessions"},{"location":"Documentation/Software_Tools/FastX/fastx/#reattaching-fastx-sessions","text":"Connections to the DAV nodes via eagle-dav.hpc.nrel.gov will connect you to a random node. To resume a session that you have suspended, take note of the node your session is running on (ed1, ed2, ed3, ed5, or ed6) before you close the FastX client or browser window, and you may directly access that node when you are ready to reconnect at ed#.hpc.nrel.gov in the FastX client or through your web browser at https://ed#.hpc.nrel.gov .","title":"Reattaching FastX Sessions"},{"location":"Documentation/Software_Tools/FastX/fastx/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Documentation/Software_Tools/FastX/fastx/#could-not-connect-to-session-bus-failed-to-connect-to-socket-tmpdbus-xxx-connection-refused","text":"This error is usually the result of a change to the default login environment, often by an alteration to ~/.bashrc by altering your $PATH, or by configuring Conda to launch into a (base) or other environment immediately upon login. For changes to your $PATH , be sure to prepend any changes with $PATH so that the default system paths are included before any custom changes that you make. For example: $PATH=$PATH:/home/username/bin instead of $PATH=/home/username/bin/:$PATH . For conda users, the command conda config --set auto_activate_base false will prevent conda from launching into a base environment upon login.","title":"Could not connect to session bus: Failed to connect to socket /tmp/dbus-XXX: Connection refused"},{"location":"Documentation/Software_Tools/FastX/fastx/#how-to-get-help","text":"Please contact the HPC Helpdesk at hpc-help@nrel.gov if you have any questions, technical issues, or receive a \"no free licenses\" error.","title":"How to Get Help"},{"location":"Documentation/Software_Tools/Git/Git/","text":"Git To begin, let's start by clearing up some common misconceptions about git. You probably have heard of git through the popular git-repository hosting web-service GitHub . GitHub (the hosting service) is to git (the open-source version control software) what the internet is to computers\u2014git is used locally to track incremental development and modifications to a collection of files, and GitHub gets those changes to serve as a synchronized, common access point. GitHub also has social aspects, like tracking who changed what and why. There are other git hosting services like GitLab which are similar to GitHub but offer slightly different features. The git workflow has some pretty colorful vocabulary, so let's define some of the terms to avoid confusion going forward: * Repository/repo : A git repository is an independent grouping of files to be tracked. A git repo has a \"root\" which is the directory that it sits in, and tracks further directory nesting from that. A single repo is often thought of as a complete project or application, though it's not uncommon to nest modules of an application as child repositories to isolate the development history of those submodules. Commit : A commit, or \"revision\", is an individual change to a file (or set of files). It's like when you save a file, except with Git, every time you save it creates a unique ID (a.k.a. the \"SHA\" or \"hash\") that allows you to keep record of what changes were made when and by who. Commits usually contain a commit message which is a brief description of what changes were made. Fork : A fork is a personal copy of another user's repository that lives on your account. Forks allow you to freely make changes to a project without affecting the original. Forks remain attached to the original, allowing you to submit a pull request to the original's author to update with your changes. You can also keep your fork up to date by pulling in updates from the original. Pull : Pull refers to when you are fetching in changes and merging them. For instance, if someone has edited the remote file you're both working on, you'll want to pull in those changes to your local copy so that it's up to date. Pull request: Pull requests are proposed changes to a repository submitted by a user and accepted or rejected by a repository's collaborators. Like issues, pull requests each have their own discussion forum. Push : Pushing refers to sending your committed changes to a remote repository, such as a repository hosted on GitHub. For instance, if you change something locally, you'd want to then push those changes so that others may access them.","title":"Git"},{"location":"Documentation/Software_Tools/Git/Git/#git","text":"To begin, let's start by clearing up some common misconceptions about git. You probably have heard of git through the popular git-repository hosting web-service GitHub . GitHub (the hosting service) is to git (the open-source version control software) what the internet is to computers\u2014git is used locally to track incremental development and modifications to a collection of files, and GitHub gets those changes to serve as a synchronized, common access point. GitHub also has social aspects, like tracking who changed what and why. There are other git hosting services like GitLab which are similar to GitHub but offer slightly different features. The git workflow has some pretty colorful vocabulary, so let's define some of the terms to avoid confusion going forward: * Repository/repo : A git repository is an independent grouping of files to be tracked. A git repo has a \"root\" which is the directory that it sits in, and tracks further directory nesting from that. A single repo is often thought of as a complete project or application, though it's not uncommon to nest modules of an application as child repositories to isolate the development history of those submodules. Commit : A commit, or \"revision\", is an individual change to a file (or set of files). It's like when you save a file, except with Git, every time you save it creates a unique ID (a.k.a. the \"SHA\" or \"hash\") that allows you to keep record of what changes were made when and by who. Commits usually contain a commit message which is a brief description of what changes were made. Fork : A fork is a personal copy of another user's repository that lives on your account. Forks allow you to freely make changes to a project without affecting the original. Forks remain attached to the original, allowing you to submit a pull request to the original's author to update with your changes. You can also keep your fork up to date by pulling in updates from the original. Pull : Pull refers to when you are fetching in changes and merging them. For instance, if someone has edited the remote file you're both working on, you'll want to pull in those changes to your local copy so that it's up to date. Pull request: Pull requests are proposed changes to a repository submitted by a user and accepted or rejected by a repository's collaborators. Like issues, pull requests each have their own discussion forum. Push : Pushing refers to sending your committed changes to a remote repository, such as a repository hosted on GitHub. For instance, if you change something locally, you'd want to then push those changes so that others may access them.","title":"Git"},{"location":"Documentation/Software_Tools/Jupyter/","text":"Introduction to Jupyter What is Jupyter? A web app for interactive Python in a browser \"Live coding\" Instant visualization Sharable Reproducible Customizable Now supports other languages besides Python (R, Julia..) https://github.com/jupyter/jupyter/wiki/Jupyter-kernels these slides were created using Markdown in Jupyter! import chart_studio.plotly as py import plotly.figure_factory as ff import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt x = np . linspace ( 0 , 5 , 10 ) y = x ** 2 n = np . array ([ 0 , 1 , 2 , 3 , 4 , 5 ]) xx = np . linspace ( - 0.75 , 1. , 100 ) fig , axes = plt . subplots ( 1 , 4 , figsize = ( 12 , 3 )) axes [ 0 ] . scatter ( xx , xx + 1.25 * np . random . randn ( len ( xx ))) #axes[0].scatter(xx, xx + 0.25*np.random.randn(len(xx))) axes [ 0 ] . set_title ( \"scatter\" ) axes [ 1 ] . step ( n , n ** 2.0 , lw = 2 ) axes [ 1 ] . set_title ( \"step\" ) axes [ 2 ] . bar ( n , n ** 2 , align = \"center\" , width = 0.5 , alpha = 0.5 ) axes [ 2 ] . set_title ( \"bar\" ) axes [ 3 ] . fill_between ( x , x ** 2.5 , x ** 3 , color = \"green\" , alpha = 0.5 ); axes [ 3 ] . set_title ( \"fill_between\" ); Terminology - a Confusion of Words Jupyterhub * Multi-user \"backend\" server * Controls launching the single-user Jupyter server * NREL's \"Europa\" runs Jupyterhub (In general, don't worry about JupyterHub--unless you're a sysadmin) Jupyter/Jupyter Server/Notebook server * The single-user server/web interface * Create/save/load .ipynb notebook files * What users generally interact with Jupyter Notebook * An individual .pynb file * Contains your Python code and visualizations * Sharable/downloadable Jupyter lab * A \"nicer\" web interface for Jupyter - \"notebooks 2.0\" * Preferred by some * Lacking some features of \"classic\" notebooks Kernel * The Python environment used by a notebook * More on kernels later Using Europa We run a Jupyterhub server that is available. Europa's Advantages: * Fast and easy access * Use regular Eagle credentials * Great for light to moderate processing/debugging/testing Europa's Disadvantages: * Limited resource: 8 cores/128GB RAM per user beefore automatic throttling * Compete with other users for CPU/RAM on a single machine * No custom environments (for now) Simple Instructions: - Visit Europa at (https://europa.hpc.nrel.gov/) - Log in using your HPC credentials - Opens a standard \"notebooks\" interface - Change url end /tree to /lab for Lab interface Using a Compute Node Advantages: * Custom environments * 36 cores and up to ~750GB RAM * No competing with other users for cores Disadvantages: * Compete with other users for nodes * Costs AU ...Is more than one node possible? Yes... please see our advanced Jupyter documentation. Using a Compute Node - Hard Way Start on a login node: ssh eagle.hpc.nrel.gov [user@el1:]$ srun -A <account> -t 02:00:00 --pty /bin/bash When the job starts on the compute node: [user@r2i7n35]$ module load conda source activate myjupenv jupyter-notebook --no-browser --ip=$(hostname -s) note the node name (r2i7n35 in this example) and the url, e.g. http://127.0.0.1:8888/?token=<alphabet soup> In a terminal on your computer: [user@laptop]$ ssh -N -L 8888:<nodename>:8888 username@eagle.hpc.nrel.gov copy full url from jupyter startup into your web browser. e.g.: http://127.0.0.1:8888/?token=<alphabet soup> Using a Compute Node - Easy Way Automation makes life better! Auto-launching with an sbatch script Full directions included in the Jupyter repo . Download sbatch_jupyter.sh and auto_launch_jupyter.sh Edit sbatch_jupyter.sh to change: --account=*yourallocation* --time=*timelimit* Run auto_launch_jupyter.sh and follow directions That's it! Using a Login Node Yes, you can run jupyter directly on a login node. Should you run jupyter directly on a login node? Reasons to Not Run Jupyter Directly on a Login Node * Heavy lifting should be done via Europa or compute nodes * Using a highly shared resource (login nodes) * Competition for cycles * arbiter2 will throttle moderate to heavy usage Custom Conda Environments and Jupyter Kernels Creating a conda environment: conda create -n myjupyter -c conda-forge jupyter ipykernel source activate myjupyter conda install -c conda-forge scipy numpy matplotlib Add custom ipykernel python -m ipykernel install --user --name=myjupyter Restart your jupyter server New kernel will appear in drop-down as an option Remove custom ipykernel jupyter kernelspec list jupyter kernelspec remove myoldjupyter Magic commands Magic commands are \"meta commands\" that add extra functionality. Magic commands begin with % or %%. A Few Useful Examples * %lsmagic - list all magic commands * %run _file.py_ - run an external python script * %%time - placed at top of cell, prints execution time * %who - list all defined variables in notebook %lsmagic Available line magics: %alias %alias_magic %autoawait %autocall %automagic %autosave %bookmark %cat %cd %clear %colors %conda %config %connect_info %cp %debug %dhist %dirs %doctest_mode %ed %edit %env %gui %hist %history %killbgscripts %ldir %less %lf %lk %ll %load %load_ext %loadpy %logoff %logon %logstart %logstate %logstop %ls %lsmagic %lx %macro %magic %man %matplotlib %mkdir %more %mv %notebook %page %pastebin %pdb %pdef %pdoc %pfile %pinfo %pinfo2 %pip %popd %pprint %precision %prun %psearch %psource %pushd %pwd %pycat %pylab %qtconsole %quickref %recall %rehashx %reload_ext %rep %rerun %reset %reset_selective %rm %rmdir %run %save %sc %set_env %store %sx %system %tb %time %timeit %unalias %unload_ext %who %who_ls %whos %xdel %xmode Available cell magics: %%! %%HTML %%SVG %%bash %%capture %%debug %%file %%html %%javascript %%js %%latex %%markdown %%perl %%prun %%pypy %%python %%python2 %%python3 %%ruby %%script %%sh %%svg %%sx %%system %%time %%timeit %%writefile Automagic is ON, % prefix IS NOT needed for line magics. Shell Commands You can also run shell commands inside a cell. For example: !conda list - see the packages installed in the environment you're using ! pwd ! ls /home/tthatche/jup auto_launch_jupyter.sh Jupyter Presentation.ipynb slurm-6445885.out geojsondemo.ipynb old sshot1.png Interesting Graphs.ipynb sbatch_jupyter.sh sshot2.png jup-logo.png slurm Interesting/Useful Notebooks Awesome Jupyter Awesome Jupyterlab Plotting with matplotlib Python for Data Science Numerical Computing in Python The Sound of Hydrogen Plotting Pitfalls GeoJSON Extension Happy Notebooking!","title":"Introduction to Jupyter"},{"location":"Documentation/Software_Tools/Jupyter/#introduction-to-jupyter","text":"","title":"Introduction to Jupyter"},{"location":"Documentation/Software_Tools/Jupyter/#what-is-jupyter","text":"A web app for interactive Python in a browser \"Live coding\" Instant visualization Sharable Reproducible Customizable Now supports other languages besides Python (R, Julia..) https://github.com/jupyter/jupyter/wiki/Jupyter-kernels these slides were created using Markdown in Jupyter! import chart_studio.plotly as py import plotly.figure_factory as ff import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt x = np . linspace ( 0 , 5 , 10 ) y = x ** 2 n = np . array ([ 0 , 1 , 2 , 3 , 4 , 5 ]) xx = np . linspace ( - 0.75 , 1. , 100 ) fig , axes = plt . subplots ( 1 , 4 , figsize = ( 12 , 3 )) axes [ 0 ] . scatter ( xx , xx + 1.25 * np . random . randn ( len ( xx ))) #axes[0].scatter(xx, xx + 0.25*np.random.randn(len(xx))) axes [ 0 ] . set_title ( \"scatter\" ) axes [ 1 ] . step ( n , n ** 2.0 , lw = 2 ) axes [ 1 ] . set_title ( \"step\" ) axes [ 2 ] . bar ( n , n ** 2 , align = \"center\" , width = 0.5 , alpha = 0.5 ) axes [ 2 ] . set_title ( \"bar\" ) axes [ 3 ] . fill_between ( x , x ** 2.5 , x ** 3 , color = \"green\" , alpha = 0.5 ); axes [ 3 ] . set_title ( \"fill_between\" );","title":"What is Jupyter?"},{"location":"Documentation/Software_Tools/Jupyter/#terminology-a-confusion-of-words","text":"","title":"Terminology - a Confusion of Words"},{"location":"Documentation/Software_Tools/Jupyter/#jupyterhub","text":"* Multi-user \"backend\" server * Controls launching the single-user Jupyter server * NREL's \"Europa\" runs Jupyterhub (In general, don't worry about JupyterHub--unless you're a sysadmin)","title":"Jupyterhub"},{"location":"Documentation/Software_Tools/Jupyter/#jupyterjupyter-servernotebook-server","text":"* The single-user server/web interface * Create/save/load .ipynb notebook files * What users generally interact with","title":"Jupyter/Jupyter Server/Notebook server"},{"location":"Documentation/Software_Tools/Jupyter/#jupyter-notebook","text":"* An individual .pynb file * Contains your Python code and visualizations * Sharable/downloadable","title":"Jupyter Notebook"},{"location":"Documentation/Software_Tools/Jupyter/#jupyter-lab","text":"* A \"nicer\" web interface for Jupyter - \"notebooks 2.0\" * Preferred by some * Lacking some features of \"classic\" notebooks","title":"Jupyter lab"},{"location":"Documentation/Software_Tools/Jupyter/#kernel","text":"* The Python environment used by a notebook * More on kernels later","title":"Kernel"},{"location":"Documentation/Software_Tools/Jupyter/#using-europa","text":"We run a Jupyterhub server that is available.","title":"Using Europa"},{"location":"Documentation/Software_Tools/Jupyter/#europas-advantages","text":"* Fast and easy access * Use regular Eagle credentials * Great for light to moderate processing/debugging/testing","title":"Europa's Advantages:"},{"location":"Documentation/Software_Tools/Jupyter/#europas-disadvantages","text":"* Limited resource: 8 cores/128GB RAM per user beefore automatic throttling * Compete with other users for CPU/RAM on a single machine * No custom environments (for now)","title":"Europa's Disadvantages:"},{"location":"Documentation/Software_Tools/Jupyter/#simple-instructions","text":"- Visit Europa at (https://europa.hpc.nrel.gov/) - Log in using your HPC credentials - Opens a standard \"notebooks\" interface - Change url end /tree to /lab for Lab interface","title":"Simple Instructions:"},{"location":"Documentation/Software_Tools/Jupyter/#using-a-compute-node","text":"","title":"Using a Compute Node"},{"location":"Documentation/Software_Tools/Jupyter/#advantages","text":"* Custom environments * 36 cores and up to ~750GB RAM * No competing with other users for cores","title":"Advantages:"},{"location":"Documentation/Software_Tools/Jupyter/#disadvantages","text":"* Compete with other users for nodes * Costs AU","title":"Disadvantages:"},{"location":"Documentation/Software_Tools/Jupyter/#is-more-than-one-node-possible","text":"Yes... please see our advanced Jupyter documentation.","title":"...Is more than one node possible?"},{"location":"Documentation/Software_Tools/Jupyter/#using-a-compute-node-hard-way","text":"","title":"Using a Compute Node - Hard Way"},{"location":"Documentation/Software_Tools/Jupyter/#start-on-a-login-node","text":"ssh eagle.hpc.nrel.gov [user@el1:]$ srun -A <account> -t 02:00:00 --pty /bin/bash","title":"Start on a login node:"},{"location":"Documentation/Software_Tools/Jupyter/#when-the-job-starts-on-the-compute-node","text":"[user@r2i7n35]$ module load conda source activate myjupenv jupyter-notebook --no-browser --ip=$(hostname -s) note the node name (r2i7n35 in this example) and the url, e.g. http://127.0.0.1:8888/?token=<alphabet soup>","title":"When the job starts on the compute node:"},{"location":"Documentation/Software_Tools/Jupyter/#in-a-terminal-on-your-computer","text":"[user@laptop]$ ssh -N -L 8888:<nodename>:8888 username@eagle.hpc.nrel.gov copy full url from jupyter startup into your web browser. e.g.: http://127.0.0.1:8888/?token=<alphabet soup>","title":"In a terminal on your computer:"},{"location":"Documentation/Software_Tools/Jupyter/#using-a-compute-node-easy-way","text":"Automation makes life better!","title":"Using a Compute Node - Easy Way"},{"location":"Documentation/Software_Tools/Jupyter/#auto-launching-with-an-sbatch-script","text":"Full directions included in the Jupyter repo . Download sbatch_jupyter.sh and auto_launch_jupyter.sh Edit sbatch_jupyter.sh to change: --account=*yourallocation* --time=*timelimit* Run auto_launch_jupyter.sh and follow directions That's it!","title":"Auto-launching with an sbatch script"},{"location":"Documentation/Software_Tools/Jupyter/#using-a-login-node","text":"Yes, you can run jupyter directly on a login node. Should you run jupyter directly on a login node?","title":"Using a Login Node"},{"location":"Documentation/Software_Tools/Jupyter/#reasons-to-not-run-jupyter-directly-on-a-login-node","text":"* Heavy lifting should be done via Europa or compute nodes * Using a highly shared resource (login nodes) * Competition for cycles * arbiter2 will throttle moderate to heavy usage","title":"Reasons to Not Run Jupyter Directly on a Login Node"},{"location":"Documentation/Software_Tools/Jupyter/#custom-conda-environments-and-jupyter-kernels","text":"","title":"Custom Conda Environments and Jupyter Kernels"},{"location":"Documentation/Software_Tools/Jupyter/#creating-a-conda-environment","text":"conda create -n myjupyter -c conda-forge jupyter ipykernel source activate myjupyter conda install -c conda-forge scipy numpy matplotlib","title":"Creating a conda environment:"},{"location":"Documentation/Software_Tools/Jupyter/#add-custom-ipykernel","text":"python -m ipykernel install --user --name=myjupyter Restart your jupyter server New kernel will appear in drop-down as an option","title":"Add custom ipykernel"},{"location":"Documentation/Software_Tools/Jupyter/#remove-custom-ipykernel","text":"jupyter kernelspec list jupyter kernelspec remove myoldjupyter","title":"Remove custom ipykernel"},{"location":"Documentation/Software_Tools/Jupyter/#magic-commands","text":"Magic commands are \"meta commands\" that add extra functionality. Magic commands begin with % or %%.","title":"Magic commands"},{"location":"Documentation/Software_Tools/Jupyter/#a-few-useful-examples","text":"* %lsmagic - list all magic commands * %run _file.py_ - run an external python script * %%time - placed at top of cell, prints execution time * %who - list all defined variables in notebook %lsmagic Available line magics: %alias %alias_magic %autoawait %autocall %automagic %autosave %bookmark %cat %cd %clear %colors %conda %config %connect_info %cp %debug %dhist %dirs %doctest_mode %ed %edit %env %gui %hist %history %killbgscripts %ldir %less %lf %lk %ll %load %load_ext %loadpy %logoff %logon %logstart %logstate %logstop %ls %lsmagic %lx %macro %magic %man %matplotlib %mkdir %more %mv %notebook %page %pastebin %pdb %pdef %pdoc %pfile %pinfo %pinfo2 %pip %popd %pprint %precision %prun %psearch %psource %pushd %pwd %pycat %pylab %qtconsole %quickref %recall %rehashx %reload_ext %rep %rerun %reset %reset_selective %rm %rmdir %run %save %sc %set_env %store %sx %system %tb %time %timeit %unalias %unload_ext %who %who_ls %whos %xdel %xmode Available cell magics: %%! %%HTML %%SVG %%bash %%capture %%debug %%file %%html %%javascript %%js %%latex %%markdown %%perl %%prun %%pypy %%python %%python2 %%python3 %%ruby %%script %%sh %%svg %%sx %%system %%time %%timeit %%writefile Automagic is ON, % prefix IS NOT needed for line magics.","title":"A Few Useful Examples"},{"location":"Documentation/Software_Tools/Jupyter/#shell-commands","text":"You can also run shell commands inside a cell. For example: !conda list - see the packages installed in the environment you're using ! pwd ! ls /home/tthatche/jup auto_launch_jupyter.sh Jupyter Presentation.ipynb slurm-6445885.out geojsondemo.ipynb old sshot1.png Interesting Graphs.ipynb sbatch_jupyter.sh sshot2.png jup-logo.png slurm","title":"Shell Commands"},{"location":"Documentation/Software_Tools/Jupyter/#interestinguseful-notebooks","text":"Awesome Jupyter Awesome Jupyterlab Plotting with matplotlib Python for Data Science Numerical Computing in Python The Sound of Hydrogen Plotting Pitfalls GeoJSON Extension","title":"Interesting/Useful Notebooks"},{"location":"Documentation/Software_Tools/Jupyter/#happy-notebooking","text":"","title":"Happy Notebooking!"},{"location":"Documentation/Software_Tools/Jupyter/jupyterhub/","text":"JupyterHub Prior to using Jupyterhub, you will have had to have logged into Eagle via the command line at least once. Given that, to start using Jupyterhub on Eagle, go to Europa in your local machine's browser, and log in with your Eagle username and password. You should land in your home directory, and see everything there via the standard Jupyter file listing. From the \"New\" pulldown on the right hand side, you can start a notebook, open a terminal, or create a file or folder. The default installation is Python version 3, and a variety of Conda modules are installed already. You can start a Python3 notebook right away, and access the Python modules that are already present. To see what's installed, from a notebook you can use the following command: !conda list Alternatively, you can start a Terminal, and use the usual conda commands from the shell. Creating a custom environment to access from the notebook Start a Terminal session, and follow the instructions on the HPC website to create an environment. Now, to make this environment visible from your future notebooks, run the following command: source activate <myenv> python -m ipykernel install --user --name <myenv> --display-name \"How-you-want-your-custom-kernel-to-appear-in-the-notebook-pulldown (<myenv>)\" where <myenv> is the argument to -n you used in your conda create command. After running this command, when you open a new notebook, you should see as an option your new environment, and once loaded be able to access all Python modules therein. Using Jupyterhub from Eagle To use inside Eagle, the Jupyterhub server exists on the internal network @ https://europa-int/. Customizing JupyterHub provides the ability to use custom kernels including ones for other popular programming languages such as Julia and R. NREL's custom kernels documentation provides more information on how to setup JupyterHub with other languages.","title":"JupyterHub"},{"location":"Documentation/Software_Tools/Jupyter/jupyterhub/#jupyterhub","text":"Prior to using Jupyterhub, you will have had to have logged into Eagle via the command line at least once. Given that, to start using Jupyterhub on Eagle, go to Europa in your local machine's browser, and log in with your Eagle username and password. You should land in your home directory, and see everything there via the standard Jupyter file listing. From the \"New\" pulldown on the right hand side, you can start a notebook, open a terminal, or create a file or folder. The default installation is Python version 3, and a variety of Conda modules are installed already. You can start a Python3 notebook right away, and access the Python modules that are already present. To see what's installed, from a notebook you can use the following command: !conda list Alternatively, you can start a Terminal, and use the usual conda commands from the shell.","title":"JupyterHub"},{"location":"Documentation/Software_Tools/Jupyter/jupyterhub/#creating-a-custom-environment-to-access-from-the-notebook","text":"Start a Terminal session, and follow the instructions on the HPC website to create an environment. Now, to make this environment visible from your future notebooks, run the following command: source activate <myenv> python -m ipykernel install --user --name <myenv> --display-name \"How-you-want-your-custom-kernel-to-appear-in-the-notebook-pulldown (<myenv>)\" where <myenv> is the argument to -n you used in your conda create command. After running this command, when you open a new notebook, you should see as an option your new environment, and once loaded be able to access all Python modules therein.","title":"Creating a custom environment to access from the notebook"},{"location":"Documentation/Software_Tools/Jupyter/jupyterhub/#using-jupyterhub-from-eagle","text":"To use inside Eagle, the Jupyterhub server exists on the internal network @ https://europa-int/.","title":"Using Jupyterhub from Eagle"},{"location":"Documentation/Software_Tools/Jupyter/jupyterhub/#customizing","text":"JupyterHub provides the ability to use custom kernels including ones for other popular programming languages such as Julia and R. NREL's custom kernels documentation provides more information on how to setup JupyterHub with other languages.","title":"Customizing"},{"location":"Documentation/Software_Tools/building-packages/","text":"Building packages on Eagle for individual or project use. This training module will walk through how to build a reasonably complex package, OpenMPI, and deploy it for use by yourself or members of a project. Acquire the package and set up for build Configure, build, and install the package Setting up your own environment module Why build your own application? Sometimes, the package version that you need, or the capabilities you want, are only available as source code. Other times, a package has dependencies on other ones with application programming interfaces that change rapidly. A source code build might have code to adapt to the (older, newer) libraries you have available, whereas a binary distribution will likely not. In other cases, a binary distribution may be associated with a particular Linux distribution and version different from Eagle's. One example is a package for Linux version X+1 (with a shiny new libc). If you try to run this on Linux version X, you will almost certainly get errors associated with the GLIBC version required. If you build the application against your own, older libc version, those dependencies are not created. Performance; for example, if a more performant numerical library is available, you may be able to link against it. A pre-built binary may have been built against a more universally available but lower performance library. The same holds for optimizing compilers. Curiosity to know more about the tools you use. Pride of building one's tools oneself. For the sheer thrill of building packages.","title":"Building Packages"},{"location":"Documentation/Software_Tools/building-packages/#building-packages-on-eagle-for-individual-or-project-use","text":"This training module will walk through how to build a reasonably complex package, OpenMPI, and deploy it for use by yourself or members of a project. Acquire the package and set up for build Configure, build, and install the package Setting up your own environment module","title":"Building packages on Eagle for individual or project use."},{"location":"Documentation/Software_Tools/building-packages/#why-build-your-own-application","text":"Sometimes, the package version that you need, or the capabilities you want, are only available as source code. Other times, a package has dependencies on other ones with application programming interfaces that change rapidly. A source code build might have code to adapt to the (older, newer) libraries you have available, whereas a binary distribution will likely not. In other cases, a binary distribution may be associated with a particular Linux distribution and version different from Eagle's. One example is a package for Linux version X+1 (with a shiny new libc). If you try to run this on Linux version X, you will almost certainly get errors associated with the GLIBC version required. If you build the application against your own, older libc version, those dependencies are not created. Performance; for example, if a more performant numerical library is available, you may be able to link against it. A pre-built binary may have been built against a more universally available but lower performance library. The same holds for optimizing compilers. Curiosity to know more about the tools you use. Pride of building one's tools oneself. For the sheer thrill of building packages.","title":"Why build your own application?"},{"location":"Documentation/Software_Tools/building-packages/acquire/","text":"Getting the package Change working directory to the location where you'll build the package. A convenient location is /scratch/$USER , which we'll use for this example. cd /scratch/$USER OpenMPI can be found at https://www.open-mpi.org/software/ompi/ . This will automatically redirect you to the latest version, but older releases can be seen in the left menu bar. For this, choose version 4.1. There are several packaging options. Here, we'll get the bzipped tarball openmpi-4.1.0.tar.bz2 . You can either download it to a local machine (laptop) and then scp the file over to Eagle, or get it directly on Eagle with wget. wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.0.tar.bz2 You should now have a compressed tarball in your scratch directory. 4, List the contents of the tarball before unpacking. This is very useful to avoid inadvertently filling a directory with gobs of files and directories when the tarball has them at the top of the file structure), tar -tf openmpi-4.1.0.tar.bz2 Unpack it via tar -xjf openmpi-4.1.0.tar.bz2 If you're curious to see what's in the file as it unpacks, add the -v option. You should now have an openmpi-4.1.0 directory. cd openmpi-4.1.0 , at which point you are in the top level of the package distribution. You can now proceed to configuring, making, and installing.","title":"Acquire"},{"location":"Documentation/Software_Tools/building-packages/acquire/#getting-the-package","text":"Change working directory to the location where you'll build the package. A convenient location is /scratch/$USER , which we'll use for this example. cd /scratch/$USER OpenMPI can be found at https://www.open-mpi.org/software/ompi/ . This will automatically redirect you to the latest version, but older releases can be seen in the left menu bar. For this, choose version 4.1. There are several packaging options. Here, we'll get the bzipped tarball openmpi-4.1.0.tar.bz2 . You can either download it to a local machine (laptop) and then scp the file over to Eagle, or get it directly on Eagle with wget. wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.0.tar.bz2 You should now have a compressed tarball in your scratch directory. 4, List the contents of the tarball before unpacking. This is very useful to avoid inadvertently filling a directory with gobs of files and directories when the tarball has them at the top of the file structure), tar -tf openmpi-4.1.0.tar.bz2 Unpack it via tar -xjf openmpi-4.1.0.tar.bz2 If you're curious to see what's in the file as it unpacks, add the -v option. You should now have an openmpi-4.1.0 directory. cd openmpi-4.1.0 , at which point you are in the top level of the package distribution. You can now proceed to configuring, making, and installing.","title":"Getting the package"},{"location":"Documentation/Software_Tools/building-packages/config_make_install/","text":"Configuring your build We will illustrate a package build that relies on the popular autotools system. Colloquially, this is the configure; make; make install process that is often encountered first by those new to package builds on Linux. Other build systems like CMake (which differ primarily in the configuration steps) won't be covered. If you need to build a package that relies on CMake, please contact hpc-help@nrel.gov for assistance. We'll use GCC version 8.4.0 for this illustration, so load the associated module first ( i.e. , gcc/8.4.0 ). Now that you've acquired and unpacked the package tarball and changed into the top-level directory of the package, you should see a script named \"configure\". In order to see all available options to an autotools configure script, use ./configure -h (don't forget to include the ./ explicit path, otherwise the script will not be found in the default Linux search paths, or worse, a different script will be found). We will build with the following command: ./configure --prefix=/scratch/$USER/openmpi/4.1.0-gcc-8.4.0 --with-slurm --with-pmi=/nopt/slurm/current --with-gnu-ld --with-lustre --with-zlib --without-psm --without-psm2 --with-ucx --without-verbs --with-hwloc=external --with-hwloc-libdir=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib --enable-cxx-exceptions --enable-mpi-cxx --enable-mpi-fortran --enable-static LDFLAGS=\"-L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64\" CPPFLAGS=-I/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/include These options are given for the following reasons. --prefix= : This sets the location that \"make install\" will ultimately populate. If this isn't given, generally the default is to install into /usr or /usr/local, both of which require privileged access. We'll set up the environment using environment modules to point to this custom location. --with-slurm : Enables the interface with the Slurm resource manager --with-pmi= : Point to the Process Management Interface, the abstraction layer for MPI options --with-gnu-ld : Letting the build system know that linking will be done with GNU's linker, rather than a commercial or alternative open one. --with-lustre : Enable Lustre features --with-zlib : Enable compression library --without-psm[2] : Explicitly turn off interfaces to Intel's Performance Scaled Messaging for the now-defunct Omni-Path network --with-ucx= : Point to UCX, an intermediate layer between the network drivers and MPI --without-verbs= : For newer MPIs, communications go through UCX and/or libfabric, not directly to the Verbs layer --with-hwloc[-libdir]= : Point to a separately built hardware localization library for process pinning --enable-cxx-exceptions , --enable-mpi-cxx : Build the C++ interface for the libraries --enable-mpi-fortran : Build the Fortran interface for the libraries --enable-static : Build the .a archive files for static linking of applications LDFLAGS : -L options point to non-standard library locations. -Wl,-rpath options embed paths into the binaries, so that having LD_LIBRARY_PATH set correctly is not necessary (i.e., no separate module for these components). CPPFLAGS : Point to header files in non-standard locations. NOTE: The CUDA paths are not needed for CUDA function per se, but the resulting MPI errors out without setting them. There appears to be a lack of modularity that sets up a seemingly unneeded dependency. After lots of messages scroll by, you should be returned to a prompt following a summary of options. It's not a bad idea to glance through these, and make sure everything makes sense and is what you intended. Now that the build is configured, you can \"make\" it. For packages that are well integrated with automake, you can speed the build up by parallelizing it over multiple processes with the -j # option. If you're building this on a compute node, feel free to set this option to the total number of cores available. On the other hand, if you're using a login node, be a good citizen and leave cores available for other users ( i.e. , don't use more than 4; Arbiter should limit access at any rate regardless of this setting). make -j 4 Try a make check and/or a make test . Not every package enables these tests, but if they do, it's a great idea to run these sanity checks to find if your build is perfect, maybe-good-enough, or totally wrong before building lots of other software on top of it. Assuming checks passed if present, it's now time for make install . Assuming that completes without errors, you can move onto creating an environment module to use your new MPI library.","title":"Config Make Install"},{"location":"Documentation/Software_Tools/building-packages/config_make_install/#configuring-your-build","text":"We will illustrate a package build that relies on the popular autotools system. Colloquially, this is the configure; make; make install process that is often encountered first by those new to package builds on Linux. Other build systems like CMake (which differ primarily in the configuration steps) won't be covered. If you need to build a package that relies on CMake, please contact hpc-help@nrel.gov for assistance. We'll use GCC version 8.4.0 for this illustration, so load the associated module first ( i.e. , gcc/8.4.0 ). Now that you've acquired and unpacked the package tarball and changed into the top-level directory of the package, you should see a script named \"configure\". In order to see all available options to an autotools configure script, use ./configure -h (don't forget to include the ./ explicit path, otherwise the script will not be found in the default Linux search paths, or worse, a different script will be found). We will build with the following command: ./configure --prefix=/scratch/$USER/openmpi/4.1.0-gcc-8.4.0 --with-slurm --with-pmi=/nopt/slurm/current --with-gnu-ld --with-lustre --with-zlib --without-psm --without-psm2 --with-ucx --without-verbs --with-hwloc=external --with-hwloc-libdir=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib --enable-cxx-exceptions --enable-mpi-cxx --enable-mpi-fortran --enable-static LDFLAGS=\"-L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64\" CPPFLAGS=-I/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/include These options are given for the following reasons. --prefix= : This sets the location that \"make install\" will ultimately populate. If this isn't given, generally the default is to install into /usr or /usr/local, both of which require privileged access. We'll set up the environment using environment modules to point to this custom location. --with-slurm : Enables the interface with the Slurm resource manager --with-pmi= : Point to the Process Management Interface, the abstraction layer for MPI options --with-gnu-ld : Letting the build system know that linking will be done with GNU's linker, rather than a commercial or alternative open one. --with-lustre : Enable Lustre features --with-zlib : Enable compression library --without-psm[2] : Explicitly turn off interfaces to Intel's Performance Scaled Messaging for the now-defunct Omni-Path network --with-ucx= : Point to UCX, an intermediate layer between the network drivers and MPI --without-verbs= : For newer MPIs, communications go through UCX and/or libfabric, not directly to the Verbs layer --with-hwloc[-libdir]= : Point to a separately built hardware localization library for process pinning --enable-cxx-exceptions , --enable-mpi-cxx : Build the C++ interface for the libraries --enable-mpi-fortran : Build the Fortran interface for the libraries --enable-static : Build the .a archive files for static linking of applications LDFLAGS : -L options point to non-standard library locations. -Wl,-rpath options embed paths into the binaries, so that having LD_LIBRARY_PATH set correctly is not necessary (i.e., no separate module for these components). CPPFLAGS : Point to header files in non-standard locations. NOTE: The CUDA paths are not needed for CUDA function per se, but the resulting MPI errors out without setting them. There appears to be a lack of modularity that sets up a seemingly unneeded dependency. After lots of messages scroll by, you should be returned to a prompt following a summary of options. It's not a bad idea to glance through these, and make sure everything makes sense and is what you intended. Now that the build is configured, you can \"make\" it. For packages that are well integrated with automake, you can speed the build up by parallelizing it over multiple processes with the -j # option. If you're building this on a compute node, feel free to set this option to the total number of cores available. On the other hand, if you're using a login node, be a good citizen and leave cores available for other users ( i.e. , don't use more than 4; Arbiter should limit access at any rate regardless of this setting). make -j 4 Try a make check and/or a make test . Not every package enables these tests, but if they do, it's a great idea to run these sanity checks to find if your build is perfect, maybe-good-enough, or totally wrong before building lots of other software on top of it. Assuming checks passed if present, it's now time for make install . Assuming that completes without errors, you can move onto creating an environment module to use your new MPI library.","title":"Configuring your build"},{"location":"Documentation/Software_Tools/building-packages/modules/","text":"Setting up your module Now that the package has been installed to your preferred location, we can set up an environment module. a. If this is your first package, then you probably need to create a place to collect modulefiles. For example, mkdir -p /scratch/$USER/modules/default/modulefiles . b. You can look at the systems module collection(s), e.g. , /nopt/nrel/apps/modules/default/modulefiles , to see how modules are organized from a filesystem perspective. In short, each library, application, or framework has its own directory in the modulefiles directory, and the modulefile itself sits either in this directory, or one level lower to accomodate additional versioning. In this example, there is the MPI version (4.1.0), as well as the compiler type and version (GCC 8.4.0) to keep track of. So, we'll make a /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0 directory, and name the file by the compiler version used to build (gcc-8.4.0). You're free to modify this scheme to suit your own intentions. c. In the openmpi/4.1.0/gcc840 directory you just made, or whatever directory name you chose, goes the actual modulefile. It's much easier to copy an example from the system collection than to write one de novo , so you can do cp /nopt/nrel/apps/modules/default/modulefiles/openmpi/4.0.4/gcc-8.4.0.lua /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0/. The Lmod modules system uses the Lua language natively for module code. It is not necessary for you to know the language to modify our examples. Tcl modules will also work under Lmod, but don't offer quite as much flexibility. d. For this example, (a) the OpenMPI version we're building is 4.1.0 instead of 4.0.4, and (b) the location is in /scratch/$USER , rather than /nopt/nrel/apps . So, edit /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0/gcc-8.4.0.lua to make the required changes. Most of these changes only need to be made at the top of the file; variable definitions take care of the rest. e. Now you need to make a one-time change in order to see modules that you put in this collection ( /scratch/$USER/modules/default/modulefiles ). In your $HOME/.bash_profile , add the following line near the top: module use /scratch/$USER/modules/default/modulefiles Obviously, if you've built packages before and enabled them this way, you don't have to do this again! Now logout, log back in, and you should see your personal modules collection with a brand new module. [cchang@el1 ~]$ module avail ---------------------------------- /scratch/cchang/modules/default/modulefiles ----------------------------------- openmpi/4.1.0/gcc-8.4.0 Notice that the \".lua\" extension does not appear--the converse is also true, if the extension is missing it will not appear via module commands! As a sanity check, it's a good idea to load the module, and check that an executable file you know exists there is in fact on your PATH: [cchang@el1 ~]$ module load openmpi/4.1.0/gcc-8.4.0 [cchang@el1 ~]$ which mpirun /scratch/cchang/openmpi/4.1.0-gcc-8.4.0/bin/mpirun","title":"Modules"},{"location":"Documentation/Software_Tools/building-packages/modules/#setting-up-your-module","text":"Now that the package has been installed to your preferred location, we can set up an environment module. a. If this is your first package, then you probably need to create a place to collect modulefiles. For example, mkdir -p /scratch/$USER/modules/default/modulefiles . b. You can look at the systems module collection(s), e.g. , /nopt/nrel/apps/modules/default/modulefiles , to see how modules are organized from a filesystem perspective. In short, each library, application, or framework has its own directory in the modulefiles directory, and the modulefile itself sits either in this directory, or one level lower to accomodate additional versioning. In this example, there is the MPI version (4.1.0), as well as the compiler type and version (GCC 8.4.0) to keep track of. So, we'll make a /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0 directory, and name the file by the compiler version used to build (gcc-8.4.0). You're free to modify this scheme to suit your own intentions. c. In the openmpi/4.1.0/gcc840 directory you just made, or whatever directory name you chose, goes the actual modulefile. It's much easier to copy an example from the system collection than to write one de novo , so you can do cp /nopt/nrel/apps/modules/default/modulefiles/openmpi/4.0.4/gcc-8.4.0.lua /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0/. The Lmod modules system uses the Lua language natively for module code. It is not necessary for you to know the language to modify our examples. Tcl modules will also work under Lmod, but don't offer quite as much flexibility. d. For this example, (a) the OpenMPI version we're building is 4.1.0 instead of 4.0.4, and (b) the location is in /scratch/$USER , rather than /nopt/nrel/apps . So, edit /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0/gcc-8.4.0.lua to make the required changes. Most of these changes only need to be made at the top of the file; variable definitions take care of the rest. e. Now you need to make a one-time change in order to see modules that you put in this collection ( /scratch/$USER/modules/default/modulefiles ). In your $HOME/.bash_profile , add the following line near the top: module use /scratch/$USER/modules/default/modulefiles Obviously, if you've built packages before and enabled them this way, you don't have to do this again! Now logout, log back in, and you should see your personal modules collection with a brand new module. [cchang@el1 ~]$ module avail ---------------------------------- /scratch/cchang/modules/default/modulefiles ----------------------------------- openmpi/4.1.0/gcc-8.4.0 Notice that the \".lua\" extension does not appear--the converse is also true, if the extension is missing it will not appear via module commands! As a sanity check, it's a good idea to load the module, and check that an executable file you know exists there is in fact on your PATH: [cchang@el1 ~]$ module load openmpi/4.1.0/gcc-8.4.0 [cchang@el1 ~]$ which mpirun /scratch/cchang/openmpi/4.1.0-gcc-8.4.0/bin/mpirun","title":"Setting up your module"},{"location":"Documentation/Systems/","text":"NREL Systems NREL operates three on-premises systems for computational work. System configurations Name Eagle Swift Vermilion OS CentOS Rocky Linux RedHat Login eagle.hpc.nrel.gov swift.hpc.nrel.gov vs.hpc.nrel.gov CPU Dual Intel Xeon Gold Skylake 6154 Dual AMD EPYC 7532 Rome CPU Dual AMD EPYC 7532 Rome CPU Interconnect InfiniBand EDR InfiniBand HDR 25GbE HPC scheduler Slurm Slurm Slurm Network Storage 17PB Lustre FS 3PB NFS 440 TB GPU Dual NVIDIA Tesla V100 None 5 nodes Single A100 Memory 96GB, 192GB, 768GB 256GB 256GB (base) Number of Nodes 2618 484 133 virtual","title":"Systems"},{"location":"Documentation/Systems/#nrel-systems","text":"NREL operates three on-premises systems for computational work.","title":"NREL Systems"},{"location":"Documentation/Systems/#system-configurations","text":"Name Eagle Swift Vermilion OS CentOS Rocky Linux RedHat Login eagle.hpc.nrel.gov swift.hpc.nrel.gov vs.hpc.nrel.gov CPU Dual Intel Xeon Gold Skylake 6154 Dual AMD EPYC 7532 Rome CPU Dual AMD EPYC 7532 Rome CPU Interconnect InfiniBand EDR InfiniBand HDR 25GbE HPC scheduler Slurm Slurm Slurm Network Storage 17PB Lustre FS 3PB NFS 440 TB GPU Dual NVIDIA Tesla V100 None 5 nodes Single A100 Memory 96GB, 192GB, 768GB 256GB 256GB (base) Number of Nodes 2618 484 133 virtual","title":"System configurations"},{"location":"Documentation/Systems/Swift/","text":"About the Swift Cluster Swift is an NREL supercomputing cluster housed in the ESIF data center for LDRD projects, as an alternative to the NREL flagship cluster Eagle. Swift is an AMD-based cluster, featuring 484 nodes with two EPYC 7532 (32 core/64 thread) CPUs and 256GB RAM each. Please see the System Configurations page for more information about hardware, storage, and networking. Accessing Swift Access to Swift requires an NREL HPC account and permission to join an existing allocation. Please see the System Access page for more information on accounts and allocations. For NREL Employees: Swift can be reached from the NREL VPN via ssh to swift.hpc.nrel.gov, swift-login-1.hpc.nrel.gov, or swift-login-2.hpc.nrel.gov. For External Collaborators: There are currently no external-facing login nodes for Swift. There are two options to connect: ssh hpcsh.nrel.gov and log in with your username, password, and OTP code. Once connected, ssh to the login nodes as above. Connect to the HPC VPN and ssh to the login nodes as above. Get Help With Swift The Known Issues and Answers page has answers to common Swift questions. Please see the Help and Support Page for further information on how to seek assistance with Swift or your NREL HPC account.","title":"Swift"},{"location":"Documentation/Systems/Swift/#about-the-swift-cluster","text":"Swift is an NREL supercomputing cluster housed in the ESIF data center for LDRD projects, as an alternative to the NREL flagship cluster Eagle. Swift is an AMD-based cluster, featuring 484 nodes with two EPYC 7532 (32 core/64 thread) CPUs and 256GB RAM each. Please see the System Configurations page for more information about hardware, storage, and networking.","title":"About the Swift Cluster"},{"location":"Documentation/Systems/Swift/#accessing-swift","text":"Access to Swift requires an NREL HPC account and permission to join an existing allocation. Please see the System Access page for more information on accounts and allocations.","title":"Accessing Swift"},{"location":"Documentation/Systems/Swift/#for-nrel-employees","text":"Swift can be reached from the NREL VPN via ssh to swift.hpc.nrel.gov, swift-login-1.hpc.nrel.gov, or swift-login-2.hpc.nrel.gov.","title":"For NREL Employees:"},{"location":"Documentation/Systems/Swift/#for-external-collaborators","text":"There are currently no external-facing login nodes for Swift. There are two options to connect: ssh hpcsh.nrel.gov and log in with your username, password, and OTP code. Once connected, ssh to the login nodes as above. Connect to the HPC VPN and ssh to the login nodes as above.","title":"For External Collaborators:"},{"location":"Documentation/Systems/Swift/#get-help-with-swift","text":"The Known Issues and Answers page has answers to common Swift questions. Please see the Help and Support Page for further information on how to seek assistance with Swift or your NREL HPC account.","title":"Get Help With Swift"},{"location":"Documentation/Systems/Swift/applications/","text":"Swift applications Some optimized versions of common applications are provided for the Swift cluster. Below is a list of how to utilize these applications and the optimizations for Swift. Modules Many are available as part of the Modules setup. TensorFlow TensorFlow has been built for the AMD architecture on Swift. This was done by using the following two build flags. -march=znver2 -mtune=znver2 This version of TensorFlow can be installed from a wheel file: pip install --upgrade --no-deps --force-reinstall /nopt/nrel/apps/wheels/tensorflow-2.4.2-cp38-cp38-linux_x86_64-cpu.whl Currently, this wheel is not built with NVIDIA CUDA support for running on GPU. TensorFlow installed on Swift with Conda may be significantly slower than the optimized version","title":"Applications"},{"location":"Documentation/Systems/Swift/applications/#swift-applications","text":"Some optimized versions of common applications are provided for the Swift cluster. Below is a list of how to utilize these applications and the optimizations for Swift.","title":"Swift applications"},{"location":"Documentation/Systems/Swift/applications/#modules","text":"Many are available as part of the Modules setup.","title":"Modules"},{"location":"Documentation/Systems/Swift/applications/#tensorflow","text":"TensorFlow has been built for the AMD architecture on Swift. This was done by using the following two build flags. -march=znver2 -mtune=znver2 This version of TensorFlow can be installed from a wheel file: pip install --upgrade --no-deps --force-reinstall /nopt/nrel/apps/wheels/tensorflow-2.4.2-cp38-cp38-linux_x86_64-cpu.whl Currently, this wheel is not built with NVIDIA CUDA support for running on GPU. TensorFlow installed on Swift with Conda may be significantly slower than the optimized version","title":"TensorFlow"},{"location":"Documentation/Systems/Swift/filesystems/","text":"Swift Filesystem Architecture Overview Swift's central storage currently has a capacity of approximately 3PB, served over NFS (Network File System). It is a performant system with multiple read and write cache layers and redundancies for data protection, but it is not a parallel filesystem, unlike Eagle's Lustre configuration. The underlying filesystem and volume management is via ZFS. Data is protected in ZFS RAID arrangements (raidz3) of 8 storage disks and 3 parity disks. Each Swift fileserver serves a single storage chassis (JBOD, \"just a bunch of disks\") consisting of multiple spinning disks plus SSD drives for read and write caches. Each fileserver is also connected to a second storage chassis to serve as a redundant backup in case the primary fileserver for that storage chassis fails, allowing continued access to the data on the storage chassis until the primary fileserver for that chassis is restored to service. Project Storage: /projects Each active project is granted a subdirectory under /projects/<projectname> . This is where the bulk of data is expected to be, and where jobs should generally be run from. Storage quotas are based on the allocation award. Quota usage can be viewed at any time by issuing a cd command into the project directory, and using the df -h command to view total, used, and remaining available space for the mounted project directory. NFS Automount System Project directories are automatically mounted or unmounted via NFS on an \"as-needed\" basis. /projects directories that have not been accessed for a period of time will be umounted and not immediately visible via a command such as ls /projects , but will become immediately available if a file or path is accessed with an ls , cd , or other file access is made in that path. Home Directories: /home /home directories are mounted as /home/<username> . Home directories are hosted under the user's initial /project directory. Quotas in /home are included as a part of the quota of that project's storage allocation. Scratch Space: /scratch/username and /scratch/username/jobid For users who also have Eagle allocations, please be aware that scratch space on Swift behaves differently, so adjustments to job scripts may be necessary. The scratch directory on each Swift compute node is a 1.8TB spinning disk, and is accessible only on that node. The default writable path for scratch use is /scratch/<username> . There is no global, network-accessible /scratch space. /projects and /home are both network-accessible, and may be used as /scratch-style working space instead. Temporary space: $TMPDIR When a job starts, the environment variable $TMPDIR is set to /scratch/<username>/<jobid> for the duration of the job. This is temporary space only, and should be purged when your job is complete. Please be sure to use this path instead of /tmp for your tempfiles. There is no expectation of data longevity in scratch space, and it is subject to purging once the node is idle. If desired data is stored here during the job, please be sure to copy it to a /projects directory as part of the job script before the job finishes. Mass Storage System There is no Mass Storage System for deep archive storage on Swift. However, Swift is expected to be a part of the upcoming Campaign Storage system (VAST storage) in the future, allowing those projects with allocations on Eagle to seamlessly transfer data between clusters, and into the Eagle MSS system. Backups and Snapshots There are no backups or snapshots of data on Swift. Though the system is protected from hardware failure by multiple layers of redundancy, please keep regular backups of important data on Swift, and consider using a Version Control System (such as Git) for important code.","title":"Filesystems"},{"location":"Documentation/Systems/Swift/filesystems/#swift-filesystem-architecture-overview","text":"Swift's central storage currently has a capacity of approximately 3PB, served over NFS (Network File System). It is a performant system with multiple read and write cache layers and redundancies for data protection, but it is not a parallel filesystem, unlike Eagle's Lustre configuration. The underlying filesystem and volume management is via ZFS. Data is protected in ZFS RAID arrangements (raidz3) of 8 storage disks and 3 parity disks. Each Swift fileserver serves a single storage chassis (JBOD, \"just a bunch of disks\") consisting of multiple spinning disks plus SSD drives for read and write caches. Each fileserver is also connected to a second storage chassis to serve as a redundant backup in case the primary fileserver for that storage chassis fails, allowing continued access to the data on the storage chassis until the primary fileserver for that chassis is restored to service.","title":"Swift Filesystem Architecture Overview"},{"location":"Documentation/Systems/Swift/filesystems/#project-storage-projects","text":"Each active project is granted a subdirectory under /projects/<projectname> . This is where the bulk of data is expected to be, and where jobs should generally be run from. Storage quotas are based on the allocation award. Quota usage can be viewed at any time by issuing a cd command into the project directory, and using the df -h command to view total, used, and remaining available space for the mounted project directory.","title":"Project Storage: /projects"},{"location":"Documentation/Systems/Swift/filesystems/#nfs-automount-system","text":"Project directories are automatically mounted or unmounted via NFS on an \"as-needed\" basis. /projects directories that have not been accessed for a period of time will be umounted and not immediately visible via a command such as ls /projects , but will become immediately available if a file or path is accessed with an ls , cd , or other file access is made in that path.","title":"NFS Automount System"},{"location":"Documentation/Systems/Swift/filesystems/#home-directories-home","text":"/home directories are mounted as /home/<username> . Home directories are hosted under the user's initial /project directory. Quotas in /home are included as a part of the quota of that project's storage allocation.","title":"Home Directories: /home"},{"location":"Documentation/Systems/Swift/filesystems/#scratch-space-scratchusername-and-scratchusernamejobid","text":"For users who also have Eagle allocations, please be aware that scratch space on Swift behaves differently, so adjustments to job scripts may be necessary. The scratch directory on each Swift compute node is a 1.8TB spinning disk, and is accessible only on that node. The default writable path for scratch use is /scratch/<username> . There is no global, network-accessible /scratch space. /projects and /home are both network-accessible, and may be used as /scratch-style working space instead.","title":"Scratch Space: /scratch/username and /scratch/username/jobid"},{"location":"Documentation/Systems/Swift/filesystems/#temporary-space-tmpdir","text":"When a job starts, the environment variable $TMPDIR is set to /scratch/<username>/<jobid> for the duration of the job. This is temporary space only, and should be purged when your job is complete. Please be sure to use this path instead of /tmp for your tempfiles. There is no expectation of data longevity in scratch space, and it is subject to purging once the node is idle. If desired data is stored here during the job, please be sure to copy it to a /projects directory as part of the job script before the job finishes.","title":"Temporary space: $TMPDIR"},{"location":"Documentation/Systems/Swift/filesystems/#mass-storage-system","text":"There is no Mass Storage System for deep archive storage on Swift. However, Swift is expected to be a part of the upcoming Campaign Storage system (VAST storage) in the future, allowing those projects with allocations on Eagle to seamlessly transfer data between clusters, and into the Eagle MSS system.","title":"Mass Storage System"},{"location":"Documentation/Systems/Swift/filesystems/#backups-and-snapshots","text":"There are no backups or snapshots of data on Swift. Though the system is protected from hardware failure by multiple layers of redundancy, please keep regular backups of important data on Swift, and consider using a Version Control System (such as Git) for important code.","title":"Backups and Snapshots"},{"location":"Documentation/Systems/Swift/help/","text":"Swift Technical Support Contacts Please direct all inquiries regarding Swift to the HPC Helpdesk at HPC-Help@nrel.gov , including: Technical issues or questions Software installation requests or assistance Account or allocation issues All other general inquiries regarding Swift Microsoft Teams More information on joining the Microsoft Teams Swift channel coming soon. Additional Support Additional HPC help and contact information can be found on the NREL HPC Help main page.","title":"Help and Support"},{"location":"Documentation/Systems/Swift/help/#swift-technical-support-contacts","text":"Please direct all inquiries regarding Swift to the HPC Helpdesk at HPC-Help@nrel.gov , including: Technical issues or questions Software installation requests or assistance Account or allocation issues All other general inquiries regarding Swift","title":"Swift Technical Support Contacts"},{"location":"Documentation/Systems/Swift/help/#microsoft-teams","text":"More information on joining the Microsoft Teams Swift channel coming soon.","title":"Microsoft Teams"},{"location":"Documentation/Systems/Swift/help/#additional-support","text":"Additional HPC help and contact information can be found on the NREL HPC Help main page.","title":"Additional Support"},{"location":"Documentation/Systems/Swift/known/","text":"Swift Known Issues and Solutions MPI IntelMPI appears to be working properly. OpenMPI appears to be working properly. Hardware There are no GPU nodes currently available on Swift. Software/Environment There is a very basic version of conda in the \"anaconda\" directory in each /nopt/nrel/apps/YYMMDDa directory. However, there is a more complete environment pointed to by the module under /nopt/nrel/apps/modules. This is set up like Eagle. Please see Eagle's Python Documentation for more information. User accounts have a default set of keys cluster and cluster.pub . The config file will use these even if you generate a new keypair using ssh-keygen . If you are adding your keys to Github or elsewhere you should either use cluster.pub or will have to modify the config file. Job Scheduling Use --cpus-per-task with srun/sbatch otherwise some applications may only utilize a single core. This behavior differs from Eagle. By default, nodes can be shared between users. To get exclusive access to a node use the --exclusive flag in your sbatch script or on the sbatch command line. There are some example slurm scripts in the example directory. How to Get Help Please visit the Help and Support Page for assistance or to report an issue.","title":"Known Issues/FAQ"},{"location":"Documentation/Systems/Swift/known/#swift-known-issues-and-solutions","text":"","title":"Swift Known Issues and Solutions"},{"location":"Documentation/Systems/Swift/known/#mpi","text":"IntelMPI appears to be working properly. OpenMPI appears to be working properly.","title":"MPI"},{"location":"Documentation/Systems/Swift/known/#hardware","text":"There are no GPU nodes currently available on Swift.","title":"Hardware"},{"location":"Documentation/Systems/Swift/known/#softwareenvironment","text":"There is a very basic version of conda in the \"anaconda\" directory in each /nopt/nrel/apps/YYMMDDa directory. However, there is a more complete environment pointed to by the module under /nopt/nrel/apps/modules. This is set up like Eagle. Please see Eagle's Python Documentation for more information. User accounts have a default set of keys cluster and cluster.pub . The config file will use these even if you generate a new keypair using ssh-keygen . If you are adding your keys to Github or elsewhere you should either use cluster.pub or will have to modify the config file.","title":"Software/Environment"},{"location":"Documentation/Systems/Swift/known/#job-scheduling","text":"Use --cpus-per-task with srun/sbatch otherwise some applications may only utilize a single core. This behavior differs from Eagle. By default, nodes can be shared between users. To get exclusive access to a node use the --exclusive flag in your sbatch script or on the sbatch command line. There are some example slurm scripts in the example directory.","title":"Job Scheduling"},{"location":"Documentation/Systems/Swift/known/#how-to-get-help","text":"Please visit the Help and Support Page for assistance or to report an issue.","title":"How to Get Help"},{"location":"Documentation/Systems/Swift/modules/","text":"Swift modules This describes how to activate and use the modules available on Swift. There are currently a number of known issues on Swift pleace check Known issues for a complete list Source Environments are provided with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. When you first login there is a default set of modules available. These can be seen by running the command: module avail Since Swift is a new machine we are experimenting with additional environments. The environments are in date stamped subdirectory under in the directory /nopt/nrel/apps. Each environemnt directory has a file myenv.*. If the myenv.*. is missing from a directory then that environment is a work in progress. Sourcing myenv.* file will enable the environment and give you a new set of modules. For example to enable the environment /nopt/nrel/apps/210728a source the provided environment file. source /nopt/nrel/apps/210728a/myenv.2107290127 You will now have access to the modules provided. These can be listed using the following: ml avail If you want to build applications you can then module load compilers and the like; for example ml gcc openmpi will load gnu 9.4 and openmpi. Software is installed using a spack hierarchy. It is possible to add software to the hierarchy. This should be only done by people responsible for installing software for all users. It is also possible to do a spack install creating a new level of the hierarchy in your personal space. These procedures are documented in https://github.nrel.gov/tkaiser2/spackit.git in the file Notes03.md under the sections Building on the hierarchy and Building outside the hierarchy . If you want to try this please contact Tim Kaiser tkaiser2@nrel.gov to walk through the procedure. Most environments have an example directory. You can copy this directory to you own space and compile and run the examples. The files runintel and runopenmp are simple batch scripts. These also have \"module load\" lines that you need to run before building with either compiler set.","title":"Modules"},{"location":"Documentation/Systems/Swift/modules/#swift-modules","text":"This describes how to activate and use the modules available on Swift. There are currently a number of known issues on Swift pleace check Known issues for a complete list","title":"Swift modules"},{"location":"Documentation/Systems/Swift/modules/#source","text":"Environments are provided with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. When you first login there is a default set of modules available. These can be seen by running the command: module avail Since Swift is a new machine we are experimenting with additional environments. The environments are in date stamped subdirectory under in the directory /nopt/nrel/apps. Each environemnt directory has a file myenv.*. If the myenv.*. is missing from a directory then that environment is a work in progress. Sourcing myenv.* file will enable the environment and give you a new set of modules. For example to enable the environment /nopt/nrel/apps/210728a source the provided environment file. source /nopt/nrel/apps/210728a/myenv.2107290127 You will now have access to the modules provided. These can be listed using the following: ml avail If you want to build applications you can then module load compilers and the like; for example ml gcc openmpi will load gnu 9.4 and openmpi. Software is installed using a spack hierarchy. It is possible to add software to the hierarchy. This should be only done by people responsible for installing software for all users. It is also possible to do a spack install creating a new level of the hierarchy in your personal space. These procedures are documented in https://github.nrel.gov/tkaiser2/spackit.git in the file Notes03.md under the sections Building on the hierarchy and Building outside the hierarchy . If you want to try this please contact Tim Kaiser tkaiser2@nrel.gov to walk through the procedure. Most environments have an example directory. You can copy this directory to you own space and compile and run the examples. The files runintel and runopenmp are simple batch scripts. These also have \"module load\" lines that you need to run before building with either compiler set.","title":"Source"},{"location":"Documentation/Systems/Swift/running/","text":"Running on Swift Please see the Modules page for information about setting up your environment and loading modules. There are currently a number of known issues on Swift please check Known issues for a complete list Login nodes swift.hpc.nrel.gov swift-login-1.hpc.nrel.gov swift-login-2.hpc.nrel.gov Slurm and Partitions As more of Swift is brought on line different partitions will be created. A list of partitions can be returned by sunning the sinfo command. Example Environments are provided with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. The environments are in date stamped subdirectories under in the directory /nopt/nrel/apps. Each environment directory has a file myenv.*. Sourcing that file will enable the environment. When you login you will have access to the default environments and the myenv file will have been sourced for you. You can see the directory containing the environment by runnint the module avail command. In the directory for an environment you will see a subdirectory example . This contains a makefile for a simple hello world program written in both Fortran and C. The README.md file contains additional information, most of which is replicated here. It is suggested you cp -r example ~/example cd ~/example Simple batch script Here is a sample batch script for running the hello world examples runopenmpi . #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --tasks-per-node=2 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 #These should be loaded before doing a make module load gcc openmpi export OMP_NUM_THREADS = 2 srun -n 4 ./fhostone -F srun -n 4 ./phostone -F To run this you must first ensure that slurm is in your path by running: module load slurm Then sbatch --partition=test runopenmpi Building hello world first Obviously for the script given above to work you must first build the application. You need to: Load the modules make Loading the modules. We are going to use gnu compilers with OpenMPI. ml gcc openmpi Run make make Full procedure [ nrmc2l@swift-login-1 ~ ] $ cd ~ [ nrmc2l@swift-login-1 ~ ] $ mkdir example [ nrmc2l@swift-login-1 ~ ] $ cd ~/example [ nrmc2l@swift-login-1 ~ ] $ cp -r /nopt/nrel/apps/210928a/example/* . [ nrmc2l@swift-login-1 ~ example ] $ cat runopenmpi #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --tasks-per-node=2 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 #These should be loaded before doing a make module load gcc openmpi export OMP_NUM_THREADS = 2 srun -n 4 ./fhostone -F srun -n 4 ./phostone -F [ nrmc2l@swift-login-1 ~ example ] $ module load gcc openmpi [ nrmc2l@swift-login-1 ~ example ] $ make mpif90 -fopenmp fhostone.f90 -o fhostone rm getit.mod mympi.mod numz.mod mpicc -fopenmp phostone.c -o phostone [ nrmc2l@swift-login-1 ~ example ] $ sbatch runopenmpi Submitted batch job 187 [ nrmc2l@swift-login-1 ~ example ] $ Results [ nrmc2l@swift-login-1 example ] $ cat *312985* #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --tasks-per-node=2 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 #These should be loaded before doing a make module load gcc openmpi export OMP_NUM_THREADS = 2 srun -n 4 ./fhostone -F srun -n 4 ./phostone -F MPI Version:Open MPI v4.1.1, package: Open MPI nrmc2l@swift-login-1.swift.hpc.nrel.gov Distribution, ident: 4 .1.1, repo rev: v4.1.1, Apr 24 , 2021 task thread node name first task # on node core 0002 0000 c1-31 0002 0000 018 0000 0000 c1-30 0000 0000 072 0000 0001 c1-30 0000 0000 095 0001 0000 c1-30 0000 0001 096 0001 0001 c1-30 0000 0001 099 0002 0001 c1-31 0002 0000 085 0003 0000 c1-31 0002 0001 063 0003 0001 c1-31 0002 0001 099 0001 0000 c1-30 0000 0001 0097 0001 0001 c1-30 0000 0001 0103 0003 0000 c1-31 0002 0001 0062 0003 0001 c1-31 0002 0001 0103 MPI VERSION Open MPI v4.1.1, package: Open MPI nrmc2l@swift-login-1.swift.hpc.nrel.gov Distribution, ident: 4 .1.1, repo rev: v4.1.1, Apr 24 , 2021 task thread node name first task # on node core 0000 0000 c1-30 0000 0000 0072 0000 0001 c1-30 0000 0000 0020 0002 0000 c1-31 0002 0000 0000 0002 0001 c1-31 0002 0000 0067 [ nrmc2l@swift-login-1 example ] $ Building with Intel Fortran or Intel C and OpenMPI You can build parallel programs using OpenMPI and the Intel Fortran ifort and Intel C icc compilers. We have the example programs build with gnu compilers and OpenMP using the lines: [ nrmc2l@swift-login-1 ~ example ] $ mpif90 -fopenmp fhostone.f90 -o fhostone [ nrmc2l@swift-login-1 ~ example ] $ mpicc -fopenmp phostone.c -o phostone This gives us: [ nrmc2l@swift-login-1 ~ example ] $ ls -l fhostone -rwxrwxr-x. 1 nrmc2l nrmc2l 36880 Jul 30 13 :36 fhostone [ nrmc2l@swift-login-1 ~ example ] $ ls -l phostone -rwxrwxr-x. 1 nrmc2l nrmc2l 27536 Jul 30 13 :36 phostone Note the size of the executable files. If you want to use the Intel compilers you first do a module load. module load intel-oneapi-mpi intel-oneapi-compilers gcc Then we can set the variables OMPI_FC=ifort and OMPI_CC=icc . Then recompile. [ nrmc2l@swift-login-1 ~ example ] $ export OMPI_FC = ifort [ nrmc2l@swift-login-1 ~ example ] $ export OMPI_CC = icc [ nrmc2l@swift-login-1 ~ example ] $ mpif90 -fopenmp fhostone.f90 -o fhostone [ nrmc2l@swift-login-1 ~ example ] $ mpicc -fopenmp phostone.c -o phostone [ nrmc2l@swift-login-1 ~ example ] $ ls -lt fhostone -rwxrwxr-x. 1 nrmc2l nrmc2l 951448 Jul 30 13 :37 fhostone [ nrmc2l@swift-login-1 ~ example ] $ ls -lt phostone -rwxrwxr-x. 1 nrmc2l nrmc2l 155856 Jul 30 13 :37 phostone [ nrmc2l@swift-login-1 ~ example ] $ Note the size of the executable files have changed. You can also see the difference by running the commands nm fhostone | grep intel | wc nm phostone | grep intel | wc on the two versions of the program. It will show how many calls to Intel routines are in each, 51 and 36 compared to 0 for the gnu versions. Building and Running with Intel MPI We can build with the Intel versions of MPI. We assume we will want to build with icc and ifort as the backend compilers. We load the modules: ml gcc ml intel-oneapi-compilers ml intel-oneapi-mpi Then, building and running the same example as above: make clean make PFC = mpiifort PCC = mpiicc Giving us: [ nrmc2l@swift-login-1 example ] $ ls -lt fhostone phostone -rwxrwxr-x. 1 nrmc2l hpcapps 155696 Aug 5 16 :14 phostone -rwxrwxr-x. 1 nrmc2l hpcapps 947112 Aug 5 16 :14 fhostone [ nrmc2l@swift-login-1 example ] $ We need to make some changes to our batch script. Replace the module load line with : module load intel-oneapi-mpi intel-oneapi-compilers gcc Launch with the srun command: srun ./a.out -F Our IntelMPI batch script is: #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --tasks-per-node=2 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 #These should be loaded before doing a make module load intel-oneapi-mpi intel-oneapi-compilers gcc export OMP_NUM_THREADS = 2 srun -n 4 ./fhostone -F srun -n 4 ./phostone -F With output MPI Version:Intel ( R ) MPI Library 2021 .3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 127 0000 0001 c1-32 0000 0000 097 0001 0000 c1-32 0000 0001 062 0001 0001 c1-32 0000 0001 099 MPI VERSION Intel ( R ) MPI Library 2021 .3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 0127 0000 0001 c1-32 0000 0000 0097 0001 0000 c1-32 0000 0001 0127 0001 0001 c1-32 0000 0001 0099 Running VASP The batch script given above can be modified to run VASP. You need to add ml vasp This will give you: [ nrmc2l@swift-login-1 ~ example ] $ which vasp_gam /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_gam [ nrmc2l@swift-login-1 ~ example ] $ which vasp_ncl /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_ncl [ nrmc2l@swift-login-1 ~ example ] $ which vasp_std /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_std [ nrmc2l@swift-login-1 ~ example ] $ Note the directory might be different. Then you need to add calls in your script to set up / point do your data files. So your final script will look something like the following. Here we use data downloaded from NREL's benchmark repository. #!/bin/bash #SBATCH --job-name=b2_4 #SBATCH --nodes=1 #SBATCH --time=4:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=debug #SBATCH --exclusive cat $0 hostname module purge ml openmpi gcc vasp #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 mkdir $SLURM_JOB_ID cp input/* $SLURM_JOB_ID cd $SLURM_JOB_ID srun -n 16 vasp_std > vasp. $SLURM_JOB_ID This will run a version of Vasp built with openmpi and gfortran/gcc. You can run a version of Vasp built with the Intel toolchain replacing the ml line with ml vaspintel intel-oneapi-mpi intel-oneapi-compilers intel-oneapi-mkl Running Jupyter / Jupyter-lab Jupyter and Jupyter-lab are available by loading the module \"python\" [ nrmc2l@swift-login-1 ~ ] $ ml python [ nrmc2l@swift-login-1 ~ ] $ which python /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/bin/python [ nrmc2l@swift-login-1 ~ ] $ which jupyter /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/bin/jupyter [ nrmc2l@swift-login-1 ~ ] $ which jupyter-lab /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/bin/jupyter-lab [ nrmc2l@swift-login-1 ~ ] $ It is recomended that you use the --no-browser option and connect to your notebook from your desktop using a ssh tunnel and web browser. On Swift enter the command and note the URLs. [ nrmc2l@swift-login-1 ~ ] $ jupyter-lab --no-browser [ I 2022 -03-30 07 :54:25.937 ServerApp ] jupyterlab | extension was successfully linked. [ I 2022 -03-30 07 :54:26.224 ServerApp ] nbclassic | extension was successfully linked. [ I 2022 -03-30 07 :54:26.255 ServerApp ] nbclassic | extension was successfully loaded. [ I 2022 -03-30 07 :54:26.257 LabApp ] JupyterLab extension loaded from /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/lib/python3.10/site-packages/jupyterlab [ I 2022 -03-30 07 :54:26.257 LabApp ] JupyterLab application directory is /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/share/jupyter/lab [ I 2022 -03-30 07 :54:26.260 ServerApp ] jupyterlab | extension was successfully loaded. [ I 2022 -03-30 07 :54:26.261 ServerApp ] Serving notebooks from local directory: /home/nrmc2l [ I 2022 -03-30 07 :54:26.261 ServerApp ] Jupyter Server 1 .11.1 is running at: [ I 2022 -03-30 07 :54:26.261 ServerApp ] http://localhost:8888/lab?token = 183d33c61bb136f8d04b83c70c4257a976060dd84afc9156 [ I 2022 -03-30 07 :54:26.261 ServerApp ] or http://127.0.0.1:8888/lab?token = 183d33c61bb136f8d04b83c70c4257a976060dd84afc9156 [ I 2022 -03-30 07 :54:26.261 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -03-30 07 :54:26.266 ServerApp ] To access the server, open this file in a browser: file:///home/nrmc2l/.local/share/jupyter/runtime/jpserver-2056000-open.html Or copy and paste one of these URLs: http://localhost:8888/lab?token = 183d33c61bb136f8d04b83c70c4257a976060dd84afc9156 or http://127.0.0.1:8888/lab?token = 183d33c61bb136f8d04b83c70c4257a976060dd84afc9156 Note the 8888 in the URL it might be different. On your desktop in a new terminal window enter the command: ssh -t -L 8888 :localhost:8888 swift-login-1.hpc.nrel.gov replacing 8888 with the number in the URL if it is different. Then in a web browser window past the URL. This should bring up a new notebook. Running Jupyter / Jupyter-lab on a compute node You can get an interactive session on a compute node by running the command [ nrmc2l@swift-login-1 ~ ] $ salloc --account = hpcapps --exclusive --time = 01 :00:00 --ntasks = 16 --nodes = 1 --partition = debug but replacing hpcapps with your account. After you get an session module load python and run as shown above. [ nrmc2l@swift-login-1 ~ ] $ salloc --account = hpcapps --exclusive --time = 01 :00:00 --ntasks = 16 --nodes = 1 --partition = debug salloc: Pending job allocation 313001 salloc: job 313001 queued and waiting for resources salloc: job 313001 has been allocated resources salloc: Granted job allocation 313001 [ nrmc2l@c1-28 ~ ] $ [ nrmc2l@c1-28 ~ ] $ module load python [ nrmc2l@c1-28 ~ ] $ [ nrmc2l@c1-28 ~ ] $ jupyter-lab --no-browser [ I 2022 -03-30 08 :04:28.063 ServerApp ] jupyterlab | extension was successfully linked. [ I 2022 -03-30 08 :04:28.468 ServerApp ] nbclassic | extension was successfully linked. [ I 2022 -03-30 08 :04:28.508 ServerApp ] nbclassic | extension was successfully loaded. [ I 2022 -03-30 08 :04:28.509 LabApp ] JupyterLab extension loaded from /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/lib/python3.10/site-packages/jupyterlab [ I 2022 -03-30 08 :04:28.509 LabApp ] JupyterLab application directory is /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/share/jupyter/lab [ I 2022 -03-30 08 :04:28.513 ServerApp ] jupyterlab | extension was successfully loaded. [ I 2022 -03-30 08 :04:28.513 ServerApp ] Serving notebooks from local directory: /home/nrmc2l [ I 2022 -03-30 08 :04:28.514 ServerApp ] Jupyter Server 1 .11.1 is running at: [ I 2022 -03-30 08 :04:28.514 ServerApp ] http://localhost:8888/lab?token = cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf [ I 2022 -03-30 08 :04:28.514 ServerApp ] or http://127.0.0.1:8888/lab?token = cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf [ I 2022 -03-30 08 :04:28.514 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -03-30 08 :04:28.519 ServerApp ] To access the server, open this file in a browser: file:///home/nrmc2l/.local/share/jupyter/runtime/jpserver-3375148-open.html Or copy and paste one of these URLs: http://localhost:8888/lab?token = cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf or http://127.0.0.1:8888/lab?token = cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf On your desktop run the command: ssh -t -L 8888 :localhost:8475 swift-login-1 ssh -L 8475 :localhost:8888 c1-28 replacing 8888 with the value in the URL if needed and c1-28 with the name of the compute node on which you are running. Then again paste the URL in a web browser. You should get a notebook running on the compute node. Running Julia Julia is also available via a module. [ nrmc2l@swift-login-1:~ ] $ module spider julia ... Versions: julia/1.6.2-ocsfign julia/1.7.2-gdp7a25 ... [ nrmc2l@swift-login-1:~ ] $ [ nrmc2l@swift-login-1:~/examples/spack ] $ module load julia/1.7.2-gdp7a25 [ nrmc2l@swift-login-1:~/examples/spack ] $ which julia /nopt/nrel/apps/210928a/level03/install/linux-rocky8-zen2/gcc-9.4.0/julia-1.7.2-gdp7a253nsglyzssybqknos2n5amkvqm/bin/julia [ nrmc2l@swift-login-1:~/examples/spack ] $ Julia can be run in a Jupyter notebook as discussed above. However, before doing so you will need to run the following commands in each Julia version you are using. julia> using Pkg julia> Pkg.add ( \"IJulia\" ) See https://datatofish.com/add-julia-to-jupyter/ more more information. If you would like to install your own copy of Julia complete with Jupyter-lab contact Tim Kaiser tkaiser2@nrel.gov for a script to do so.","title":"Running on Swift"},{"location":"Documentation/Systems/Swift/running/#running-on-swift","text":"Please see the Modules page for information about setting up your environment and loading modules. There are currently a number of known issues on Swift please check Known issues for a complete list","title":"Running on Swift"},{"location":"Documentation/Systems/Swift/running/#login-nodes","text":"swift.hpc.nrel.gov swift-login-1.hpc.nrel.gov swift-login-2.hpc.nrel.gov","title":"Login nodes"},{"location":"Documentation/Systems/Swift/running/#slurm-and-partitions","text":"As more of Swift is brought on line different partitions will be created. A list of partitions can be returned by sunning the sinfo command.","title":"Slurm and Partitions"},{"location":"Documentation/Systems/Swift/running/#example","text":"Environments are provided with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. The environments are in date stamped subdirectories under in the directory /nopt/nrel/apps. Each environment directory has a file myenv.*. Sourcing that file will enable the environment. When you login you will have access to the default environments and the myenv file will have been sourced for you. You can see the directory containing the environment by runnint the module avail command. In the directory for an environment you will see a subdirectory example . This contains a makefile for a simple hello world program written in both Fortran and C. The README.md file contains additional information, most of which is replicated here. It is suggested you cp -r example ~/example cd ~/example","title":"Example"},{"location":"Documentation/Systems/Swift/running/#simple-batch-script","text":"Here is a sample batch script for running the hello world examples runopenmpi . #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --tasks-per-node=2 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 #These should be loaded before doing a make module load gcc openmpi export OMP_NUM_THREADS = 2 srun -n 4 ./fhostone -F srun -n 4 ./phostone -F To run this you must first ensure that slurm is in your path by running: module load slurm Then sbatch --partition=test runopenmpi","title":"Simple batch script"},{"location":"Documentation/Systems/Swift/running/#building-hello-world-first","text":"Obviously for the script given above to work you must first build the application. You need to: Load the modules make","title":"Building hello world first"},{"location":"Documentation/Systems/Swift/running/#loading-the-modules","text":"We are going to use gnu compilers with OpenMPI. ml gcc openmpi","title":"Loading the modules."},{"location":"Documentation/Systems/Swift/running/#run-make","text":"make","title":"Run make"},{"location":"Documentation/Systems/Swift/running/#full-procedure","text":"[ nrmc2l@swift-login-1 ~ ] $ cd ~ [ nrmc2l@swift-login-1 ~ ] $ mkdir example [ nrmc2l@swift-login-1 ~ ] $ cd ~/example [ nrmc2l@swift-login-1 ~ ] $ cp -r /nopt/nrel/apps/210928a/example/* . [ nrmc2l@swift-login-1 ~ example ] $ cat runopenmpi #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --tasks-per-node=2 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 #These should be loaded before doing a make module load gcc openmpi export OMP_NUM_THREADS = 2 srun -n 4 ./fhostone -F srun -n 4 ./phostone -F [ nrmc2l@swift-login-1 ~ example ] $ module load gcc openmpi [ nrmc2l@swift-login-1 ~ example ] $ make mpif90 -fopenmp fhostone.f90 -o fhostone rm getit.mod mympi.mod numz.mod mpicc -fopenmp phostone.c -o phostone [ nrmc2l@swift-login-1 ~ example ] $ sbatch runopenmpi Submitted batch job 187 [ nrmc2l@swift-login-1 ~ example ] $","title":"Full procedure"},{"location":"Documentation/Systems/Swift/running/#results","text":"[ nrmc2l@swift-login-1 example ] $ cat *312985* #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --tasks-per-node=2 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 #These should be loaded before doing a make module load gcc openmpi export OMP_NUM_THREADS = 2 srun -n 4 ./fhostone -F srun -n 4 ./phostone -F MPI Version:Open MPI v4.1.1, package: Open MPI nrmc2l@swift-login-1.swift.hpc.nrel.gov Distribution, ident: 4 .1.1, repo rev: v4.1.1, Apr 24 , 2021 task thread node name first task # on node core 0002 0000 c1-31 0002 0000 018 0000 0000 c1-30 0000 0000 072 0000 0001 c1-30 0000 0000 095 0001 0000 c1-30 0000 0001 096 0001 0001 c1-30 0000 0001 099 0002 0001 c1-31 0002 0000 085 0003 0000 c1-31 0002 0001 063 0003 0001 c1-31 0002 0001 099 0001 0000 c1-30 0000 0001 0097 0001 0001 c1-30 0000 0001 0103 0003 0000 c1-31 0002 0001 0062 0003 0001 c1-31 0002 0001 0103 MPI VERSION Open MPI v4.1.1, package: Open MPI nrmc2l@swift-login-1.swift.hpc.nrel.gov Distribution, ident: 4 .1.1, repo rev: v4.1.1, Apr 24 , 2021 task thread node name first task # on node core 0000 0000 c1-30 0000 0000 0072 0000 0001 c1-30 0000 0000 0020 0002 0000 c1-31 0002 0000 0000 0002 0001 c1-31 0002 0000 0067 [ nrmc2l@swift-login-1 example ] $","title":"Results"},{"location":"Documentation/Systems/Swift/running/#building-with-intel-fortran-or-intel-c-and-openmpi","text":"You can build parallel programs using OpenMPI and the Intel Fortran ifort and Intel C icc compilers. We have the example programs build with gnu compilers and OpenMP using the lines: [ nrmc2l@swift-login-1 ~ example ] $ mpif90 -fopenmp fhostone.f90 -o fhostone [ nrmc2l@swift-login-1 ~ example ] $ mpicc -fopenmp phostone.c -o phostone This gives us: [ nrmc2l@swift-login-1 ~ example ] $ ls -l fhostone -rwxrwxr-x. 1 nrmc2l nrmc2l 36880 Jul 30 13 :36 fhostone [ nrmc2l@swift-login-1 ~ example ] $ ls -l phostone -rwxrwxr-x. 1 nrmc2l nrmc2l 27536 Jul 30 13 :36 phostone Note the size of the executable files. If you want to use the Intel compilers you first do a module load. module load intel-oneapi-mpi intel-oneapi-compilers gcc Then we can set the variables OMPI_FC=ifort and OMPI_CC=icc . Then recompile. [ nrmc2l@swift-login-1 ~ example ] $ export OMPI_FC = ifort [ nrmc2l@swift-login-1 ~ example ] $ export OMPI_CC = icc [ nrmc2l@swift-login-1 ~ example ] $ mpif90 -fopenmp fhostone.f90 -o fhostone [ nrmc2l@swift-login-1 ~ example ] $ mpicc -fopenmp phostone.c -o phostone [ nrmc2l@swift-login-1 ~ example ] $ ls -lt fhostone -rwxrwxr-x. 1 nrmc2l nrmc2l 951448 Jul 30 13 :37 fhostone [ nrmc2l@swift-login-1 ~ example ] $ ls -lt phostone -rwxrwxr-x. 1 nrmc2l nrmc2l 155856 Jul 30 13 :37 phostone [ nrmc2l@swift-login-1 ~ example ] $ Note the size of the executable files have changed. You can also see the difference by running the commands nm fhostone | grep intel | wc nm phostone | grep intel | wc on the two versions of the program. It will show how many calls to Intel routines are in each, 51 and 36 compared to 0 for the gnu versions.","title":"Building with Intel Fortran or Intel C and OpenMPI"},{"location":"Documentation/Systems/Swift/running/#building-and-running-with-intel-mpi","text":"We can build with the Intel versions of MPI. We assume we will want to build with icc and ifort as the backend compilers. We load the modules: ml gcc ml intel-oneapi-compilers ml intel-oneapi-mpi Then, building and running the same example as above: make clean make PFC = mpiifort PCC = mpiicc Giving us: [ nrmc2l@swift-login-1 example ] $ ls -lt fhostone phostone -rwxrwxr-x. 1 nrmc2l hpcapps 155696 Aug 5 16 :14 phostone -rwxrwxr-x. 1 nrmc2l hpcapps 947112 Aug 5 16 :14 fhostone [ nrmc2l@swift-login-1 example ] $ We need to make some changes to our batch script. Replace the module load line with : module load intel-oneapi-mpi intel-oneapi-compilers gcc Launch with the srun command: srun ./a.out -F Our IntelMPI batch script is: #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --tasks-per-node=2 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 #These should be loaded before doing a make module load intel-oneapi-mpi intel-oneapi-compilers gcc export OMP_NUM_THREADS = 2 srun -n 4 ./fhostone -F srun -n 4 ./phostone -F With output MPI Version:Intel ( R ) MPI Library 2021 .3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 127 0000 0001 c1-32 0000 0000 097 0001 0000 c1-32 0000 0001 062 0001 0001 c1-32 0000 0001 099 MPI VERSION Intel ( R ) MPI Library 2021 .3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 0127 0000 0001 c1-32 0000 0000 0097 0001 0000 c1-32 0000 0001 0127 0001 0001 c1-32 0000 0001 0099","title":"Building and Running with Intel MPI"},{"location":"Documentation/Systems/Swift/running/#running-vasp","text":"The batch script given above can be modified to run VASP. You need to add ml vasp This will give you: [ nrmc2l@swift-login-1 ~ example ] $ which vasp_gam /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_gam [ nrmc2l@swift-login-1 ~ example ] $ which vasp_ncl /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_ncl [ nrmc2l@swift-login-1 ~ example ] $ which vasp_std /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_std [ nrmc2l@swift-login-1 ~ example ] $ Note the directory might be different. Then you need to add calls in your script to set up / point do your data files. So your final script will look something like the following. Here we use data downloaded from NREL's benchmark repository. #!/bin/bash #SBATCH --job-name=b2_4 #SBATCH --nodes=1 #SBATCH --time=4:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=debug #SBATCH --exclusive cat $0 hostname module purge ml openmpi gcc vasp #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 mkdir $SLURM_JOB_ID cp input/* $SLURM_JOB_ID cd $SLURM_JOB_ID srun -n 16 vasp_std > vasp. $SLURM_JOB_ID This will run a version of Vasp built with openmpi and gfortran/gcc. You can run a version of Vasp built with the Intel toolchain replacing the ml line with ml vaspintel intel-oneapi-mpi intel-oneapi-compilers intel-oneapi-mkl","title":"Running VASP"},{"location":"Documentation/Systems/Swift/running/#running-jupyter-jupyter-lab","text":"Jupyter and Jupyter-lab are available by loading the module \"python\" [ nrmc2l@swift-login-1 ~ ] $ ml python [ nrmc2l@swift-login-1 ~ ] $ which python /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/bin/python [ nrmc2l@swift-login-1 ~ ] $ which jupyter /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/bin/jupyter [ nrmc2l@swift-login-1 ~ ] $ which jupyter-lab /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/bin/jupyter-lab [ nrmc2l@swift-login-1 ~ ] $ It is recomended that you use the --no-browser option and connect to your notebook from your desktop using a ssh tunnel and web browser. On Swift enter the command and note the URLs. [ nrmc2l@swift-login-1 ~ ] $ jupyter-lab --no-browser [ I 2022 -03-30 07 :54:25.937 ServerApp ] jupyterlab | extension was successfully linked. [ I 2022 -03-30 07 :54:26.224 ServerApp ] nbclassic | extension was successfully linked. [ I 2022 -03-30 07 :54:26.255 ServerApp ] nbclassic | extension was successfully loaded. [ I 2022 -03-30 07 :54:26.257 LabApp ] JupyterLab extension loaded from /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/lib/python3.10/site-packages/jupyterlab [ I 2022 -03-30 07 :54:26.257 LabApp ] JupyterLab application directory is /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/share/jupyter/lab [ I 2022 -03-30 07 :54:26.260 ServerApp ] jupyterlab | extension was successfully loaded. [ I 2022 -03-30 07 :54:26.261 ServerApp ] Serving notebooks from local directory: /home/nrmc2l [ I 2022 -03-30 07 :54:26.261 ServerApp ] Jupyter Server 1 .11.1 is running at: [ I 2022 -03-30 07 :54:26.261 ServerApp ] http://localhost:8888/lab?token = 183d33c61bb136f8d04b83c70c4257a976060dd84afc9156 [ I 2022 -03-30 07 :54:26.261 ServerApp ] or http://127.0.0.1:8888/lab?token = 183d33c61bb136f8d04b83c70c4257a976060dd84afc9156 [ I 2022 -03-30 07 :54:26.261 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -03-30 07 :54:26.266 ServerApp ] To access the server, open this file in a browser: file:///home/nrmc2l/.local/share/jupyter/runtime/jpserver-2056000-open.html Or copy and paste one of these URLs: http://localhost:8888/lab?token = 183d33c61bb136f8d04b83c70c4257a976060dd84afc9156 or http://127.0.0.1:8888/lab?token = 183d33c61bb136f8d04b83c70c4257a976060dd84afc9156 Note the 8888 in the URL it might be different. On your desktop in a new terminal window enter the command: ssh -t -L 8888 :localhost:8888 swift-login-1.hpc.nrel.gov replacing 8888 with the number in the URL if it is different. Then in a web browser window past the URL. This should bring up a new notebook.","title":"Running Jupyter / Jupyter-lab"},{"location":"Documentation/Systems/Swift/running/#running-jupyter-jupyter-lab-on-a-compute-node","text":"You can get an interactive session on a compute node by running the command [ nrmc2l@swift-login-1 ~ ] $ salloc --account = hpcapps --exclusive --time = 01 :00:00 --ntasks = 16 --nodes = 1 --partition = debug but replacing hpcapps with your account. After you get an session module load python and run as shown above. [ nrmc2l@swift-login-1 ~ ] $ salloc --account = hpcapps --exclusive --time = 01 :00:00 --ntasks = 16 --nodes = 1 --partition = debug salloc: Pending job allocation 313001 salloc: job 313001 queued and waiting for resources salloc: job 313001 has been allocated resources salloc: Granted job allocation 313001 [ nrmc2l@c1-28 ~ ] $ [ nrmc2l@c1-28 ~ ] $ module load python [ nrmc2l@c1-28 ~ ] $ [ nrmc2l@c1-28 ~ ] $ jupyter-lab --no-browser [ I 2022 -03-30 08 :04:28.063 ServerApp ] jupyterlab | extension was successfully linked. [ I 2022 -03-30 08 :04:28.468 ServerApp ] nbclassic | extension was successfully linked. [ I 2022 -03-30 08 :04:28.508 ServerApp ] nbclassic | extension was successfully loaded. [ I 2022 -03-30 08 :04:28.509 LabApp ] JupyterLab extension loaded from /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/lib/python3.10/site-packages/jupyterlab [ I 2022 -03-30 08 :04:28.509 LabApp ] JupyterLab application directory is /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/share/jupyter/lab [ I 2022 -03-30 08 :04:28.513 ServerApp ] jupyterlab | extension was successfully loaded. [ I 2022 -03-30 08 :04:28.513 ServerApp ] Serving notebooks from local directory: /home/nrmc2l [ I 2022 -03-30 08 :04:28.514 ServerApp ] Jupyter Server 1 .11.1 is running at: [ I 2022 -03-30 08 :04:28.514 ServerApp ] http://localhost:8888/lab?token = cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf [ I 2022 -03-30 08 :04:28.514 ServerApp ] or http://127.0.0.1:8888/lab?token = cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf [ I 2022 -03-30 08 :04:28.514 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -03-30 08 :04:28.519 ServerApp ] To access the server, open this file in a browser: file:///home/nrmc2l/.local/share/jupyter/runtime/jpserver-3375148-open.html Or copy and paste one of these URLs: http://localhost:8888/lab?token = cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf or http://127.0.0.1:8888/lab?token = cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf On your desktop run the command: ssh -t -L 8888 :localhost:8475 swift-login-1 ssh -L 8475 :localhost:8888 c1-28 replacing 8888 with the value in the URL if needed and c1-28 with the name of the compute node on which you are running. Then again paste the URL in a web browser. You should get a notebook running on the compute node.","title":"Running Jupyter / Jupyter-lab on a compute node"},{"location":"Documentation/Systems/Swift/running/#running-julia","text":"Julia is also available via a module. [ nrmc2l@swift-login-1:~ ] $ module spider julia ... Versions: julia/1.6.2-ocsfign julia/1.7.2-gdp7a25 ... [ nrmc2l@swift-login-1:~ ] $ [ nrmc2l@swift-login-1:~/examples/spack ] $ module load julia/1.7.2-gdp7a25 [ nrmc2l@swift-login-1:~/examples/spack ] $ which julia /nopt/nrel/apps/210928a/level03/install/linux-rocky8-zen2/gcc-9.4.0/julia-1.7.2-gdp7a253nsglyzssybqknos2n5amkvqm/bin/julia [ nrmc2l@swift-login-1:~/examples/spack ] $ Julia can be run in a Jupyter notebook as discussed above. However, before doing so you will need to run the following commands in each Julia version you are using. julia> using Pkg julia> Pkg.add ( \"IJulia\" ) See https://datatofish.com/add-julia-to-jupyter/ more more information. If you would like to install your own copy of Julia complete with Jupyter-lab contact Tim Kaiser tkaiser2@nrel.gov for a script to do so.","title":"Running Julia"},{"location":"Documentation/Systems/Vermillion/","text":"Vermilion Vermilion is an OpenHPC-based cluster running on Dual AMD EPYC 7532 Rome CPUs. The nodes run as virtual machines in a local virtual private cloud (OpenStack). Mellanox drivers and OFED are installed on all nodes. Collaboration / Help You can get help with Vermilion via email at HPC-Help@nrel.gov or live chat . Live chat: - Users with access to NREL's Teams chat can collaborate via the Vermilion Users Teams room. - HPC-Help: For specific questions about work you're running on Vermillion, send email to hpc-help(@)nrel.gov and specify vermilion on the subject line . Connecting to Vermilion To access vermilion, log into the NREL network and connect via ssh: ssh vs.hpc.nrel.gov ssh vermilion.hpc.nrel.gov There are currently two login nodes. They share the same home directory so work done on one will appear on the other. They are: vs-login-1 vs-login-2 You may connect directly to a login node, but they may be cycled in and out of the pool. If a node is unavailable, try connecting to another login node or the vs.hpc.nrel.gov round-robin option. Building code Don't build or run code on a login node. Login nodes have limited CPU and memory available. Use a compute or GPU node instead. Simply start an interactive job on an appropriately provisioned node and partition for your work and do your builds there. Similarly, build your projects under /projects/your_project_name/ as home directories are limited to 5GB per user.","title":"Vermilion"},{"location":"Documentation/Systems/Vermillion/#vermilion","text":"Vermilion is an OpenHPC-based cluster running on Dual AMD EPYC 7532 Rome CPUs. The nodes run as virtual machines in a local virtual private cloud (OpenStack). Mellanox drivers and OFED are installed on all nodes.","title":"Vermilion"},{"location":"Documentation/Systems/Vermillion/#collaboration-help","text":"You can get help with Vermilion via email at HPC-Help@nrel.gov or live chat . Live chat: - Users with access to NREL's Teams chat can collaborate via the Vermilion Users Teams room. - HPC-Help: For specific questions about work you're running on Vermillion, send email to hpc-help(@)nrel.gov and specify vermilion on the subject line .","title":"Collaboration / Help"},{"location":"Documentation/Systems/Vermillion/#connecting-to-vermilion","text":"To access vermilion, log into the NREL network and connect via ssh: ssh vs.hpc.nrel.gov ssh vermilion.hpc.nrel.gov There are currently two login nodes. They share the same home directory so work done on one will appear on the other. They are: vs-login-1 vs-login-2 You may connect directly to a login node, but they may be cycled in and out of the pool. If a node is unavailable, try connecting to another login node or the vs.hpc.nrel.gov round-robin option.","title":"Connecting to Vermilion"},{"location":"Documentation/Systems/Vermillion/#building-code","text":"Don't build or run code on a login node. Login nodes have limited CPU and memory available. Use a compute or GPU node instead. Simply start an interactive job on an appropriately provisioned node and partition for your work and do your builds there. Similarly, build your projects under /projects/your_project_name/ as home directories are limited to 5GB per user.","title":"Building code"},{"location":"Documentation/Systems/Vermillion/applications/","text":"Applications The Vermilion HPC cluster marries traditional HPC deployments and modern cloud architectures, both using the OpenHPC infrastructure, and spack. https://spack.io . There are a few packages installed using the OpenHPC infrastructure. These can be found in /opt/ohpc/pub/ . These are not in your path by default. Some can be loaded via the module load command. Running the command module avail you will see which of the packages can be loaded under the heading /opt/ohpc/pub/modulefiles . However, there ary many additional modules that can be made available. Instructions for enabling additional modules, Information about partitions, and running on Vermilion can be found in the documents Modules and Running . The page Modules discuses how to activate and use the modules on Vermilion. Modules are not available by default and must be activated. Please see the Modules page for more information about setting up your environment and loading modules. The page Running describes running on Vermilion in more detail including a description of the hardware, partitions, simple build and run scripts and launching Vasp.","title":"Applications"},{"location":"Documentation/Systems/Vermillion/applications/#applications","text":"The Vermilion HPC cluster marries traditional HPC deployments and modern cloud architectures, both using the OpenHPC infrastructure, and spack. https://spack.io . There are a few packages installed using the OpenHPC infrastructure. These can be found in /opt/ohpc/pub/ . These are not in your path by default. Some can be loaded via the module load command. Running the command module avail you will see which of the packages can be loaded under the heading /opt/ohpc/pub/modulefiles . However, there ary many additional modules that can be made available. Instructions for enabling additional modules, Information about partitions, and running on Vermilion can be found in the documents Modules and Running . The page Modules discuses how to activate and use the modules on Vermilion. Modules are not available by default and must be activated. Please see the Modules page for more information about setting up your environment and loading modules. The page Running describes running on Vermilion in more detail including a description of the hardware, partitions, simple build and run scripts and launching Vasp.","title":"Applications"},{"location":"Documentation/Systems/Vermillion/modules/","text":"The page Running describes running on Vermilion in more detail including a description of the hardware, partitions, simple build and run scripts and launching Vasp. Vermilion Modules and Applications This page describes how to activate and use the modules available on Vermilion. Modules are not available by default on the machine. This page discusses how to enable them. Selecting a user Environment Environments are provided with a number of commonly used modules including compilers, common build tools, optimized libraries, and some analysis tools. Since Vermilion is a new machine with an unusual architecture we are experimenting with environments. The environments are defined in date stamped subdirectories under the directory /nopt/nrel/apps. Some of the environments in this directory are experimental and not intended for general use. User environments have a file myenv.* in the date stamped directory. These are for general use. If a directory does not have a myenv.* file then it is experimental, old, or not yet complete. The current user environments can be found by going to the directory /nopt/nrel/apps and looking for myenv.* in sub directories. For example [joeuser2@vs-login-1 apps]$ ls -1 `pwd`/*/myenv* /nopt/nrel/apps/210729a/myenv.2107300124 /nopt/nrel/apps/210901a/myenv.2109020548 /nopt/nrel/apps/210929a/myenv.2110041605 /nopt/nrel/apps/220525b/myenv.2110041605 [joeuser2@vs-login-1 apps]$ 210729a A bit dated but still should work. 210901a A bit dated but still should work. 210929a This is the recommended user environment. 220525b Has some newer versions of compilers and other packages such as python 3.10.2 & gcc 12.1. Currently, none of these environments are loaded by default for users. Users must source one of the /nopt/nrel/apps/210929a/myenv.* files to enable an environment. The recommended environment is enabled by running the source command: source /nopt/nrel/apps/210929a/myenv.2110041605 NOTE: You may want to add this line to your .bashrc file so modules are available at login. After sourcing this file you will have access to a set of modules. These can be listed using the following command: module avail If you want to build applications you can then \"module load\" compilers and the like; for example [joeuser2@vs-login-1 apps]$ ml gcc [joeuser2@vs-login-1 apps]$ ml openmpi will load gnu 9.4 and openmpi. This will give you access to gcc, gfortran, mpicc, mpif90 and related commands. You can load the Intel compilers (icc,icpc, ifort, mpiicc, mpiifort...) with the following commands. Note you should also load gcc when using the Intel compilers because the Intel compilers actually use some gcc libraries.) [joeuser2@vs-login-1 apps]$ ml intel-oneapi-compilers [joeuser2@vs-login-1 apps]$ ml intel-oneapi-mpi [joeuser2@vs-login-1 apps]$ ml gcc [joeuser2@vs-login-1 apps]$ The python in this environment is very up to date, version 3.10.0. It also contains many important packages including: numpy, scypi, matplotlib, pandas, jupyter, and jupyter-lab. Examples The directory /nopt/nrel/apps/210929a/example contains some simple build and run scripts. The directory /nopt/nrel/apps/210929a/example/vasp contains information about running Vasp. These are discussed in more detail in the page Running .","title":"Modules"},{"location":"Documentation/Systems/Vermillion/modules/#vermilion-modules-and-applications","text":"This page describes how to activate and use the modules available on Vermilion. Modules are not available by default on the machine. This page discusses how to enable them.","title":"Vermilion Modules and Applications"},{"location":"Documentation/Systems/Vermillion/modules/#selecting-a-user-environment","text":"Environments are provided with a number of commonly used modules including compilers, common build tools, optimized libraries, and some analysis tools. Since Vermilion is a new machine with an unusual architecture we are experimenting with environments. The environments are defined in date stamped subdirectories under the directory /nopt/nrel/apps. Some of the environments in this directory are experimental and not intended for general use. User environments have a file myenv.* in the date stamped directory. These are for general use. If a directory does not have a myenv.* file then it is experimental, old, or not yet complete. The current user environments can be found by going to the directory /nopt/nrel/apps and looking for myenv.* in sub directories. For example [joeuser2@vs-login-1 apps]$ ls -1 `pwd`/*/myenv* /nopt/nrel/apps/210729a/myenv.2107300124 /nopt/nrel/apps/210901a/myenv.2109020548 /nopt/nrel/apps/210929a/myenv.2110041605 /nopt/nrel/apps/220525b/myenv.2110041605 [joeuser2@vs-login-1 apps]$ 210729a A bit dated but still should work. 210901a A bit dated but still should work. 210929a This is the recommended user environment. 220525b Has some newer versions of compilers and other packages such as python 3.10.2 & gcc 12.1. Currently, none of these environments are loaded by default for users. Users must source one of the /nopt/nrel/apps/210929a/myenv.* files to enable an environment. The recommended environment is enabled by running the source command: source /nopt/nrel/apps/210929a/myenv.2110041605 NOTE: You may want to add this line to your .bashrc file so modules are available at login. After sourcing this file you will have access to a set of modules. These can be listed using the following command: module avail If you want to build applications you can then \"module load\" compilers and the like; for example [joeuser2@vs-login-1 apps]$ ml gcc [joeuser2@vs-login-1 apps]$ ml openmpi will load gnu 9.4 and openmpi. This will give you access to gcc, gfortran, mpicc, mpif90 and related commands. You can load the Intel compilers (icc,icpc, ifort, mpiicc, mpiifort...) with the following commands. Note you should also load gcc when using the Intel compilers because the Intel compilers actually use some gcc libraries.) [joeuser2@vs-login-1 apps]$ ml intel-oneapi-compilers [joeuser2@vs-login-1 apps]$ ml intel-oneapi-mpi [joeuser2@vs-login-1 apps]$ ml gcc [joeuser2@vs-login-1 apps]$ The python in this environment is very up to date, version 3.10.0. It also contains many important packages including: numpy, scypi, matplotlib, pandas, jupyter, and jupyter-lab.","title":"Selecting a user Environment"},{"location":"Documentation/Systems/Vermillion/modules/#examples","text":"The directory /nopt/nrel/apps/210929a/example contains some simple build and run scripts. The directory /nopt/nrel/apps/210929a/example/vasp contains information about running Vasp. These are discussed in more detail in the page Running .","title":"Examples"},{"location":"Documentation/Systems/Vermillion/running/","text":"The page Modules discuses how to activate and use the modules on Vermilion. Modules are not available by default and must be activated. Please see the Modules page for more information about setting up your environment and loading modules. Running on Vermilion This page discusses the compute nodes, partitions and gives some examples of building and running applications including running Vasp. Compute hosts Vermilion is a collection of physical nodes with each regular node containing Dual AMD EPYC 7532 Rome CPUs. However, each node is virtualized. That is it is split up into virtual nodes with each virtual node having a portion of the cores and memory of the physical node. Similar virtual nodes are then assigned slurm partitions as shown below. Shared file systems Vermilion's home directories are shared across all nodes. There is also /scratch/$USER and /projects spaces seen across all nodes. Partitions Partitions are flexible and fluid on Vermilion. A list of partitions can be found by running the sinfo command. Here are the partitions as of 10/20/2022. Partition Name Qty RAM Cores/node /var/scratch 1K-blocks gpu 1 x NVIDIA Tesla A100 5 114 GB 30 6,240,805,336 lg 18 229 GB 60 1,031,070,000 std 62 114 GB 30 515,010,816 sm 31 61 GB 16 256,981,000 t 15 16 GB 4 61,665,000 Operating Software The Vermilion HPC cluster runs fairly current versions of OpenHPC and SLURM on top of OpenStack. Example Environments are provided with a number of commonly used compilers, common build tools, specific optimized libraries, and some analysis tools. Environments must be enabled before modules can be seen. This is discussed in detail on the page Modules You can use the \"standard\" environment by running the command: source /nopt/nrel/apps/210929a/myenv.2110041605 The examples on this page uses the environment enabled by this command. You may want to add this command to your .bashrc file so you have a useful environment when you login. In the directory /nopt/nrel/apps/210929a you will see a subdirectory example . This contains a makefile for a simple hello world program written in both Fortran and C and several run scripts. The README.md file contains additional information, some of which is replicated here. It is suggested you copy the directory to run the examples: cp -r /nopt/nrel/apps/210929a/example ~/example cd ~/example Simple batch script Here is a sample batch script, runopenmpi , for running the hello world examples . NOTE: You must build the applications before running this script. Please see Building hello world first below. #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --exclusive #SBATCH --partition=t #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210929a/myenv* ml gcc openmpi export OMP_NUM_THREADS=2 srun --mpi=pmi2 -n 2 ./fhostone -F srun --mpi=pmi2 -n 2 ./phostone -F The submission command is: sbatch --partition=sm --account=MY_HPC_ACCOUNT runopenmpi where MY_HPC_ACCOUNT is your account. Building hello world first For the script given above to work you must first build the application. You need to: Load the environment Load the modules make Loading the environment Loading the environment is just a matter of sourcing the file source /nopt/nrel/apps/210929a/myenv.2110041605 Loading the modules. We are going to use gnu compilers with OpenMPI. module load gcc module load openmpi Run make make Full procedure screen dump [joeuser@vs-login-1 ~]$ cp -r /nopt/nrel/apps/210929a/example ~/example [joeuser@vs-login-1 ~]$ cd example/ [joeuser@vs-login-1 example]$ source /nopt/nrel/apps/210929a/myenv.2110041605 [joeuser@vs-login-1 example]$ module load gcc [joeuser@vs-login-1 example]$ module load openmpi [joeuser@vs-login-1 example]$ make mpif90 -Wno-argument-mismatch -g -fopenmp fhostone.f90 -o fhostone rm getit.mod mympi.mod numz.mod mpicc -g -fopenmp phostone.c -o phostone [joeuser@vs-login-1 example]$ cat runopenmpi #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --exclusive #SBATCH --partition=t #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210929a/myenv* ml gcc openmpi export OMP_NUM_THREADS=2 srun --mpi=pmi2 -n 2 ./fhostone -F srun --mpi=pmi2 -n 2 ./phostone -F [joeuser@vs-login-1 example]$ sbatch --account=MY_HPC_ACCOUNT runopenmpi Submitted batch job 50031771 [joeuser@vs-login-1 example]$ Results [joeuser@vs example]$ cat slurm-187.out [joeuser@vs-login-1 example]$ cat slurm-50031771.out #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --exclusive #SBATCH --partition=t #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210929a/myenv* ml gcc openmpi export OMP_NUM_THREADS=2 srun --mpi=pmi2 -n 2 ./fhostone -F srun --mpi=pmi2 -n 2 ./phostone -F SRUN --mpi=pmi2 -n 2 ./fhostone -F MPI Version:Open MPI v4.1.1, package: Open MPI joeuser@vs-sm-0001 Distribution, ident: 4.1.1, repo rev: v4.1.1, Apr 24, 2021 task thread node name first task # on node core 0000 0000 vs-t-0012.vs.hpc.n 0000 0000 002 0000 0001 vs-t-0012.vs.hpc.n 0000 0000 003 0001 0000 vs-t-0013.vs.hpc.n 0001 0000 003 0001 0001 vs-t-0013.vs.hpc.n 0001 0000 002 SRUN --mpi=pmi2 -n 2 ./phostone -F MPI VERSION Open MPI v4.1.1, package: Open MPI joeuser@vs-sm-0001 Distribution, ident: 4.1.1, repo rev: v4.1.1, Apr 24, 2021 task thread node name first task # on node core 0000 0000 vs-t-0012.vs.hpc.nrel.gov 0000 0000 0003 0000 0001 vs-t-0012.vs.hpc.nrel.gov 0000 0000 0002 0001 0000 vs-t-0013.vs.hpc.nrel.gov 0001 0000 0003 0001 0001 vs-t-0013.vs.hpc.nrel.gov 0001 0000 0000 [joeuser@vs-login-1 example]$ Many programs can be built/run with OpenMPI and with icc/ifort as the backend compilers or built/run with the Intel version of MPI with either gcc/gfortran or icc/ifort as the backend compilers. These options are discussed below. Building with Intel Fortran or Intel C and OpenMPI You can build parallel programs using OpenMPI and the Intel Fortran ifort and Intel C icc compilers. If you want to use the Intel compilers you first do a module load. ml intel-oneapi-compilers Then we can set the variables OMPI_FC=ifort and OMPI_CC=icc . Then recompile. [joeuser@vs example]$ export OMPI_FC=ifort [joeuser@vs example]$ export OMPI_CC=icc [joeuser@vs example]$ mpif90 -fopenmp fhostone.f90 -o fhostone [joeuser@vs example]$ mpicc -fopenmp phostone.c -o phostone If you do a ls -l on the executable files you will note the size of the files change with different compiler versions. You can also see the difference by running the commands nm fhostone | grep intel | wc nm phostone | grep intel | wc on the two versions of the program. It will show how many calls to Intel routines are in each, 51 and 36 compared to 0 for the gnu versions. Building and Running with Intel MPI We can build with the Intel versions of MPI and with icc and ifort as the backend compilers. We load the modules: ml gcc ml intel-oneapi-compilers ml intel-oneapi-mpi Then, building and running the same example as above: make clean make PFC=mpiifort PCC=mpiicc The actual compile lines produced by make are: mpiifort -g -fopenmp fhostone.f90 -o fhostone mpiicc -g -fopenmp phostone.c -o phostone For running, we need to make some changes to our batch script. Replace the load of openmpi with: ml intel-oneapi-compilers ml intel-oneapi-mpi Launch with the srun command: srun --mpi=pmi2 ./a.out -F Our IntelMPI batch script is: [joeuser@vs-login-1 example]$ cat runintel #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --exclusive #SBATCH --partition=lg #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210929a/myenv* ml intel-oneapi-mpi intel-oneapi-compilers gcc export OMP_NUM_THREADS=2 srun --mpi=pmi2 -n 2 ./fhostone -F srun --mpi=pmi2 -n 2 ./phostone -F With output MPI Version:Intel(R) MPI Library 2021.3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 127 0000 0001 c1-32 0000 0000 097 0001 0000 c1-32 0000 0001 062 0001 0001 c1-32 0000 0001 099 MPI VERSION Intel(R) MPI Library 2021.3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 0127 0000 0001 c1-32 0000 0000 0097 0001 0000 c1-32 0000 0001 0127 0001 0001 c1-32 0000 0001 0099 Linking Intel's MKL library. The environment defined by sourcing the file /nopt/nrel/apps/210929a/myenv.2110041605 enables loading of many other modules, including one for Intel's MKL library. Then to build against MKL using the Intel compilers icc or ifort you normally just need to add the flag -mkl . There are examples in the directory /nopt/nrel/apps/210929a/example/mkl. There is a Readme.md file that explains in a bit more detail. Assuming you copied the example directory to you home directory the mkl examples will be in ~example/mkl The short version is that you can: [joeuser@vs-login-1 mkl]$ cd ~/example/mkl [joeuser@vs-login-1 mkl]$ source /nopt/nrel/apps/210929a/myenv.2110041605 [joeuser@vs-login-1 mkl]$ module purge [joeuser@vs-login-1 mkl]$ module load intel-oneapi-compilers [joeuser@vs-login-1 mkl]$ module load intel-oneapi-mkl [joeuser@vs-login-1 mkl]$ module load gcc [joeuser@vs-login-1 mkl]$ icc -O3 -o mklc mkl.c -mkl [joeuser@vs-login-1 mkl]$ ifort -O3 -o mklf mkl.f90 -mkl or to build and run the examples using make instead directly calling icc and ifort you can: make run Running VASP on Vermilion A few different versions of VASP are available on Vermilion: - VASP 5 (Intel MPI) - VASP 6 (Intel MPI) - VASP 6 (Open MPI) - VASP 6 on GPUs Running VASP with Open MPI shows a small improvement compared to running with Intel MPI, and running VASP on GPUs shows an even larger improvement. VASP runs faster on 1 node than on 2 nodes. In some cases, VASP run times on 2 nodes have been observed to be double (or more) the run times on a single node. Many issues have been reported for running VASP on multiple nodes. In order for MPI to work successfully on Vermilion, it is necessary to specify the interconnect network that Vermilion should use to communicate between nodes. This is documented in each of the scripts below. The documented recommendations for setting the interconnect network have been shown to work well for multi-node jobs on 2 nodes, but aren't guaranteed to produce succesful multi-node runs on 4 nodes. If many cores are needed for your VASP calcualtion, it is recommended to run VASP on a singe node in the lg partition (60 cores/node), which provides the largest numbers of cores per node. Running VASP 5 with IntelMPI on CPUs To load a build of VASP 5 that is compatible with Intel MPI (and other necessary modules): module use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0/ ml vasp/5.5.4 ml intel-oneapi-mkl ml intel-oneapi-compilers ml intel-oneapi-mpi This will give you: [myuser@vs example]$ which vasp_gam /nopt/nrel/apps/220525b/level01/install/opt/spack/linux-rocky8-zen2/gcc-12.1.0/vasp544/bin/vasp_gam [myuser@vs example]$ which vasp_ncl /nopt/nrel/apps/220525b/level01/install/opt/spack/linux-rocky8-zen2/gcc-12.1.0/vasp544/bin/vasp_ncl [myuser@vs example]$ which vasp_std /nopt/nrel/apps/220525b/level01/install/opt/spack/linux-rocky8-zen2/gcc-12.1.0/vasp544/bin/vasp_std Note the directory might be different. In order to run on more than one node, we need to specify the network interconnect. To do so, use mpirun instead of srun. We want to use \"ens7\" as the interconnect. The mpirun command looks like this. I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std For VASP calculations on a single node, srun is sufficient. However, srun and mpirun produce similar run times. To run with srun for single node calculations, use the following line. srun -n 16 vasp_std Then you need to add calls in your script to set up and point to your data files. So your final script will look something like the following. Here we download data from NREL's benchmark repository. #!/bin/bash #SBATCH --job-name=vasp #SBATCH --nodes=1 #SBATCH --time=8:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=sm #SBATCH --exclusive cat $0 hostname source /nopt/nrel/apps/210929a/myenv.2110041605 module purge module use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0/ ml vasp/5.5.4 ml intel-oneapi-mkl ml intel-oneapi-compilers ml intel-oneapi-mpi # some extra lines that have been shown to improve VASP reliability on Vermilion ulimit -s unlimited export UCX_TLS=tcp,self export OMP_NUM_THREADS=1 #### wget is needed to download data ml wget #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 mkdir input wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M -q -O INCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC -q -O POTCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2 -q -O POSCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS # mpirun is recommended (necessary for multi-node calculations) I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std # srun can be used instead of mpirun for sinlge node calculations # srun -n 16 vasp_std Running VASP 6 with IntelMPI on CPUs To load a build of VASP 6 that is compatible with Intel MPI (and other necessary modules): source /nopt/nrel/apps/210929a/myenv.2110041605 ml vaspintel ml intel-oneapi-mkl ml intel-oneapi-compilers ml intel-oneapi-mpi This will give you: [myuser@vs example]$ which vasp_gam /nopt/nrel/apps/210929a/level01/linux-centos8-zen2/gcc-9.4.0/vaspintel-1.0-dwljo4wr6xcrgxqaq7pz35yqfxdxxsq4/bin/vasp_gam [myuser@vs example]$ which vasp_ncl /nopt/nrel/apps/210929a/level01/linux-centos8-zen2/gcc-9.4.0/vaspintel-1.0-dwljo4wr6xcrgxqaq7pz35yqfxdxxsq4/bin/vasp_ncl [myuser@vs example]$ which vasp_std /nopt/nrel/apps/210929a/level01/linux-centos8-zen2/gcc-9.4.0/vaspintel-1.0-dwljo4wr6xcrgxqaq7pz35yqfxdxxsq4/bin/vasp_std Note the directory might be different. In order to run on more than one node, we need to specify the network interconnect. To do so, use mpirun instead of srun. We want to use \"ens7\" as the interconnect. The mpirun command looks like this. I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std For VASP calculations on a single node, srun is sufficient. However, srun and mpirun produce similar run times. To run with srun for single node calculations, use the following line. srun -n 16 vasp_std Then you need to add calls in your script to set up and point to your data files. So your final script will look something like the following. Here we download data from NREL's benchmark repository. #!/bin/bash #SBATCH --job-name=vasp #SBATCH --nodes=1 #SBATCH --time=8:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=sm #SBATCH --exclusive cat $0 hostname source /nopt/nrel/apps/210929a/myenv.2110041605 module purge source /nopt/nrel/apps/210929a/myenv.2110041605 ml intel-oneapi-mkl ml intel-oneapi-compilers ml intel-oneapi-mpi ml vaspintel # some extra lines that have been shown to improve VASP reliability on Vermilion ulimit -s unlimited export UCX_TLS=tcp,self export OMP_NUM_THREADS=1 #### wget is needed to download data ml wget #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 mkdir input wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M -q -O INCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC -q -O POTCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2 -q -O POSCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS # mpirun is recommended (necessary for multi-node calculations) I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std # srun can be used instead of mpirun for sinlge node calculations # srun -n 16 vasp_std Running VASP 6 with OpenMPI on CPUs To load a build of VASP 6 that is compatible with Open MPI: source /nopt/nrel/apps/210929a/myenv.2110041605 ml vasp This will give you: [myuser@vs example]$ which vasp_gam /nopt/nrel/apps/123456a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_gam [myuser@vs example]$ which vasp_ncl /nopt/nrel/apps/123456a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_ncl [myuser@vs example]$ which vasp_std /nopt/nrel/apps/123456a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_std Note the directory might be different. In order to specify the network interconnect, we need to set the OMPI_MCA_param variable. We want to use \"ens7\" as the interconnect. module use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0 module load openmpi OMPI_MCA_param=\"btl_tcp_if_include ens7\" Then you need to add calls in your script to set up and point to your data files. So your final script will look something like the following. Here we download data from NREL's benchmark repository. #!/bin/bash #SBATCH --job-name=vasp #SBATCH --nodes=1 #SBATCH --time=8:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=sm #SBATCH --exclusive cat $0 hostname source /nopt/nrel/apps/210929a/myenv.2110041605 module purge ml gcc ml vasp # some extra lines that have been shown to improve VASP reliability on Vermilion ulimit -s unlimited export UCX_TLS=tcp,self export OMP_NUM_THREADS=1 # lines to set \"ens7\" as the interconnect network module use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0 module load openmpi OMPI_MCA_param=\"btl_tcp_if_include ens7\" #### wget is needed to download data ml wget #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 mkdir input wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M -q -O INCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC -q -O POTCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2 -q -O POSCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS srun --mpi=pmi2 -n 16 vasp_std Running VASP 6 on GPUs VASP can also be run on Vermilion's GPUs. To do this we need to add a few #SBATCH lines at the top of the script to assign the job to run in the gpu partition and to set the gpu binding. The --gpu-bind flag requires 1 set of \"0,1\" for each node used. #SBATCH --nodes=2 #SBATCH --partition=gpu #SBATCH --gpu-bind=map_gpu:0,1,0,1 A gpu build of VASP can be accessed by adding the following path to your PATH variable. export PATH=/projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc:$PATH This will give you: [myuser@vs example]$ which vasp_gam /projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc/vasp_gam [myuser@vs example]$ which vasp_ncl /projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc/vasp_ncl [myuser@vs example]$ which vasp_std /projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc/vasp_std Instead of srun, use mpirun to run VASP on GPUs. Since Vermilion only has 1 GPU per node, it's important to make sure you are only requesting 1 task per node by setting -npernode 1. mpirun -npernode 1 vasp_std > vasp.$SLURM_JOB_ID There's a few more modules needed to run VASP on GPUs, and two library variables need to be set. We can modify the VASP CPU script to include lines to load the modules, set library variables and make the changes outlined above. The final script will look something like this. #!/bin/bash #SBATCH --job-name=vasp #SBATCH --nodes=2 #SBATCH --time=1:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=gpu #SBATCH --gpu-bind=map_gpu:0,1,0,1 #SBATCH --exclusive cat $0 hostname #load necessary modules and set library paths module use /nopt/nrel/apps/220421a/modules/lmod/linux-rocky8-x86_64/gcc/11.3.0/ ml nvhpc ml gcc ml fftw export LD_LIBRARY_PATH=/nopt/nrel/apps//220421a/install/opt/spack/linux-rocky8-zen2/gcc-11.3.0/nvhpc-22.2-ruzrtpyewnnrif6s7w7rehvpk7jimdrd/Linux_x86_64/22.2/compilers/extras/qd/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/nopt/nrel/apps//220421a/install/opt/spack/linux-rocky8-zen2/gcc-11.3.0/gcc-11.3.0-c3u46uvtuljfuqimb4bgywoz6oynridg/lib64:$LD_LIBRARY_PATH #add a path to the gpu build of VASP to your script export PATH=/projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc:$PATH #### wget is needed to download data ml wget #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 mkdir input wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M -q -O INCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC -q -O POTCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2 -q -O POSCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS mpirun -npernode 1 vasp_std > vasp.$SLURM_JOB_ID.","title":"Running on Vermillion"},{"location":"Documentation/Systems/Vermillion/running/#running-on-vermilion","text":"This page discusses the compute nodes, partitions and gives some examples of building and running applications including running Vasp.","title":"Running on Vermilion"},{"location":"Documentation/Systems/Vermillion/running/#compute-hosts","text":"Vermilion is a collection of physical nodes with each regular node containing Dual AMD EPYC 7532 Rome CPUs. However, each node is virtualized. That is it is split up into virtual nodes with each virtual node having a portion of the cores and memory of the physical node. Similar virtual nodes are then assigned slurm partitions as shown below.","title":"Compute hosts"},{"location":"Documentation/Systems/Vermillion/running/#shared-file-systems","text":"Vermilion's home directories are shared across all nodes. There is also /scratch/$USER and /projects spaces seen across all nodes.","title":"Shared file systems"},{"location":"Documentation/Systems/Vermillion/running/#partitions","text":"Partitions are flexible and fluid on Vermilion. A list of partitions can be found by running the sinfo command. Here are the partitions as of 10/20/2022. Partition Name Qty RAM Cores/node /var/scratch 1K-blocks gpu 1 x NVIDIA Tesla A100 5 114 GB 30 6,240,805,336 lg 18 229 GB 60 1,031,070,000 std 62 114 GB 30 515,010,816 sm 31 61 GB 16 256,981,000 t 15 16 GB 4 61,665,000","title":"Partitions"},{"location":"Documentation/Systems/Vermillion/running/#operating-software","text":"The Vermilion HPC cluster runs fairly current versions of OpenHPC and SLURM on top of OpenStack.","title":"Operating Software"},{"location":"Documentation/Systems/Vermillion/running/#example","text":"Environments are provided with a number of commonly used compilers, common build tools, specific optimized libraries, and some analysis tools. Environments must be enabled before modules can be seen. This is discussed in detail on the page Modules You can use the \"standard\" environment by running the command: source /nopt/nrel/apps/210929a/myenv.2110041605 The examples on this page uses the environment enabled by this command. You may want to add this command to your .bashrc file so you have a useful environment when you login. In the directory /nopt/nrel/apps/210929a you will see a subdirectory example . This contains a makefile for a simple hello world program written in both Fortran and C and several run scripts. The README.md file contains additional information, some of which is replicated here. It is suggested you copy the directory to run the examples: cp -r /nopt/nrel/apps/210929a/example ~/example cd ~/example","title":"Example"},{"location":"Documentation/Systems/Vermillion/running/#simple-batch-script","text":"Here is a sample batch script, runopenmpi , for running the hello world examples . NOTE: You must build the applications before running this script. Please see Building hello world first below. #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --exclusive #SBATCH --partition=t #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210929a/myenv* ml gcc openmpi export OMP_NUM_THREADS=2 srun --mpi=pmi2 -n 2 ./fhostone -F srun --mpi=pmi2 -n 2 ./phostone -F The submission command is: sbatch --partition=sm --account=MY_HPC_ACCOUNT runopenmpi where MY_HPC_ACCOUNT is your account.","title":"Simple batch script"},{"location":"Documentation/Systems/Vermillion/running/#building-hello-world-first","text":"For the script given above to work you must first build the application. You need to: Load the environment Load the modules make","title":"Building hello world first"},{"location":"Documentation/Systems/Vermillion/running/#loading-the-environment","text":"Loading the environment is just a matter of sourcing the file source /nopt/nrel/apps/210929a/myenv.2110041605","title":"Loading the environment"},{"location":"Documentation/Systems/Vermillion/running/#loading-the-modules","text":"We are going to use gnu compilers with OpenMPI. module load gcc module load openmpi","title":"Loading the modules."},{"location":"Documentation/Systems/Vermillion/running/#run-make","text":"make","title":"Run make"},{"location":"Documentation/Systems/Vermillion/running/#full-procedure-screen-dump","text":"[joeuser@vs-login-1 ~]$ cp -r /nopt/nrel/apps/210929a/example ~/example [joeuser@vs-login-1 ~]$ cd example/ [joeuser@vs-login-1 example]$ source /nopt/nrel/apps/210929a/myenv.2110041605 [joeuser@vs-login-1 example]$ module load gcc [joeuser@vs-login-1 example]$ module load openmpi [joeuser@vs-login-1 example]$ make mpif90 -Wno-argument-mismatch -g -fopenmp fhostone.f90 -o fhostone rm getit.mod mympi.mod numz.mod mpicc -g -fopenmp phostone.c -o phostone [joeuser@vs-login-1 example]$ cat runopenmpi #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --exclusive #SBATCH --partition=t #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210929a/myenv* ml gcc openmpi export OMP_NUM_THREADS=2 srun --mpi=pmi2 -n 2 ./fhostone -F srun --mpi=pmi2 -n 2 ./phostone -F [joeuser@vs-login-1 example]$ sbatch --account=MY_HPC_ACCOUNT runopenmpi Submitted batch job 50031771 [joeuser@vs-login-1 example]$","title":"Full procedure screen dump"},{"location":"Documentation/Systems/Vermillion/running/#results","text":"[joeuser@vs example]$ cat slurm-187.out [joeuser@vs-login-1 example]$ cat slurm-50031771.out #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --exclusive #SBATCH --partition=t #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210929a/myenv* ml gcc openmpi export OMP_NUM_THREADS=2 srun --mpi=pmi2 -n 2 ./fhostone -F srun --mpi=pmi2 -n 2 ./phostone -F SRUN --mpi=pmi2 -n 2 ./fhostone -F MPI Version:Open MPI v4.1.1, package: Open MPI joeuser@vs-sm-0001 Distribution, ident: 4.1.1, repo rev: v4.1.1, Apr 24, 2021 task thread node name first task # on node core 0000 0000 vs-t-0012.vs.hpc.n 0000 0000 002 0000 0001 vs-t-0012.vs.hpc.n 0000 0000 003 0001 0000 vs-t-0013.vs.hpc.n 0001 0000 003 0001 0001 vs-t-0013.vs.hpc.n 0001 0000 002 SRUN --mpi=pmi2 -n 2 ./phostone -F MPI VERSION Open MPI v4.1.1, package: Open MPI joeuser@vs-sm-0001 Distribution, ident: 4.1.1, repo rev: v4.1.1, Apr 24, 2021 task thread node name first task # on node core 0000 0000 vs-t-0012.vs.hpc.nrel.gov 0000 0000 0003 0000 0001 vs-t-0012.vs.hpc.nrel.gov 0000 0000 0002 0001 0000 vs-t-0013.vs.hpc.nrel.gov 0001 0000 0003 0001 0001 vs-t-0013.vs.hpc.nrel.gov 0001 0000 0000 [joeuser@vs-login-1 example]$ Many programs can be built/run with OpenMPI and with icc/ifort as the backend compilers or built/run with the Intel version of MPI with either gcc/gfortran or icc/ifort as the backend compilers. These options are discussed below.","title":"Results"},{"location":"Documentation/Systems/Vermillion/running/#building-with-intel-fortran-or-intel-c-and-openmpi","text":"You can build parallel programs using OpenMPI and the Intel Fortran ifort and Intel C icc compilers. If you want to use the Intel compilers you first do a module load. ml intel-oneapi-compilers Then we can set the variables OMPI_FC=ifort and OMPI_CC=icc . Then recompile. [joeuser@vs example]$ export OMPI_FC=ifort [joeuser@vs example]$ export OMPI_CC=icc [joeuser@vs example]$ mpif90 -fopenmp fhostone.f90 -o fhostone [joeuser@vs example]$ mpicc -fopenmp phostone.c -o phostone If you do a ls -l on the executable files you will note the size of the files change with different compiler versions. You can also see the difference by running the commands nm fhostone | grep intel | wc nm phostone | grep intel | wc on the two versions of the program. It will show how many calls to Intel routines are in each, 51 and 36 compared to 0 for the gnu versions.","title":"Building with Intel Fortran or Intel C and OpenMPI"},{"location":"Documentation/Systems/Vermillion/running/#building-and-running-with-intel-mpi","text":"We can build with the Intel versions of MPI and with icc and ifort as the backend compilers. We load the modules: ml gcc ml intel-oneapi-compilers ml intel-oneapi-mpi Then, building and running the same example as above: make clean make PFC=mpiifort PCC=mpiicc The actual compile lines produced by make are: mpiifort -g -fopenmp fhostone.f90 -o fhostone mpiicc -g -fopenmp phostone.c -o phostone For running, we need to make some changes to our batch script. Replace the load of openmpi with: ml intel-oneapi-compilers ml intel-oneapi-mpi Launch with the srun command: srun --mpi=pmi2 ./a.out -F Our IntelMPI batch script is: [joeuser@vs-login-1 example]$ cat runintel #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=2 #SBATCH --exclusive #SBATCH --partition=lg #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210929a/myenv* ml intel-oneapi-mpi intel-oneapi-compilers gcc export OMP_NUM_THREADS=2 srun --mpi=pmi2 -n 2 ./fhostone -F srun --mpi=pmi2 -n 2 ./phostone -F With output MPI Version:Intel(R) MPI Library 2021.3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 127 0000 0001 c1-32 0000 0000 097 0001 0000 c1-32 0000 0001 062 0001 0001 c1-32 0000 0001 099 MPI VERSION Intel(R) MPI Library 2021.3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 0127 0000 0001 c1-32 0000 0000 0097 0001 0000 c1-32 0000 0001 0127 0001 0001 c1-32 0000 0001 0099","title":"Building and Running with Intel MPI"},{"location":"Documentation/Systems/Vermillion/running/#linking-intels-mkl-library","text":"The environment defined by sourcing the file /nopt/nrel/apps/210929a/myenv.2110041605 enables loading of many other modules, including one for Intel's MKL library. Then to build against MKL using the Intel compilers icc or ifort you normally just need to add the flag -mkl . There are examples in the directory /nopt/nrel/apps/210929a/example/mkl. There is a Readme.md file that explains in a bit more detail. Assuming you copied the example directory to you home directory the mkl examples will be in ~example/mkl The short version is that you can: [joeuser@vs-login-1 mkl]$ cd ~/example/mkl [joeuser@vs-login-1 mkl]$ source /nopt/nrel/apps/210929a/myenv.2110041605 [joeuser@vs-login-1 mkl]$ module purge [joeuser@vs-login-1 mkl]$ module load intel-oneapi-compilers [joeuser@vs-login-1 mkl]$ module load intel-oneapi-mkl [joeuser@vs-login-1 mkl]$ module load gcc [joeuser@vs-login-1 mkl]$ icc -O3 -o mklc mkl.c -mkl [joeuser@vs-login-1 mkl]$ ifort -O3 -o mklf mkl.f90 -mkl or to build and run the examples using make instead directly calling icc and ifort you can: make run","title":"Linking Intel's MKL library."},{"location":"Documentation/Systems/Vermillion/running/#running-vasp-on-vermilion","text":"A few different versions of VASP are available on Vermilion: - VASP 5 (Intel MPI) - VASP 6 (Intel MPI) - VASP 6 (Open MPI) - VASP 6 on GPUs Running VASP with Open MPI shows a small improvement compared to running with Intel MPI, and running VASP on GPUs shows an even larger improvement. VASP runs faster on 1 node than on 2 nodes. In some cases, VASP run times on 2 nodes have been observed to be double (or more) the run times on a single node. Many issues have been reported for running VASP on multiple nodes. In order for MPI to work successfully on Vermilion, it is necessary to specify the interconnect network that Vermilion should use to communicate between nodes. This is documented in each of the scripts below. The documented recommendations for setting the interconnect network have been shown to work well for multi-node jobs on 2 nodes, but aren't guaranteed to produce succesful multi-node runs on 4 nodes. If many cores are needed for your VASP calcualtion, it is recommended to run VASP on a singe node in the lg partition (60 cores/node), which provides the largest numbers of cores per node.","title":"Running VASP on Vermilion"},{"location":"Documentation/Systems/Vermillion/running/#running-vasp-5-with-intelmpi-on-cpus","text":"To load a build of VASP 5 that is compatible with Intel MPI (and other necessary modules): module use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0/ ml vasp/5.5.4 ml intel-oneapi-mkl ml intel-oneapi-compilers ml intel-oneapi-mpi This will give you: [myuser@vs example]$ which vasp_gam /nopt/nrel/apps/220525b/level01/install/opt/spack/linux-rocky8-zen2/gcc-12.1.0/vasp544/bin/vasp_gam [myuser@vs example]$ which vasp_ncl /nopt/nrel/apps/220525b/level01/install/opt/spack/linux-rocky8-zen2/gcc-12.1.0/vasp544/bin/vasp_ncl [myuser@vs example]$ which vasp_std /nopt/nrel/apps/220525b/level01/install/opt/spack/linux-rocky8-zen2/gcc-12.1.0/vasp544/bin/vasp_std Note the directory might be different. In order to run on more than one node, we need to specify the network interconnect. To do so, use mpirun instead of srun. We want to use \"ens7\" as the interconnect. The mpirun command looks like this. I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std For VASP calculations on a single node, srun is sufficient. However, srun and mpirun produce similar run times. To run with srun for single node calculations, use the following line. srun -n 16 vasp_std Then you need to add calls in your script to set up and point to your data files. So your final script will look something like the following. Here we download data from NREL's benchmark repository. #!/bin/bash #SBATCH --job-name=vasp #SBATCH --nodes=1 #SBATCH --time=8:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=sm #SBATCH --exclusive cat $0 hostname source /nopt/nrel/apps/210929a/myenv.2110041605 module purge module use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0/ ml vasp/5.5.4 ml intel-oneapi-mkl ml intel-oneapi-compilers ml intel-oneapi-mpi # some extra lines that have been shown to improve VASP reliability on Vermilion ulimit -s unlimited export UCX_TLS=tcp,self export OMP_NUM_THREADS=1 #### wget is needed to download data ml wget #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 mkdir input wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M -q -O INCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC -q -O POTCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2 -q -O POSCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS # mpirun is recommended (necessary for multi-node calculations) I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std # srun can be used instead of mpirun for sinlge node calculations # srun -n 16 vasp_std","title":"Running VASP 5 with IntelMPI on CPUs"},{"location":"Documentation/Systems/Vermillion/running/#running-vasp-6-with-intelmpi-on-cpus","text":"To load a build of VASP 6 that is compatible with Intel MPI (and other necessary modules): source /nopt/nrel/apps/210929a/myenv.2110041605 ml vaspintel ml intel-oneapi-mkl ml intel-oneapi-compilers ml intel-oneapi-mpi This will give you: [myuser@vs example]$ which vasp_gam /nopt/nrel/apps/210929a/level01/linux-centos8-zen2/gcc-9.4.0/vaspintel-1.0-dwljo4wr6xcrgxqaq7pz35yqfxdxxsq4/bin/vasp_gam [myuser@vs example]$ which vasp_ncl /nopt/nrel/apps/210929a/level01/linux-centos8-zen2/gcc-9.4.0/vaspintel-1.0-dwljo4wr6xcrgxqaq7pz35yqfxdxxsq4/bin/vasp_ncl [myuser@vs example]$ which vasp_std /nopt/nrel/apps/210929a/level01/linux-centos8-zen2/gcc-9.4.0/vaspintel-1.0-dwljo4wr6xcrgxqaq7pz35yqfxdxxsq4/bin/vasp_std Note the directory might be different. In order to run on more than one node, we need to specify the network interconnect. To do so, use mpirun instead of srun. We want to use \"ens7\" as the interconnect. The mpirun command looks like this. I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std For VASP calculations on a single node, srun is sufficient. However, srun and mpirun produce similar run times. To run with srun for single node calculations, use the following line. srun -n 16 vasp_std Then you need to add calls in your script to set up and point to your data files. So your final script will look something like the following. Here we download data from NREL's benchmark repository. #!/bin/bash #SBATCH --job-name=vasp #SBATCH --nodes=1 #SBATCH --time=8:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=sm #SBATCH --exclusive cat $0 hostname source /nopt/nrel/apps/210929a/myenv.2110041605 module purge source /nopt/nrel/apps/210929a/myenv.2110041605 ml intel-oneapi-mkl ml intel-oneapi-compilers ml intel-oneapi-mpi ml vaspintel # some extra lines that have been shown to improve VASP reliability on Vermilion ulimit -s unlimited export UCX_TLS=tcp,self export OMP_NUM_THREADS=1 #### wget is needed to download data ml wget #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 mkdir input wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M -q -O INCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC -q -O POTCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2 -q -O POSCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS # mpirun is recommended (necessary for multi-node calculations) I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std # srun can be used instead of mpirun for sinlge node calculations # srun -n 16 vasp_std","title":"Running VASP 6 with IntelMPI on CPUs"},{"location":"Documentation/Systems/Vermillion/running/#running-vasp-6-with-openmpi-on-cpus","text":"To load a build of VASP 6 that is compatible with Open MPI: source /nopt/nrel/apps/210929a/myenv.2110041605 ml vasp This will give you: [myuser@vs example]$ which vasp_gam /nopt/nrel/apps/123456a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_gam [myuser@vs example]$ which vasp_ncl /nopt/nrel/apps/123456a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_ncl [myuser@vs example]$ which vasp_std /nopt/nrel/apps/123456a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_std Note the directory might be different. In order to specify the network interconnect, we need to set the OMPI_MCA_param variable. We want to use \"ens7\" as the interconnect. module use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0 module load openmpi OMPI_MCA_param=\"btl_tcp_if_include ens7\" Then you need to add calls in your script to set up and point to your data files. So your final script will look something like the following. Here we download data from NREL's benchmark repository. #!/bin/bash #SBATCH --job-name=vasp #SBATCH --nodes=1 #SBATCH --time=8:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=sm #SBATCH --exclusive cat $0 hostname source /nopt/nrel/apps/210929a/myenv.2110041605 module purge ml gcc ml vasp # some extra lines that have been shown to improve VASP reliability on Vermilion ulimit -s unlimited export UCX_TLS=tcp,self export OMP_NUM_THREADS=1 # lines to set \"ens7\" as the interconnect network module use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0 module load openmpi OMPI_MCA_param=\"btl_tcp_if_include ens7\" #### wget is needed to download data ml wget #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 mkdir input wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M -q -O INCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC -q -O POTCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2 -q -O POSCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS srun --mpi=pmi2 -n 16 vasp_std","title":"Running VASP 6 with OpenMPI on CPUs"},{"location":"Documentation/Systems/Vermillion/running/#running-vasp-6-on-gpus","text":"VASP can also be run on Vermilion's GPUs. To do this we need to add a few #SBATCH lines at the top of the script to assign the job to run in the gpu partition and to set the gpu binding. The --gpu-bind flag requires 1 set of \"0,1\" for each node used. #SBATCH --nodes=2 #SBATCH --partition=gpu #SBATCH --gpu-bind=map_gpu:0,1,0,1 A gpu build of VASP can be accessed by adding the following path to your PATH variable. export PATH=/projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc:$PATH This will give you: [myuser@vs example]$ which vasp_gam /projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc/vasp_gam [myuser@vs example]$ which vasp_ncl /projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc/vasp_ncl [myuser@vs example]$ which vasp_std /projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc/vasp_std Instead of srun, use mpirun to run VASP on GPUs. Since Vermilion only has 1 GPU per node, it's important to make sure you are only requesting 1 task per node by setting -npernode 1. mpirun -npernode 1 vasp_std > vasp.$SLURM_JOB_ID There's a few more modules needed to run VASP on GPUs, and two library variables need to be set. We can modify the VASP CPU script to include lines to load the modules, set library variables and make the changes outlined above. The final script will look something like this. #!/bin/bash #SBATCH --job-name=vasp #SBATCH --nodes=2 #SBATCH --time=1:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=gpu #SBATCH --gpu-bind=map_gpu:0,1,0,1 #SBATCH --exclusive cat $0 hostname #load necessary modules and set library paths module use /nopt/nrel/apps/220421a/modules/lmod/linux-rocky8-x86_64/gcc/11.3.0/ ml nvhpc ml gcc ml fftw export LD_LIBRARY_PATH=/nopt/nrel/apps//220421a/install/opt/spack/linux-rocky8-zen2/gcc-11.3.0/nvhpc-22.2-ruzrtpyewnnrif6s7w7rehvpk7jimdrd/Linux_x86_64/22.2/compilers/extras/qd/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/nopt/nrel/apps//220421a/install/opt/spack/linux-rocky8-zen2/gcc-11.3.0/gcc-11.3.0-c3u46uvtuljfuqimb4bgywoz6oynridg/lib64:$LD_LIBRARY_PATH #add a path to the gpu build of VASP to your script export PATH=/projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc:$PATH #### wget is needed to download data ml wget #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 mkdir input wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M -q -O INCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC -q -O POTCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2 -q -O POSCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS mpirun -npernode 1 vasp_std > vasp.$SLURM_JOB_ID.","title":"Running VASP 6 on GPUs"},{"location":"Documentation/languages/bash/bash-starter/","text":"An Introduction to Bash Scripting Bash ( B ourne A gain Sh ell) is one of the most widely available and used command line shell applications. Along with basic shell functionality, it offers a wide variety of features which, if utilized thoughtfully, can create powerful automated execution sequences that run software, manipulate text and files, parallelize otherwise single-process software, or anything else you may want to do from the command line. Shell scripts are also one of the most common ways our HPC community submits jobs, and running a large parallel workload often requires some initialization of the software environment before meaningful computations can begin. This typically involves tasks such as declaring environment variables, preparing input files or staging directories for data, loading modules and libraries that the software needs to run, preparing inputs, manipulating datasets, and so on. Bash can even be used to launch several single-core jobs, effectively taking on the role of an ad hoc batch executor, as well. This article provides a brief introduction to bash, as well as a list of tips, tricks, and good practices when it comes to writing effective bash scripts that can apply widely in both HPC and non-HPC environments. We will also provide links to some additional resources to help further your bash scripting skills. Executing/Invoking Scripts All of bash commands work at the command prompt \"live\", i.e. interpreted line-by-line as you type commands and press enter. A bash \"script\" may be regarded as a list of bash commands that have been saved to a file for convenience, usually with some basic formatting, and possibly comments, for legibility. All bash scripts must begin with a special character combination, called the \"shebang\" or #! character, followed by the name of an interpreter: #!/bin/bash This declares that the contents of the file that follow are to be interpreted as commands, using /bin/bash as the interpreter. This includes commands, control structures, and comments. Plenty of other interpreters exist. For example, Python scripts begin with: #!/usr/bin/python or /usr/bin/env python , perl scripts: #!/usr/bin/perl , and so on. Bash Scripting Syntax If you read a bash script, you may be tempted to default to your usual understanding of how code generally works. For example, with most languages, typically there is a binary or kernel which digests the code you write (compilers/gcc for C, the python interpreter/shell, Java Virtual Machine for Java, and so on.) The binary/kernel/interpreter then interprets the text into some sort of data structure which enforces the priority of certain commands over others, and finally generates some execution of operations based on that data structure. Bash isn't too far off from this model, and in some respects functions as any other interpreted language: you enter a command (or a control structure) and it is executed. However, as a shell that also serves as your major interface to the underlying operating system, it does have some properties and features that may blur the lines between what you think of as 'interpreted' versus 'compiled'. For instance, many aspects of the bash \"language\" are actually just the names of pre-compiled binaries which do the heavy lifting. Much the same way you can run python or ssh in a command line, under the hood normal bash operations such as if , echo , and exit are actually just programs that expect a certain cadence for the arguments you give it. A block such as: if true ; then echo \"true was true\" ; fi This is really just a sequence of executing many compiled applications or shell built-ins with arguments; the names of these commands were just chosen to read as a typical programming grammar. A good example is the program [ which is just an oddly-named command you can invoke. Try running which [ at a command prompt. The results may surprise you: /usr/bin/[ is actually a compiled program on disk, not a \"built-in\" function! This is why you need to have a space between the brackets and your conditional, because the conditional itself is passed as an argument to the command [ . In languages like C it's common to write the syntax as if (conditional) { ...; } . However, in bash, if you try to run if [true] you will likely get an error saying there isn't a command called [true] that you can run. This is also why you often see stray semicolons that seem somewhat arbitrary, as semicolons separate the execution of two binaries. Take this snippet for example: echo \"First message.\" ; echo \"Second message.\" This is equivalent to: echo \"First message.\" echo \"Second message.\" In the first snippet, if the semicolon was not present, the second echo would be interpreted as an argument to the first echo and would end up outputting: First message. echo Second message. Bash interprets ; and \\n (newline) as separators. If you need to pass these characters into a function (for example, common in find 's -exec flag) you need to escape them with a \\ . This is useful for placing arguments on separate lines to improve readability like this example: chromium-browser \\ --start-fullscreen \\ --new-window \\ --incognito \\ 'https://google.com' Similarly, normal if-then-else control flow that you would expect of any programming/scripting language has the same caveats. Consider this snippet: if true then echo \"true is true\" else echo \"false is true?\" fi If we break down what's essentially happening here (omitting some of the technical details): if invokes the command true which always exits with a successful exit code ( 0 ) if interprets a success exit code ( 0 ) as a truism and runs the then . the then command will execute anything it's given until else , elif , or fi the else command is the same as then but will only execute if if returned an erroneous exit code. the fi command indicates that no more conditional branches exist relative to the logical expression given to the original if . All this to say, this is why you often see if-then-else blocks written succinctly as if [ <CONDITIONAL> ]; then <COMMANDS>; fi with seemingly arbitrary semicolons and spaces. It is exactly why things work this way that bash is able to execute arbitrary executables (some of which you may end up writing) and not require something like Python's subprocess module. This is just to give you an understanding for why some of the syntax you will encounter is the way it is. Everything in bash is either a command or an argument to a command. Parentheses, Braces, and Brackets Bash utilizes many flavors of symbolic enclosures. A complete guide is beyond the scope of this document, but you may see the following: ( ) - Single parentheses: run enclosed commands in a subshell a='bad';(a='good'; mkdir $a); echo $a result: directory \"good\" is made, echoes \"bad\" to screen $( ) - Single parentheses with dollar sign: subshell output to string(command substitution) (preferred method) echo \"my name is $( whoami )\" result: prints your username <( ) - Parentheses with angle bracket: process substitution sort -n -k 5 <( ls -l ./dir1) <(ls -l ./dir2) result: sorts ls -l results of two directories by column 5 (size) [ ] - Single Brackets: truth testing with filename expansion or word splitting if [ -e myfile.txt ]; then echo \"yay\"; else echo \"boo\"; fi result: if myfile.txt exists, celebrate { } - Single Braces/curly brackets: expansion of a range ${ } - Single braces with dollar sign: expansion with interpolation ` ` - Backticks: command/process substitution (( )) - Double parentheses: integer arithmetic $(( )) - Double parentheses with dollar sign: integer arithmatic to string [[ ]] - Double brackets: truth testing with regex Additional Notes on ( ) (Single Parentheses) There are 3 features in Bash which are denoted by a pair of parentheses, which are Bash subshells, Bash array declarations, and Bash function declarations. See the table below for when each feature is enacted: Syntax Bash Feature Command/line begins with ( Run the contained expression(s) in a subshell. This will pass everything until a closing ) to a child-fork of Bash that inherits the environment from the invoking Bash instance, and exits with the exit code of the last command the subshell exitted with. See the section on subshells for more info. A valid Bash identifier is set equal to a parnethetically enclosed list of items (.e.g. arr=(\"a\" \"b\" \"c\") ) Creates a Bash array with elements enclosed by the parentheses. The default indexing of the elements is numerically incremental from 0 in the given order, but this order can be overridden or string-based keys can be used. See the section on arrays for more info. A valid Bash identifier is followed by () and contains some function(s) enclosed by { } (i.e. func() { echo \"test\"; } ) Declare a function which can be re/used throughout a Bash script. See the either of \" { } \" or functions for more info. Examples of Enclosure Usage Note that whitespace is required, prohibited, or ignored in certain situations. See this block for specific examples of how to use whitespace in the various contexts of parantheses. ### Subshells ( echo hi ) # OK ( echo hi ) # OK ( echo hi ) # OK ( echo hi ) # OK ### Arrays arr =( \"a\" \"b\" \"c\" ) # Array of 3 strings arr =( \"a\" \"b\" \"c\" ) # ERROR arr = ( \"a\" \"b\" \"c\" ) # ERROR arr = ( \"a\" \"b\" \"c\" ) # ERROR arr =( \"a\"\"b\"\"c\" ) # Array of one element that is \"abc\" arr =( \"a\" , \"b\" , \"c\" ) # Array of one element that is \"a,b,c\" arr =( \"a\" , \"b\" , \"c\" ) # ${arr[0]} == \"a,\" ### Functions func (){ echo hi ; } # ERROR func (){ echo hi ; } # OK func (){ echo hi ; } # OK func () { echo hi ; } # OK func () { echo hi ; } # OK Command Behavior (ls -1 | head -n 1) Run the command in a subshell. This will return the exit code of the last process that was ran. test_var=(ls -1) Create a bash array with the elements ls and -1 , meaning ${test_var[1]} will evaluate to -1 . test_var=$(ls -1) Evaluate ls -1 and capture the output as a string. test_var=(`ls -1`) or test_var=($(ls -1)) Evaluate ls -1 and capture the output as an array. Bracket Usage: Correct: [ cmd ] - There must be spaces or terminating characters ( \\n or ; ) surrounding any brackets. Like many common bash commands, \"[\" is actually a standalone executable, usually located at /usr/bin/[ , so it requires spaces to invoke correctly. Erroneous: [cmd] - tries to find a command called [cmd] which likely doesn't exist [cmd ] - tries to find a command called [cmd and pass ] as an argument to it [ cmd] - tries to pass cmd] as an argument to [ which expects an argument of ] that isn't technically provided. There are many other examples of using enclosures in bash scripting beyond the scope of this introduction. Please see the resources section for more information. Variables Variable assignment in bash is simply to assign a value to a string of characters. All subsequent references to that variable must be prefixed by $ : $ MYSTRING = \"a string\" $ echo $MYSTRING a string $ MYNUMBER = \"42\" $ echo $MYNUMBER 42 Exporting Variables When you declare a variable in bash, that variable is only available in the shell in which it is declared; if you spawn a sub-shell, the variable will not be accessible. Using the export command, you can essentially declare the variable to be inheritable. # without exporting: $ TESTVAR = 100 $ echo $TESTVAR 100 # returns a result $ bash # spawn a sub-shell $ echo $TESTVAR # no result $ exit # exit the subshell # with exporting: $ export TESTVAR = 100 $ echo $TESTVAR 100 # returns a result $ bash # spawn a sub-shell $ echo $TESTVAR 100 # value is passed into the subshell $ exit # exit the subshell $ Sourcing Variables \"Source\" (shortcut: . ) is a built-in bash command that takes a bash script as an argument. Bash will execute the contents of that file in the current shell, instead of spawning a sub-shell. This will load any variables, function declarations, and so on into your current shell. A common example of using the source command is when making changes to your ~/.bashrc , which is usually only parsed once upon login. Rather than logging out and logging back in every time you wish to make a change, you can simply run source ~/.bashrc or . ~/.bashrc and the changes will take effect immediately. Declaring Variables Variable typing in bash is implicit, and the need to declare a type is rare, but the declare command can be used when necessary: $ declare -i MYNUMBER # set type as an integer $ echo $MYNUMBER 0 $ declare -l MYWORD = \"LOWERCASE\" # set type as lowercase $ echo $MYWORD lowercase $ see help declare at the command line for more information on types that can be declared. Further Resources NREL HPC Github - User-contributed bash script and examples that you can use on HPC systems. BASH cheat sheet - A concise and extensive list of example commands, built-ins, control structures, and other useful bash scripting material.","title":"Intro to Bash Scripting"},{"location":"Documentation/languages/bash/bash-starter/#an-introduction-to-bash-scripting","text":"Bash ( B ourne A gain Sh ell) is one of the most widely available and used command line shell applications. Along with basic shell functionality, it offers a wide variety of features which, if utilized thoughtfully, can create powerful automated execution sequences that run software, manipulate text and files, parallelize otherwise single-process software, or anything else you may want to do from the command line. Shell scripts are also one of the most common ways our HPC community submits jobs, and running a large parallel workload often requires some initialization of the software environment before meaningful computations can begin. This typically involves tasks such as declaring environment variables, preparing input files or staging directories for data, loading modules and libraries that the software needs to run, preparing inputs, manipulating datasets, and so on. Bash can even be used to launch several single-core jobs, effectively taking on the role of an ad hoc batch executor, as well. This article provides a brief introduction to bash, as well as a list of tips, tricks, and good practices when it comes to writing effective bash scripts that can apply widely in both HPC and non-HPC environments. We will also provide links to some additional resources to help further your bash scripting skills.","title":"An Introduction to Bash Scripting"},{"location":"Documentation/languages/bash/bash-starter/#executinginvoking-scripts","text":"All of bash commands work at the command prompt \"live\", i.e. interpreted line-by-line as you type commands and press enter. A bash \"script\" may be regarded as a list of bash commands that have been saved to a file for convenience, usually with some basic formatting, and possibly comments, for legibility. All bash scripts must begin with a special character combination, called the \"shebang\" or #! character, followed by the name of an interpreter: #!/bin/bash This declares that the contents of the file that follow are to be interpreted as commands, using /bin/bash as the interpreter. This includes commands, control structures, and comments. Plenty of other interpreters exist. For example, Python scripts begin with: #!/usr/bin/python or /usr/bin/env python , perl scripts: #!/usr/bin/perl , and so on.","title":"Executing/Invoking Scripts"},{"location":"Documentation/languages/bash/bash-starter/#bash-scripting-syntax","text":"If you read a bash script, you may be tempted to default to your usual understanding of how code generally works. For example, with most languages, typically there is a binary or kernel which digests the code you write (compilers/gcc for C, the python interpreter/shell, Java Virtual Machine for Java, and so on.) The binary/kernel/interpreter then interprets the text into some sort of data structure which enforces the priority of certain commands over others, and finally generates some execution of operations based on that data structure. Bash isn't too far off from this model, and in some respects functions as any other interpreted language: you enter a command (or a control structure) and it is executed. However, as a shell that also serves as your major interface to the underlying operating system, it does have some properties and features that may blur the lines between what you think of as 'interpreted' versus 'compiled'. For instance, many aspects of the bash \"language\" are actually just the names of pre-compiled binaries which do the heavy lifting. Much the same way you can run python or ssh in a command line, under the hood normal bash operations such as if , echo , and exit are actually just programs that expect a certain cadence for the arguments you give it. A block such as: if true ; then echo \"true was true\" ; fi This is really just a sequence of executing many compiled applications or shell built-ins with arguments; the names of these commands were just chosen to read as a typical programming grammar. A good example is the program [ which is just an oddly-named command you can invoke. Try running which [ at a command prompt. The results may surprise you: /usr/bin/[ is actually a compiled program on disk, not a \"built-in\" function! This is why you need to have a space between the brackets and your conditional, because the conditional itself is passed as an argument to the command [ . In languages like C it's common to write the syntax as if (conditional) { ...; } . However, in bash, if you try to run if [true] you will likely get an error saying there isn't a command called [true] that you can run. This is also why you often see stray semicolons that seem somewhat arbitrary, as semicolons separate the execution of two binaries. Take this snippet for example: echo \"First message.\" ; echo \"Second message.\" This is equivalent to: echo \"First message.\" echo \"Second message.\" In the first snippet, if the semicolon was not present, the second echo would be interpreted as an argument to the first echo and would end up outputting: First message. echo Second message. Bash interprets ; and \\n (newline) as separators. If you need to pass these characters into a function (for example, common in find 's -exec flag) you need to escape them with a \\ . This is useful for placing arguments on separate lines to improve readability like this example: chromium-browser \\ --start-fullscreen \\ --new-window \\ --incognito \\ 'https://google.com' Similarly, normal if-then-else control flow that you would expect of any programming/scripting language has the same caveats. Consider this snippet: if true then echo \"true is true\" else echo \"false is true?\" fi If we break down what's essentially happening here (omitting some of the technical details): if invokes the command true which always exits with a successful exit code ( 0 ) if interprets a success exit code ( 0 ) as a truism and runs the then . the then command will execute anything it's given until else , elif , or fi the else command is the same as then but will only execute if if returned an erroneous exit code. the fi command indicates that no more conditional branches exist relative to the logical expression given to the original if . All this to say, this is why you often see if-then-else blocks written succinctly as if [ <CONDITIONAL> ]; then <COMMANDS>; fi with seemingly arbitrary semicolons and spaces. It is exactly why things work this way that bash is able to execute arbitrary executables (some of which you may end up writing) and not require something like Python's subprocess module. This is just to give you an understanding for why some of the syntax you will encounter is the way it is. Everything in bash is either a command or an argument to a command.","title":"Bash Scripting Syntax"},{"location":"Documentation/languages/bash/bash-starter/#parentheses-braces-and-brackets","text":"Bash utilizes many flavors of symbolic enclosures. A complete guide is beyond the scope of this document, but you may see the following: ( ) - Single parentheses: run enclosed commands in a subshell a='bad';(a='good'; mkdir $a); echo $a result: directory \"good\" is made, echoes \"bad\" to screen $( ) - Single parentheses with dollar sign: subshell output to string(command substitution) (preferred method) echo \"my name is $( whoami )\" result: prints your username <( ) - Parentheses with angle bracket: process substitution sort -n -k 5 <( ls -l ./dir1) <(ls -l ./dir2) result: sorts ls -l results of two directories by column 5 (size) [ ] - Single Brackets: truth testing with filename expansion or word splitting if [ -e myfile.txt ]; then echo \"yay\"; else echo \"boo\"; fi result: if myfile.txt exists, celebrate { } - Single Braces/curly brackets: expansion of a range ${ } - Single braces with dollar sign: expansion with interpolation ` ` - Backticks: command/process substitution (( )) - Double parentheses: integer arithmetic $(( )) - Double parentheses with dollar sign: integer arithmatic to string [[ ]] - Double brackets: truth testing with regex","title":"Parentheses, Braces, and Brackets"},{"location":"Documentation/languages/bash/bash-starter/#additional-notes-on-single-parentheses","text":"There are 3 features in Bash which are denoted by a pair of parentheses, which are Bash subshells, Bash array declarations, and Bash function declarations. See the table below for when each feature is enacted: Syntax Bash Feature Command/line begins with ( Run the contained expression(s) in a subshell. This will pass everything until a closing ) to a child-fork of Bash that inherits the environment from the invoking Bash instance, and exits with the exit code of the last command the subshell exitted with. See the section on subshells for more info. A valid Bash identifier is set equal to a parnethetically enclosed list of items (.e.g. arr=(\"a\" \"b\" \"c\") ) Creates a Bash array with elements enclosed by the parentheses. The default indexing of the elements is numerically incremental from 0 in the given order, but this order can be overridden or string-based keys can be used. See the section on arrays for more info. A valid Bash identifier is followed by () and contains some function(s) enclosed by { } (i.e. func() { echo \"test\"; } ) Declare a function which can be re/used throughout a Bash script. See the either of \" { } \" or functions for more info.","title":"Additional Notes on ( ) (Single Parentheses)"},{"location":"Documentation/languages/bash/bash-starter/#examples-of-enclosure-usage","text":"Note that whitespace is required, prohibited, or ignored in certain situations. See this block for specific examples of how to use whitespace in the various contexts of parantheses. ### Subshells ( echo hi ) # OK ( echo hi ) # OK ( echo hi ) # OK ( echo hi ) # OK ### Arrays arr =( \"a\" \"b\" \"c\" ) # Array of 3 strings arr =( \"a\" \"b\" \"c\" ) # ERROR arr = ( \"a\" \"b\" \"c\" ) # ERROR arr = ( \"a\" \"b\" \"c\" ) # ERROR arr =( \"a\"\"b\"\"c\" ) # Array of one element that is \"abc\" arr =( \"a\" , \"b\" , \"c\" ) # Array of one element that is \"a,b,c\" arr =( \"a\" , \"b\" , \"c\" ) # ${arr[0]} == \"a,\" ### Functions func (){ echo hi ; } # ERROR func (){ echo hi ; } # OK func (){ echo hi ; } # OK func () { echo hi ; } # OK func () { echo hi ; } # OK Command Behavior (ls -1 | head -n 1) Run the command in a subshell. This will return the exit code of the last process that was ran. test_var=(ls -1) Create a bash array with the elements ls and -1 , meaning ${test_var[1]} will evaluate to -1 . test_var=$(ls -1) Evaluate ls -1 and capture the output as a string. test_var=(`ls -1`) or test_var=($(ls -1)) Evaluate ls -1 and capture the output as an array.","title":"Examples of Enclosure Usage"},{"location":"Documentation/languages/bash/bash-starter/#bracket-usage","text":"Correct: [ cmd ] - There must be spaces or terminating characters ( \\n or ; ) surrounding any brackets. Like many common bash commands, \"[\" is actually a standalone executable, usually located at /usr/bin/[ , so it requires spaces to invoke correctly. Erroneous: [cmd] - tries to find a command called [cmd] which likely doesn't exist [cmd ] - tries to find a command called [cmd and pass ] as an argument to it [ cmd] - tries to pass cmd] as an argument to [ which expects an argument of ] that isn't technically provided. There are many other examples of using enclosures in bash scripting beyond the scope of this introduction. Please see the resources section for more information.","title":"Bracket Usage:"},{"location":"Documentation/languages/bash/bash-starter/#variables","text":"Variable assignment in bash is simply to assign a value to a string of characters. All subsequent references to that variable must be prefixed by $ : $ MYSTRING = \"a string\" $ echo $MYSTRING a string $ MYNUMBER = \"42\" $ echo $MYNUMBER 42","title":"Variables"},{"location":"Documentation/languages/bash/bash-starter/#exporting-variables","text":"When you declare a variable in bash, that variable is only available in the shell in which it is declared; if you spawn a sub-shell, the variable will not be accessible. Using the export command, you can essentially declare the variable to be inheritable. # without exporting: $ TESTVAR = 100 $ echo $TESTVAR 100 # returns a result $ bash # spawn a sub-shell $ echo $TESTVAR # no result $ exit # exit the subshell # with exporting: $ export TESTVAR = 100 $ echo $TESTVAR 100 # returns a result $ bash # spawn a sub-shell $ echo $TESTVAR 100 # value is passed into the subshell $ exit # exit the subshell $","title":"Exporting Variables"},{"location":"Documentation/languages/bash/bash-starter/#sourcing-variables","text":"\"Source\" (shortcut: . ) is a built-in bash command that takes a bash script as an argument. Bash will execute the contents of that file in the current shell, instead of spawning a sub-shell. This will load any variables, function declarations, and so on into your current shell. A common example of using the source command is when making changes to your ~/.bashrc , which is usually only parsed once upon login. Rather than logging out and logging back in every time you wish to make a change, you can simply run source ~/.bashrc or . ~/.bashrc and the changes will take effect immediately.","title":"Sourcing Variables"},{"location":"Documentation/languages/bash/bash-starter/#declaring-variables","text":"Variable typing in bash is implicit, and the need to declare a type is rare, but the declare command can be used when necessary: $ declare -i MYNUMBER # set type as an integer $ echo $MYNUMBER 0 $ declare -l MYWORD = \"LOWERCASE\" # set type as lowercase $ echo $MYWORD lowercase $ see help declare at the command line for more information on types that can be declared.","title":"Declaring Variables"},{"location":"Documentation/languages/bash/bash-starter/#further-resources","text":"NREL HPC Github - User-contributed bash script and examples that you can use on HPC systems. BASH cheat sheet - A concise and extensive list of example commands, built-ins, control structures, and other useful bash scripting material.","title":"Further Resources"},{"location":"Documentation/languages/fortran/f90/","text":"Advanced Fortran 90 This document is derived from an HTML page written at the San Diego Supercomper Center many years ago. Its purpose is to Introduce Fortran 90 concepts to Fortran 77 programers. It does this by presenting an example program and introducing concepts as various routines of the program are presented. The original web page has been used over the years and has been translated into several languages. Format for our presentation We will \"develop\" an application Incorporate f90 features Show source code Explain what and why as we do it Application is a genetic algorithm Easy to understand and program Offers rich opportunities for enhancement We also provide an summary of F90 syntax, key words, operators, constants, and functions What was in mind of the language writers? What were they thinking? Enable portable codes Same precision Include many common extensions More reliable programs Getting away from underlying hardware Move toward parallel programming Run old programs Ease of programming Writing Maintaining Understanding Reading Recover C and C++ users Why Fortran? Famous Quote: \"I don't know what the technical characteristics of the standard language for scientific and engineering computation in the year 2000 will be... but I know it will be called Fortran.\" John Backus.* ### Note: He claimed that he never said this. Language of choice for Scientific programming Large installed user base. Fortran 90 has most of the features of C . . . and then some The compilers produce better programs Justification of topics Enhance performance Enhance portability Enhance reliability Enhance maintainability Classification of topics New useful features Old tricks Power features Overview of F90 Listing_of_topics_covered Listing of topics covered What is a Genetic Algorithm Simple algorithm for a GA Our example problem Start of real Fortran 90 discussion Comparing a FORTRAN 77 routine to a Fortran 90 routine Obsolescent features New source Form and related things New data declaration method Kind facility Modules Module functions and subroutines Allocatable arrays (the basics) Passing arrays to subroutines Interface for passing arrays Optional arguments and intent Derived data types Using defined types User defined operators Recursive functions introduction Fortran 90 recursive functions Pointers Function and subroutine overloading Fortran Minval and Minloc routines Pointer assignment More pointer usage, association and nullify Pointer usage to reference an array Data assignment with structures Using the user defined operator Passing arrays with a given arbitrary lower bounds Using pointers to access sections of arrays Allocating an array inside a subroutine Our fitness function Linked lists Linked list usage Our map representation Date and time functions Non advancing and character IO Internal IO Inquire function Namelist Vector valued functions Complete source for recent discussions Some array specific intrinsic functions The rest of our GA Compiler Information Summary Overview of F90 Fortran 95 References What is a Genetic Algorithm A \"suboptimization\" system Find good, but maybe not optimal, solutions to difficult problems Often used on NP-Hard or combinatorial optimization problems Requirements Solution(s) to the problem represented as a string A fitness function Takes as input the solution string Output the desirability of the solution A method of combining solution strings to generate new solutions Find solutions to problems by Darwinian evolution Potential solutions ar though of as living entities in a population The strings are the genetic codes for the individuals Fittest individuals are allowed to survive to reproduce Simple algorithm for a GA Generate a initial population, a collection of strings do for some time evaluate each individual (string) of the population using the fitness function sort the population with fittest coming to the top allow the fittest individuals to \"sexually\" reproduce replacing the old population allow for mutation end do Our example problem Instance:Given a map of the N states or countries and a fixed number of colors Find a coloring of the map, if it exists, such that no two states that share a boarder have the same color Notes - In general, for a fixed number of colors and an arbitrary map the only known way to find if there is a valid coloring is a brute force search with the number of combinations = (NUMBER_OF_COLORS)**(NSTATES) The strings of our population are integer vectors represent the coloring Our fitness function returns the number of boarder violations The GA searches for a mapping with few, hopefully 0 violations This problem is related to several important NP_HARD problems in computer science Processor scheduling Communication and grid allocation for parallel computing Routing Start of real Fortran 90 discussion Comparing a FORTRAN 77 routine to a Fortran 90 routine The routine is one of the random number generators from: Numerical Recipes, The Art of Scientific Computing. Press, Teukolsky, Vetterling and Flannery. Cambridge University Press 1986. Changes correct bugs increase functionality aid portability Original function ran1 ( idum ) real ran1 integer idum real r ( 97 ) parameter ( m1 = 259200 , ia1 = 7141 , ic1 = 54773 ) parameter ( m2 = 134456 , ia2 = 8121 , ic2 = 28411 ) parameter ( m3 = 243000 , ia3 = 4561 , ic3 = 51349 ) integer j integer iff , ix1 , ix2 , ix3 data iff / 0 / if ( idum . lt . 0. or . iff . eq . 0 ) then rm1 = 1.0 / m1 rm2 = 1.0 / m2 iff = 1 ix1 = mod ( ic1 - idum , m1 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ix1 , m2 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix3 = mod ( ix1 , m3 ) do 11 j = 1 , 97 ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) r ( j ) = ( real ( ix1 ) + real ( ix2 ) * rm2 ) * rm1 11 continue idum = 1 endif ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) ix3 = mod ( ia3 * ix3 + ic3 , m3 ) j = 1 + ( 97 * ix3 ) / m3 if ( j . gt . 9 7. or . j . lt . 1 ) then write ( * , * ) ' error in ran1 j=' , j stop endif ran1 = r ( j ) r ( j ) = ( real ( ix1 ) + real ( ix2 ) * rm2 ) * rm1 return end Fortran 90 module ran_mod contains function ran1 ( idum ) use numz implicit none !note after use statement real ( b8 ) ran1 integer , intent ( inout ), optional :: idum real ( b8 ) r ( 97 ), rm1 , rm2 integer , parameter :: m1 = 259200 , ia1 = 7141 , ic1 = 54773 integer , parameter :: m2 = 134456 , ia2 = 8121 , ic2 = 28411 integer , parameter :: m3 = 243000 , ia3 = 4561 , ic3 = 51349 integer j integer iff , ix1 , ix2 , ix3 data iff / 0 / save ! corrects a bug in the original routine if ( present ( idum )) then if ( idum . lt . 0. or . iff . eq . 0 ) then rm1 = 1.0_b8 m1 rm2 = 1.0_b8 m2 iff = 1 ix1 = mod ( ic1 - idum , m1 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ix1 , m2 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix3 = mod ( ix1 , m3 ) do j = 1 , 97 ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) r ( j ) = ( real ( ix1 , b8 ) + real ( ix2 , b8 ) * rm2 ) * rm1 enddo idum = 1 endif endif ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) ix3 = mod ( ia3 * ix3 + ic3 , m3 ) j = 1 + ( 97 * ix3 ) / m3 if ( j . gt . 9 7. or . j . lt . 1 ) then write ( * , * ) ' error in ran1 j=' , j stop endif ran1 = r ( j ) r ( j ) = ( real ( ix1 , b8 ) + real ( ix2 , b8 ) * rm2 ) * rm1 return end function ran1 Comments Modules are a way of encapsulating functions an data. More below. The use numz line is similar to an include file. In this case it defines our real data type. real (b8) is a new way to specify percision for data types in a portable way. integer , intent(inout), optional :: idum we are saying idum is an optional input parameter integer , parameter :: just a different syntax The save statement is needed for program correctness present(idum) is a function to determine if ran1 was called with the optional parameter Obsolescent features The following are available in Fortran 90. On the other hand, the concept of \"obsolescence\" is introduced. This means that some constructs may be removed in the future. - Arithmetic IF-statement - Control variables in a DO-loop which are floating point or double-precision floating-point - Terminating several DO-loops on the same statement - Terminating the DO-loop in some other way than with CONTINUE or END DO - Alternate return - Jump to END IF from an outer block - PAUSE - ASSIGN and assigned GOTO and assigned FORMAT , that is the whole \"statement number variable\" concept. - Hollerith editing in FORMAT. New source form and related things Summary ! now indicates the start of a comment & indicates the next line is a continuation Lines can be longer than 72 characters Statements can start in any column Use ; to put multiple statements on one line New forms for the do loop Many functions are generic 32 character names Many new array assignment techniques Features Flexibility can aid in program readability Readability decreases errors Got ya! Can no longer use C to start a comment Character in column 5 no longer is continue Tab is not a valid character (may produce a warning) Characters past 72 now count program darwin real a ( 10 ), b ( 10 ), c ( 10 ), d ( 10 ), e ( 10 ), x , y integer odd ( 5 ), even ( 5 ) ! this line is continued by using \"&\" write ( * , * ) \"starting \" ,& \"darwin\" ! this line in a continued from above ! multiple statement per line --rarely a good idea x = 1 ; y = 2 ; write ( * , * ) x , y do i = 1 , 10 ! statement lable is not required for do e ( i ) = i enddo odd = ( / 1 , 3 , 5 , 7 , 9 / ) ! array assignment even = ( / 2 , 4 , 6 , 8 , 10 / ) ! array assignment a = 1 ! array assignment, every element of a = 1 b = 2 c = a + b + e ! element by element assignment c ( odd ) = c ( even ) - 1 ! can use arrays of indices on both sides d = sin ( c ) ! element by element application of intrinsics write ( * , * ) d write ( * , * ) abs ( d ) ! many intrinsic functions are generic a_do_loop : do i = 1 , 10 write ( * , * ) i , c ( i ), d ( i ) enddo a_do_loop do if ( c ( 10 ) . lt . 0.0 ) exit c ( 10 ) = c ( 10 ) - 1 enddo write ( * , * ) c ( 10 ) do while ( c ( 9 ) . gt . 0 ) c ( 9 ) = c ( 9 ) - 1 enddo write ( * , * ) c ( 9 ) end program New data declaration method Motivation Variables can now have attributes such as - Parameter - Save - Dimension Attributes are assigned in the variable declaration statement One variable can have several attributes Requires Fortran 90 to have a new statement form integer , parameter :: in2 = 14 real , parameter :: pi = 3.141592653589793239 real , save , dimension ( 10 ) :: cpu_times , wall_times !**** the old way of doing the same ****! !**** real cpu_times(10),wall_times(10) ****! !**** save cpu_times, wall_times ****! - Other Attributes - allocatable - public - private - target - pointer - intent - optional Kind facility Motivation Assume we have a program that we want to run on two different machines We want the same representation of reals on both machines (same number of significant digits) Problem: different machines have different representations for reals Digits of precision for some (old) machines and data type Machine Real Double Precision IBM (SP) 6 15 Cray (T90) 15 33 Cray (T3E) 15 15 * or * We may want to run with at least 6 digits today and at least 14 digits tomorrow Use the Select_Real_Kind(P) function to create a data type with P digits of precision program darwin ! e has at least 4 significant digits real ( selected_real_kind ( 4 )) e ! b8 will be used to define reals with 14 digits integer , parameter :: b8 = selected_real_kind ( 14 ) real ( b8 ), parameter :: pi = 3.141592653589793239_b8 ! note usage of _b8 ! with a constant ! to force precision e = 2.71828182845904523536 write ( * , * ) \"starting \" ,& ! this line is continued by using \"&\" \"darwin\" ! this line in a continued from above write ( * , * ) \"pi has \" , precision ( pi ), \" digits precision \" , pi write ( * , * ) \"e has \" , precision ( e ), \" digits precision \" , e end program Example output sp001 % darwin starting darwin pi has 15 digits precision 3.14159265358979312 e has 6 digits precision 2.718281746 sp001 % Can convert to/from given precision for all variables created using \"b8\" by changing definition of \"b8\" Use the Select_Real_Kind(P,R) function to create a data type with P digits of precision and exponent range of R Modules Motivation: Common block usage is prone to error Provide most of capability of common blocks but safer Provide capabilities beyond common blocks Modules can contain: Data definitions Data to be shared much like using a labeled common Functions and subroutines Interfaces (more on this later) You \"include\" a module with a \"use\" statement module numz integer , parameter :: b8 = selected_real_kind ( 14 ) real ( b8 ), parameter :: pi = 3.141592653589793239_b8 integergene_size end module program darwin use numz implicit none ! now part of the standard, put it after the use statements write ( * , * ) \"pi has \" , precision ( pi ), \" digits precision \" , pi call set_size () write ( * , * ) \"gene_size=\" , gene_size end program subroutine set_size use numz gene_size = 10 end subroutine An example run pi has 15 digits precision 3.14159265358979312 gene_size=10 Module functions and subroutines Motivation: Encapsulate related functions and subroutines Can \"USE\" these functions in a program or subroutine Can be provided as a library Only routines that contain the use statement can see the routines Example is a random number package: module ran_mod ! module contains three functions ! ran1 returns a uniform random number between 0-1 ! spread returns random number between min - max ! normal returns a normal distribution contains function ran1 () !returns random number between 0 - 1 use numz implicit none real ( b8 ) ran1 , x call random_number ( x ) ! built in fortran 90 random number function ran1 = x end function ran1 function spread ( min , max ) !returns random # between min/max use numz implicit none real ( b8 ) spread real ( b8 ) min , max spread = ( max - min ) * ran1 () + min end function spread function normal ( mean , sigma ) !returns a normal distribution use numz implicit none real ( b8 ) normal , tmp real ( b8 ) mean , sigma integer flag real ( b8 ) fac , gsave , rsq , r1 , r2 save flag , gsave data flag / 0 / if ( flag . eq . 0 ) then rsq = 2.0_b8 do while ( rsq . ge . 1.0_b8 . or . rsq . eq . 0.0_b8 ) ! new from for do r1 = 2.0_b8 * ran1 () - 1.0_b8 r2 = 2.0_b8 * ran1 () - 1.0_b8 rsq = r1 * r1 + r2 * r2 enddo fac = sqrt ( - 2.0_b8 * log ( rsq ) / rsq ) gsave = r1 * fac tmp = r2 * fac flag = 1 else tmp = gsave flag = 0 endif normal = tmp * sigma + mean return end function normal end module ran_mod Exersize 1: Write a program that returns 10 uniform random numbers. Allocatable arrays (the basics) Motivation: At compile time we may not know the size an array needs to be We may want to change problem size without recompiling Allocatable arrays allow us to set the size at run time We set the size of the array using the allocate statement We may want to change the lower bound for an array A simple example: module numz integer , parameter :: b8 = selected_real_kind ( 14 ) integer gene_size , num_genes integer , allocatable :: a_gene (:), many_genes (:,:) end module program darwin use numz implicit none integer ierr call set_size () allocate ( a_gene ( gene_size ), stat = ierr ) !stat= allows for an error code return if ( ierr /= 0 ) write ( * , * ) \"allocation error\" ! /= is .ne. allocate ( many_genes ( gene_size , num_genes ), stat = ierr ) !2d array if ( ierr /= 0 ) write ( * , * ) \"allocation error\" write ( * , * ) lbound ( a_gene ), ubound ( a_gene ) ! get lower and upper bound ! for the array write ( * , * ) size ( many_genes ), size ( many_genes , 1 ) !get total size and size !along 1st dimension deallocate ( many_genes ) ! free the space for the array and matrix deallocate ( a_gene ) allocate ( a_gene ( 0 : gene_size )) ! now allocate starting at 0 instead of 1 write ( * , * ) allocated ( many_genes ), allocated ( a_gene ) ! shows if allocated write ( * , * ) lbound ( a_gene ), ubound ( a_gene ) end program subroutine set_size use numz write ( * , * ) 'enter gene size:' read ( * , * ) gene_size write ( * , * ) 'enter number of genes:' read ( * , * ) num_genes end subroutine set_size Example run enter gene size: 10 enter number of genes: 20 1 10 200 10 F T 0 10 Passing arrays to subroutines There are several ways to specify arrays for subroutines Explicit shape integer, dimension(8,8)::an_explicit_shape_array Assumed size integer, dimension(i,*)::an_assumed_size_array Assumed Shape integer, dimension(:,:)::an_assumed_shape_array Example subroutine arrays ( an_explicit_shape_array ,& i ,& !note we pass all bounds except the last an_assumed_size_array ,& an_assumed_shape_array ) ! Explicit shape integer , dimension ( 8 , 8 ) :: an_explicit_shape_array ! Assumed size integer , dimension ( i , * ) :: an_assumed_size_array ! Assumed Shape integer , dimension (:,:) :: an_assumed_shape_array write ( * , * ) sum ( an_explicit_shape_array ) write ( * , * ) lbound ( an_assumed_size_array ) ! why does sum not work here? write ( * , * ) sum ( an_assumed_shape_array ) end subroutine Interface for passing arrays !!!!Warning!!!! When passing assumed shape arrays as arguments you must provide an interface Similar to C prototypes but much more versatile The interface is a copy of the invocation line and the argument definitions Modules are a good place for interfaces If a procedure is part of a \"contains\" section in a module an interface is not required !!!!Warning!!!! The compiler may not tell you that you need an interface module numz integer , parameter :: b8 = selected_real_kind ( 14 ) integer , allocatable :: a_gene (:), many_genes (:,:) end module module face interface fitness function fitness ( vector ) use numz implicit none real ( b8 ) fitness integer , dimension (:) :: vector end function fitness end interface end module program darwin use numz use face implicit none integer i integer vect ( 10 ) ! just a regular array allocate ( a_gene ( 10 )); allocate ( many_genes ( 3 , 10 )) a_gene = 1 !sets every element of a_gene to 1 write ( * , * ) fitness ( a_gene ) vect = 8 write ( * , * ) fitness ( vect ) ! also works with regular arrays many_genes = 3 !sets every element to 3 many_genes ( 1 ,:) = a_gene !sets column 1 to a_gene many_genes ( 2 ,:) = 2 * many_genes ( 1 ,:) do i = 1 , 3 write ( * , * ) fitness ( many_genes ( i ,:)) enddo write ( * , * ) fitness ( many_genes (:, 1 )) !go along other dimension !!!!write(*,*)fitness(many_genes)!!!!does not work end program function fitness ( vector ) use numz implicit none real ( b8 ) fitness integer , dimension (:) :: vector ! must match interface fitness = sum ( vector ) end function Exersize 2: Run this program using the \"does not work line\". Why? Using intrinsic functions make it work? Exersize 3: Prove that f90 does not \"pass by address\". Optional arguments and intent Motivation: We may have a function or subroutine that we may not want to always pass all arguments Initialization Two examples Seeding the intrinsic random number generator requires keyword arguments To define an optional argument in our own function we use the optional attribute integer :: my_seed becomes integer, optional :: my_seed Used like this: ! ran1 returns a uniform random number between 0-1 ! the seed is optional and used to reset the generator contains function ran1 ( my_seed ) use numz implicit none real ( b8 ) ran1 , r integer , optional , intent ( in ) :: my_seed ! optional argument not changed in the routine integer , allocatable :: seed (:) integer the_size , j if ( present ( my_seed )) then ! use the seed if present call random_seed ( size = the_size ) ! how big is the intrisic seed? allocate ( seed ( the_size )) ! allocate space for seed do j = 1 , the_size ! create the seed seed ( j ) = abs ( my_seed ) + ( j - 1 ) ! abs is generic enddo call random_seed ( put = seed ) ! assign the seed deallocate ( seed ) ! deallocate space endif call random_number ( r ) ran1 = r end function ran1 end module program darwin use numz use ran_mod ! interface required if we have ! optional or intent arguments real ( b8 ) x , y x = ran1 ( my_seed = 12345 ) ! we can specify the name of the argument y = ran1 () write ( * , * ) x , y x = ran1 ( 12345 ) ! with only one optional argument we don't need to y = ran1 () write ( * , * ) x , y end program Intent is a hint to the compiler to enable optimization intent(in) We will not change this value in our subroutine intent(out) We will define this value in our routine intent(inout) The normal situation Derived data types Motivation: Derived data types can be used to group different types of data together (integers, reals, character, complex) Can not be done in F77 although people have \"faked\" it Example In our GA we define a collection of genes as a 2d array We call the fitness function for every member of the collection We want to sort the collection of genes based on result of fitness function Define a data type that holds the fitness value and an index into the 2d array Create an array of this data type, 1 for each member of the collection Call fitness function with the result being placed into the new data type along with a pointer into the array Again modules are a good place for data type definitions module galapagos use numz type thefit !the name of the type sequence ! sequence forces the data elements ! to be next to each other in memory ! where might this be useful? real ( b8 ) val ! our result from the fitness function integer index ! the index into our collection of genes end type thefit end module Using defined types Use the % to reference various components of the derived data type program darwin use numz use galapagos ! the module that contains the type definition use face ! contains various interfaces implicit none ! define an allocatable array of the data type ! than contains an index and a real value type ( thefit ), allocatable , target :: results (:) ! create a single instance of the data type type ( thefit ) best integer , allocatable :: genes (:,:) ! our genes for the genetic algorithm integer j integer num_genes , gene_size num_genes = 10 gene_size = 10 allocate ( results ( num_genes )) ! allocate the data type ! to hold fitness and index allocate ( genes ( num_genes , gene_size )) ! allocate our collection of genes call init_genes ( genes ) ! starting data write ( * , '(\"input\")' ) ! we can put format in write statement do j = 1 , num_genes results ( j )% index = j results ( j )% val = fitness ( genes ( j ,:)) ! just a dummy routine for now write ( * , \"(f10.8,i4)\" ) results ( j )% val , results ( j )% index enddo end program User defined operators Motivation With derived data types we may want (need) to define operations (Assignment is predefined) Example: .lt. .gt. == not defined for our data types - We want to find the minimum of our fitness values so we need < operator - In our sort routine we want to do <, >, == - In C++ terms the operators are overloaded We are free to define new operators Two step process to define operators Define a special interface Define the function that performs the operation module sort_mod !defining the interfaces interface operator (. lt .) ! overloads standard .lt. module procedure theless ! the function that does it end interface interface operator (. gt .) ! overloads standard .gt. module procedure thegreat ! the function that does it end interface interface operator (. ge .) ! overloads standard .ge. module procedure thetest ! the function that does it end interface interface operator (. converged .) ! new operator module procedure index_test ! the function that does it end interface contains ! our module will contain ! the required functions function theless ( a , b ) ! overloads .lt. for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical theless ! what we return if ( a % val . lt . b % val ) then ! this is where we do the test theless = . true . else theless = . false . endif return end function theless function thegreat ( a , b ) ! overloads .gt. for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical thegreat if ( a % val . gt . b % val ) then thegreat = . true . else thegreat = . false . endif return end function thegreat function thetest ( a , b ) ! overloads .gt.= for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical thetest if ( a % val >= b % val ) then thetest = . true . else thetest = . false . endif return end function thetest function index_test ( a , b ) ! defines a new operation for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical index_test if ( a % index . gt . b % index ) then ! check the index value for a difference index_test = . true . else index_test = . false . endif return end function index_test Recursive functions introduction Notes Recursive function is one that calls itself Anything that can be done with a do loop can be done using a recursive function Motivation Sometimes it is easier to think recursively Divide an conquer algorithms are recursive by nature - Fast FFTs - Searching - Sorting Algorithm of searching for minimum of an array function findmin ( array ) is size of array 1 ? min in the array is first element else find minimum in left half of array using findmin function find minimum in right half of array using findmin function global minimum is min of left and right half end function Fortran 90 recursive functions Recursive functions should have an interface The result and recursive keywords are required as part of the function definition Example is a function finds the minimum value for an array recursive function realmin ( ain ) result ( themin ) ! recursive and result are required for recursive functions use numz implicit none real ( b8 ) themin , t1 , t2 integer n , right real ( b8 ) , dimension (:) :: ain n = size ( ain ) if ( n == 1 ) then themin = ain ( 1 ) ! if the size is 1 return value return else right = n / 2 t1 = realmin ( ain ( 1 : right )) ! find min in left half t2 = realmin ( ain ( right + 1 : n )) ! find min in right half themin = min ( t1 , t2 ) ! find min of the two sides endif end function Example 2 is the same except the input data is our derived data type !this routine works with the data structure thefit not reals recursive function typemin ( ain ) result ( themin ) use numz use sort_mod use galapagos implicit none real ( b8 ) themin , t1 , t2 integer n , right type ( thefit ) , dimension (:) :: ain ! this line is different n = size ( ain ) if ( n == 1 ) then themin = ain ( 1 )% val ! this line is different return else right = n / 2 t1 = typemin ( ain ( 1 : right )) t2 = typemin ( ain ( right + 1 : n )) themin = min ( t1 , t2 ) endif end function Pointers Motivation Can increase performance Can improve readability Required for some derived data types (linked lists and trees) Useful for allocating \"arrays\" within subroutines Useful for referencing sections of arrays Notes Pointers can be thought of as an alias to another variable In some cases can be used in place of an array To assign a pointer use => instead of just = Unlike C and C++, pointer arithmetic is not allowed First pointer example Similar to the last findmin routine Return a pointer to the minimum recursive function pntmin ( ain ) result ( themin ) ! return a pointer use numz use galapagos use sort_mod ! contains the .lt. operator for thefit type implicit none type ( thefit ), pointer :: themin , t1 , t2 integer n , right type ( thefit ) , dimension (:), target :: ain n = size ( ain ) if ( n == 1 ) then themin => ain ( 1 ) !this is how we do pointer assignment return else right = n / 2 t1 => pntmin ( ain ( 1 : right )) t2 => pntmin ( ain ( right + 1 : n )) if ( t1 . lt . t2 ) then ; themin => t1 ; else ; themin => t2 ; endif endif end function Exercise 4: Carefully write a recursive N! program. Function and subroutine overloading Motivation Allows us to call functions or subroutine with the same name with different argument types Increases readability Notes: Similar in concept to operator overloading Requires an interface Syntax for subroutines is same as for functions Many intrinsic functions have this capability - abs (reals,complex,integer) - sin,cos,tan,exp(reals, complex) - array functions(reals, complex,integer) Example - Recall we had two functions that did the same thing but with different argument types recursive function realmin(ain) result (themin) real(b8) ,dimension(:) :: ain recursive function typemin(ain) result (themin) type (thefit) ,dimension(:) :: ain - We can define a generic interface for these two functions and call them using the same name ! note we have two functions within the same interface ! this is how we indicate function overloading ! both functions are called \"findmin\" in the main program interface findmin ! the first is called with an array of reals as input recursive function realmin ( ain ) result ( themin ) use numz real ( b8 ) themin real ( b8 ) , dimension (:) :: ain end function ! the second is called with a array of data structures as input recursive function typemin ( ain ) result ( themin ) use numz use galapagos real ( b8 ) themin type ( thefit ) , dimension (:) :: ain end function end interface Example usage program darwin use numz use ran_mod use galapagos ! the module that contains the type definition use face ! contains various interfaces use sort_mod ! more about this later it ! contains our sorting routine ! and a few other tricks implicit none ! create an allocatable array of the data type ! than contains an index and a real value type ( thefit ), allocatable , target :: results (:) ! create a single instance of the data type type ( thefit ) best ! pointers to our type type ( thefit ) , pointer :: worst , tmp integer , allocatable :: genes (:,:) ! our genes for the ga integer j integer num_genes , gene_size real ( b8 ) x real ( b8 ), allocatable :: z (:) real ( b8 ), pointer :: xyz (:) ! we'll talk about this next num_genes = 10 gene_size = 10 allocate ( results ( num_genes )) ! allocate the data type to allocate ( genes ( num_genes , gene_size )) ! hold our collection of genes call init_genes ( genes ) ! starting data write ( * , '(\"input\")' ) do j = 1 , num_genes results ( j )% index = j results ( j )% val = fitness ( genes ( j ,:)) ! just a dummy routine write ( * , \"(f10.8,i4)\" ) results ( j )% val , results ( j )% index enddo allocate ( z ( size ( results ))) z = results (:)% val ! copy our results to a real array ! use a recursive subroutine operating on the real array write ( * , * ) \"the lowest fitness: \" , findmin ( z ) ! use a recursive subroutine operating on the data structure write ( * , * ) \"the lowest fitness: \" , findmin ( results ) end program Fortran Minval and Minloc routines Fortran has routines for finding minimum and maximum values in arrays and the locations minval maxval minloc (returns an array) maxloc (returns an array) ! we show two other methods of getting the minimum fitness ! use the built in f90 routines on a real array write ( * , * ) \"the lowest fitness: \" , minval ( z ), minloc ( z ) Pointer assignment This is how we use the pointer function defined above worst is a pointer to our data type note the use of => ! use a recursive subroutine operating on the data ! structure and returning a pointer to the result worst=>pntmin(results) ! note pointer assignment ! what will this line write? write(*,*)\"the lowest fitness: \",worst More pointer usage, association and nullify Motivation Need to find if pointers point to anything Need to find if two pointers point to the same thing Need to deallocate and nullify when they are no longer used Usage We can use associated() to tell if a pointer has been set We can use associated() to compare pointers We use nullify to zero a pointer ! This code will print \"true\" when we find a match, ! that is the pointers point to the same object do j = 1 , num_genes tmp => results ( j ) write ( * , \"(f10.8,i4,l3)\" ) results ( j )% val , & results ( j )% index , & associated ( tmp , worst ) enddo nullify ( tmp ) Notes: If a pointer is nullified the object to which it points is not deallocated. In general, pointers as well as allocatable arrays become undefined on leaving a subroutine This can cause a memory leak Pointer usage to reference an array without copying Motivation Our sort routine calls a recursive sorting routine It is messy and inefficient to pass the array to the recursive routine Solution We define a \"global\" pointer in a module We point the pointer to our input array module Merge_mod_types use galapagos type ( thefit ), allocatable :: work (:) ! a \"global\" work array type ( thefit ), pointer :: a_pntr (:) ! this will be the pointer to our input array end module Merge_mod_types subroutine Sort ( ain , n ) use Merge_mod_types implicit none integer n type ( thefit ), target :: ain ( n ) allocate ( work ( n )) nullify ( a_pntr ) a_pntr => ain ! we assign the pointer to our array ! in RecMergeSort we reference it just like an array call RecMergeSort ( 1 , n ) ! very similar to the findmin functions deallocate ( work ) return end subroutine Sort In our main program sort is called like this: ! our sort routine is also recursive but ! also shows a new usage for pointers call sort ( results , num_genes ) do j = 1 , num_genes write ( * , \"(f10.8,i4)\" ) results ( j )% val , & results ( j )% index enddo Data assignment with structures ! we can copy a whole structure ! with a single assignment best = results ( 1 ) write ( * , * ) \"best result \" , best Using the user defined operator ! using the user defined operator to see if best is worst ! recall that the operator .converged. checks to see if %index matches worst => pntmin ( results ) write ( * , * ) \"worst result \" , worst write ( * , * ) \"converged=\" ,( best . converged . worst ) Passing arrays with a given arbitrary lower bounds Motivation Default lower bound within a subroutine is 1 May want to use a different lower bound if ( allocated ( z )) deallocate ( z ) allocate ( z ( - 10 : 10 )) ! a 21 element array do j =- 10 , 10 z ( j ) = j enddo ! pass z and its lower bound ! in this routine we give the array a specific lower ! bound and show how to use a pointer to reference ! different parts of an array using different indices call boink1 ( z , lbound ( z , 1 )) ! why not just lbound(z) instead of lbound(z,1)? ! lbound(z) returns a rank 1 array subroutine boink1 ( a , n ) use numz implicit none integer , intent ( in ) :: n real ( b8 ), dimension ( n :) :: a ! this is how we set lower bounds in a subroutine write ( * , * ) lbound ( a ), ubound ( a ) end subroutine Warning: because we are using an assumed shape array we need an interface Using pointers to access sections of arrays Motivation Can increase efficiency Can increase readability call boink2 ( z , lbound ( z , 1 )) subroutine boink2 ( a , n ) use numz implicit none integer , intent ( in ) :: n real ( b8 ), dimension ( n :), target :: a real ( b8 ), dimension (:), pointer :: b b => a ( n :) ! b(1) \"points\" to a(-10) write ( * , * ) \"a(-10) =\" , a ( - 10 ), \"b(1) =\" , b ( 1 ) b => a ( 0 :) ! b(1) \"points\" to a(0) write ( * , * ) \"a(-6) =\" , a ( - 6 ), \"b(-5) =\" , b ( - 5 ) end subroutine Allocating an array inside a subroutine and passing it back Motivation Size of arrays are calculated in the subroutine module numz integer , parameter :: b8 = selected_real_kind ( 14 ) end module program bla use numz real ( b8 ), dimension (:) , pointer :: xyz interface boink subroutine boink ( a ) use numz implicit none real ( b8 ), dimension (:), pointer :: a end subroutine end interface nullify ( xyz ) ! nullify sets a pointer to null write ( * , '(l5)' ) associated ( xyz ) ! is a pointer null, should be call boink ( xyz ) write ( * , '(l5)' , advance = \"no\" ) associated ( xyz ) if ( associated ( xyz )) write ( * , '(i5)' ) size ( xyz ) end program subroutine boink ( a ) use numz implicit none real ( b8 ), dimension (:), pointer :: a if ( associated ( a )) deallocate ( a ) allocate ( a ( 10 )) end subroutine An example run F T 10 Our fitness function Given a fixed number of colors, M, and a description of a map of a collection of N states. Find a coloring of the map such that no two states that share a boarder have the same coloring. Example input is a sorted list of 22 western states 22 ar ok tx la mo xx az ca nm ut nv xx ca az nv or xx co nm ut wy ne ks xx ia mo ne sd mn xx id wa or nv ut wy mt xx ks ne co ok mo xx la tx ar xx mn ia sd nd xx mo ar ok ks ne ia xx mt wy id nd xx nd mt sd wy xx ne sd wy co ks mo ia xx nm az co ok tx mn xx nv ca or id ut az xx ok ks nm tx ar mo xx or ca wa id xx sd nd wy ne ia mn xx tx ok nm la ar xx ut nv az co wy id xx wa id or mt xx wy co mt id ut nd sd ne xx Our fitness function takes a potential coloring, that is, an integer vector of length N and a returns the number of boarders that have states of the same coloring How do we represent the map in memory? One way would be to use an array but it would be very sparse Linked lists are often a better way Linked lists Motivation We have a collection of states and for each state a list of adjoining states. (Do not count a boarder twice.) Problem is that you do not know the length of the list until runtime. List of adjoining states will be different lengths for different states Solution - Linked list are a good way to handle such situations Linked lists use a derived data type with at least two components Data Pointer to next element module list_stuff type llist integer index ! data type ( llist ), pointer :: next ! pointer to the ! next element end type llist end module Linked list usage One way to fill a linked list is to use a recursive function `fortran recursive subroutine insert (item, root) use list_stuff implicit none type(llist), pointer :: root integer item if (.not. associated(root)) then allocate(root) nullify(root%next) root%index = item else call insert(item,root%next) endif end subroutine - - - - - - ## Our map representation - An array of the derived data type states - State is name of a state - Linked list containing boarders ```fortran type states character(len=2)name type(llist),pointer:: list end type states - Notes: - We have an array of linked lists - This data structure is often used to represent sparse arrays - We could have a linked list of linked lists - State name is not really required Date and time functions Motivation May want to know the date and time of your program Two functions ! all arguments are optional call date_and_time ( date = c_date , & ! character(len=8) ccyymmdd time = c_time , & ! character(len=10) hhmmss.sss zone = c_zone , & ! character(len=10) +/-hhmm (time zone) values = ivalues ) ! integer ivalues(8) all of the above call system_clock ( count = ic , & ! count of system clock (clicks) count_rate = icr , & ! clicks / second count_max = max_c ) ! max value for count Non advancing and character IO Motivation We read the states using the two character identification One line per state and do not know how many boarder states per line Note: Our list of states is presorted character ( len = 2 ) a ! we have a character variable of length 2 read ( 12 , * ) nstates ! read the number of states allocate ( map ( nstates )) ! and allocate our map do i = 1 , nstates read ( 12 , \"(a2)\" , advance = \"no\" ) map ( i )% name ! read the name !write(*,*)\"state:\",map(i)%name nullify ( map ( i )% list ) ! \"zero out\" our list do read ( 12 , \"(1x,a2)\" , advance = \"no\" ) a ! read list of states ! without going to the ! next line if ( lge ( a , \"xx\" ) . and . lle ( a , \"xx\" )) then ! if state == xx backspace ( 12 ) ! go to the next line read ( 12 , \"(1x,a2)\" , end = 1 ) a ! go to the next line exit endif 1 continue if ( llt ( a , map ( i )% name )) then ! we only add a state to ! our list if its name ! is before ours thus we ! only count boarders 1 time ! what we want put into our linked list is an index ! into our map where we find the bordering state ! thus we do the search here ! any ideas on a better way of doing this search? found =- 1 do j = 1 , i - 1 if ( lge ( a , map ( j )% name ) . and . lle ( a , map ( j )% name )) then !write(*,*)a found = j exit endif enddo if ( found == - 1 ) then write ( * , * ) \"error\" stop endif ! found the index of the boarding state insert it into our list ! note we do the insert into the linked list for a particular state call insert ( found , map ( i )% list ) endif enddo enddo Internal IO Motivation May need to create strings on the fly May need to convert from strings to reals and integers Similar to sprintf and sscanf How it works Create a string Do a normal write except write to the string instead of file number Example 1: creating a date and time stamped file name character ( len = 12 ) tmpstr write ( tmpstr , \"(a12)\" )( c_date ( 5 : 8 ) // c_time ( 1 : 4 ) // \".dat\" ) ! // does string concatination write ( * , * ) \"name of file= \" , tmpstr open ( 14 , file = tmpstr ) name of file = 0327111 4. dat Example 2: Creating a format statement at run time (array of integers and a real) ! test_vect is an array that we do not know its length until run time nstate = 9 ! the size of the array write ( fstr , '(\"(\",i4,\"i1,1x,f10.5)\")' ) nstates write ( * , * ) \"format= \" , fstr write ( * , fstr ) test_vect , fstr format = ( 9 i1 , 1 x , f10 . 5 ) Any other ideas for writing an array when you do not know its length? Example 3: Reading from a string integer ht , minut , sec read ( c_time , \"(3i2)\" ) hr , minut , sec Inquire function Motivation Need to get information about I/O Inquire statement has two forms Information about files (23 different requests can be done) Information about space required for binary output of a value Example: find the size of your real relative to the \"standard\" real Useful for inter language programming Useful for determining data types in MPI (MPI_REAL or MPI_DOUBLE_PRECISION) inquire ( iolength = len_real ) 1.0 inquire ( iolength = len_b8 ) 1.0_b8 write ( * , * ) \"len_b8 \" , len_b8 write ( * , * ) \"len_real\" , len_real iratio = len_b8 / len_real select case ( iratio ) case ( 1 ) my_mpi_type = mpi_real case ( 2 ) my_mpi_type = mpi_double_precision case default write ( * , * ) \"type undefined\" my_mpi_type = 0 end select An example run len_b8 2 len_real 1 Namelist Now part of the standard Motivation A convenient method of doing I/O Good for cases where you have similar runs but change one or two variables Good for formatted output Notes: A little flaky No options for overloading format Example: integer ncolor logical force namelist / the_input / ncolor , force ncolor = 4 force = . true . read ( 13 , the_input ) write ( * , the_input ) On input: & THE_INPUT NCOLOR=4,FORCE = F / Output is &THE_INPUT NCOLOR = 4, FORCE = F / Vector valued functions Motivation May want a function that returns a vector Notes Again requires an interface Use explicit or assumed size array Do not return a pointer to a vector unless you really want a pointer Example: Take an integer input vector which represents an integer in some base and add 1 Could be used in our program to find a \"brute force\" solution function add1 ( vector , max ) result ( rtn ) integer , dimension (:), intent ( in ) :: vector integer , dimension ( size ( vector )) :: rtn integer max integer len logical carry len = size ( vector ) rtn = vector i = 0 carry = . true . do while ( carry ) ! just continue until we do not do a carry i = i + 1 rtn ( i ) = rtn ( i ) + 1 if ( rtn ( i ) . gt . max ) then if ( i == len ) then ! role over set everything back to 0 rtn = 0 else rtn ( i ) = 0 endif else carry = . false . endif enddo end function Usage test_vect = 0 do test_vect = add1 ( test_vect , 3 ) result = fitness ( test_vect ) if ( result . lt . 1.0_b8 ) then write ( * , * ) test_vect stop endif enddo Complete source for recent discussions recent.f90 fort.13 Exersize 5 Modify the program to use the random number generator given earlier. Some array specific intrinsic functions ALL True if all values are true (LOGICAL) ANY True if any value is true (LOGICAL) COUNT Number of true elements in an array (LOGICAL) DOT_PRODUCT Dot product of two rank one arrays MATMUL Matrix multiplication MAXLOC Location of a maximum value in an array MAXVAL Maximum value in an array MINLOC Location of a minimum value in an array MINVAL Minimum value in an array PACK Pack an array into an array of rank one PRODUCT Product of array elements RESHAPE Reshape an array SPREAD Replicates array by adding a dimension SUM Sum of array elements TRANSPOSE Transpose an array of rank two UNPACK Unpack an array of rank one into an array under a mask Examples program matrix real w ( 10 ), x ( 10 ), mat ( 10 , 10 ) call random_number ( w ) call random_number ( mat ) x = matmul ( w , mat ) ! regular matrix multiply USE IT write ( * , '(\"dot(x,x)=\",f10.5)' ), dot_product ( x , x ) end program program allit character ( len = 10 ) :: f1 = \"(3l1)\" character ( len = 10 ) :: f2 = \"(3i2)\" integer b ( 2 , 3 ), c ( 2 , 3 ), one_d ( 6 ) logical l ( 2 , 3 ) one_d = ( / 1 , 3 , 5 , 2 , 4 , 6 / ) b = transpose ( reshape (( / 1 , 3 , 5 , 2 , 4 , 6 / ), shape = ( / 3 , 2 / ))) C = transpose ( reshape (( / 0 , 3 , 5 , 7 , 4 , 8 / ), shape = ( / 3 , 2 / ))) l = ( b . ne . c ) write ( * , f2 )(( b ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f2 )(( c ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f1 )(( l ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f1 ) all ( b . ne . C ) !is .false. write ( * , f1 ) all ( b . ne . C , DIM = 1 ) !is [.true., .false., .false.] write ( * , f1 ) all ( b . ne . C , DIM = 2 ) !is [.false., .false.] end The output is: 1 3 5 2 4 6 0 3 5 7 4 8 TFF TFT F TFF FF The rest of our GA Includes Reproduction Mutation Nothing new in either of these files Source and makefile \"git\" Source and makefile \"*tgz\" Compiler Information gfortran .f, .for, .ftn .f77 fixed-format Fortran source; compile .f90, .f95 free-format Fortran source; compile -fbacktrace Add debug information for runtime traceback -ffree-form -ffixed-form source form -O0, -O1, -O2, -O3 optimization level .fpp, .FPP, .F, .FOR, .FTN, .F90, .F95, .F03 or .F08 Fortran source file with preprocessor directives -fopenmp turn on OpenMP Intel .f, .for, .ftn fixed-format Fortran source; compile .f90, .f95 free-format Fortran source; compile -O0, -O1, -O2, -O3, -O4 optimization level .fpp, .F, .FOR, .FTN, .FPP, .F90 Fortran source file with preprocessor directives -g compile for debug * -traceback -notraceback (default) Add debug information for runtime traceback -nofree, -free Source is fixed or free format -fopenmp turn on OpenMP Portland Group (x86) .f, .for, .ftn fixed-format Fortran source; compile .f90, .f95, .f03 free-format Fortran source; compile .cuf free-format CUDA Fortran source; compile .CUF free-format CUDA Fortran source; preprocess, compile -O0, -O1, -O2, -O3, -O4 optimization level -g compile for debug * -traceback (default) -notraceback Add debug information for runtime traceback -Mfixed, -Mfree Source is fixed or free format -qmp turn on OpenMP IBM xlf xlf, xlf_r, f77, fort77 Compile FORTRAN 77 source files. _r = thread safe xlf90, xlf90_r, f90 Compile Fortran 90 source files. _r = thread safe xlf95, xlf95_r, f95 Compile Fortran 95 source files. _r = thread safe xlf2003, xlf2003_r,f2003 * Compile Fortran 2003 source files. _r = thread safe xlf2008, xlf2008_r, f2008 * Compile Fortran 2008 source files. .f, .f77, .f90, .f95, .f03, .f08 Fortran source file .F, .F77, .F90, .F95, .F03, .F08 Fortran source file with preprocessor directives -qtbtable=full Add debug information for runtime traceback -qsmp=omp turn on OpenMP -O0, -O1, -O2, -O3, -O4, O5 optimization level -g , g0, g1,...g9 compile for debug Summary Fortran 90 has features to: Enhance performance Enhance portability Enhance reliability Enhance maintainability Fortran 90 has new language elements Source form Derived data types Dynamic memory allocation functions Kind facility for portability and easy modification Many new intrinsic function Array assignments Examples Help show how things work Reference for future use Overview of F90 Introduction to Fortran Language Meta language used in this compact summary Structure of files that can be compiled Executable Statements and Constructs Declarations Key words - other than I/O Key words related to I/O Operators Constants Input/Output Statements Formats Intrinsic Functions Introduction to Fortran Language Brought to you by ANSI committee X3J3 and ISO-IEC/JTC1/SC22/WG5 (Fortran) This is neither complete nor precisely accurate, but hopefully, after a small investment of time it is easy to read and very useful. This is the free form version of Fortran, no statement numbers, no C in column 1, start in column 1 (not column 7), typically indent 2, 3, or 4 spaces per each structure. The typical extension is .f90 . Continue a statement on the next line by ending the previous line with an ampersand &amp; . Start the continuation with &amp; for strings. The rest of any line is a comment starting with an exclamation mark ! . Put more than one statement per line by separating statements with a semicolon ; . Null statements are OK, so lines can end with semicolons. Separate words with space or any form of \"white space\" or punctuation. Meta language used in this compact summary <xxx> means fill in something appropriate for xxx and do not type the \"<\" or \">\" . ... ellipsis means the usual, fill in something, one or more lines [stuff] means supply nothing or at most one copy of \"stuff\" [stuff1 [stuff2]] means if \"stuff1\" is included, supply nothing or at most one copy of stuff2. \"old\" means it is in the language, like almost every feature of past Fortran standards, but should not be used to write new programs. Structure of files that can be compiled program <name> usually file name is <name>.f90 use <module_name> bring in any needed modules implicit none good for error detection <declarations> <executable statements> order is important, no more declarations end program <name> block data <name> old <declarations> common, dimension, equivalence now obsolete end block data <name> module <name> bring back in with use <name> implicit none good for error detection <declarations> can have private and public and interface end module <name> subroutine <name> use: call <name> to execute implicit none good for error detection <declarations> <executable statements> end subroutine <name> subroutine <name>(par1, par2, ...) use: call <name>(arg1, arg2,... ) to execute implicit none optional, good for error detection <declarations> par1, par2, ... are defined in declarations and can be specified in, inout, pointer, etc. <executable statements> return optional, end causes automatic return entry <name> (par...) old, optional other entries end subroutine <name> function <name>(par1, par2, ...) result(<rslt>) use: <name>(arg1, arg2, ... argn) as variable implicit none optional, good for error detection <declarations> rslt, par1, ... are defined in declarations <executable statements> <rslt> = <expression> required somewhere in execution [return] optional, end causes automatic return end function <name> old <type> function(...) <name> use: <name>(arg1, arg2, ... argn) as variable <declarations> <executable statements> <name> = <expression> required somewhere in execution [return] optional, end causes automatic return end function <name> Executable Statements and Constructs <statement> will mean exactly one statement in this section a construct is multiple lines <label> : <statement> any statement can have a label (a name) <variable> = <expression> assignment statement <pointer> >= <variable> the pointer is now an alias for the variable <pointer1> >= <pointer2> pointer1 now points same place as pointer2 stop can be in any executable statement group, stop <integer> terminates execution of the program, stop <string> can have optional integer or string return exit from subroutine or function do <variable>=<from>,<to> [,<increment&gt] optional: <label> : do ... <statements> exit \\_optional or exit <label&gt if (<boolean expression>) exit / exit the loop cycle \\_optional or cycle <label> if (<boolean expression>) cycle / continue with next loop iteration end do optional: end do <name> do while (<boolean expression>) ... optional exit and cycle allowed end do do ... exit required to end the loop optional cycle can be used end do if ( <boolean expression> ) <statement> execute the statement if the boolean expression is true if ( <boolean expression1> ) then ... execute if expression1 is true else if ( <boolean expression2> ) then ... execute if expression2 is true else if ( <boolean expression3> ) then ... execute if expression3 is true else ... execute if none above are true end if select case (<expression>) optional <name> : select case ... case (<value>) <statements> execute if expression == value case (<value1>:<value2>) <statements> execute if value1 &le; expression &le; value2 ... case default <statements> execute if no values above match end select optional end select <name> real, dimension(10,12) :: A, R a sample declaration for use with \"where\" ... where (A /= 0.0) conditional assignment, only assignment allowed R = 1.0/A elsewhere R = 1.0 elements of R set to 1.0 where A == 0.0 end where go to <statement number> old go to (<statement number list>), <expression> old for I/O statements, see: section 10.0 Input/Output Statements many old forms of statements are not listed Declarations There are five (5) basic types: integer, real, complex, character and logical. There may be any number of user derived types. A modern (not old) declaration starts with a type, has attributes, then ::, then variable(s) names integer i, pivot, query old integer, intent (inout) :: arg1 integer (selected_int_kind (5)) :: i1, i2 integer, parameter :: m = 7 integer, dimension(0:4, -5:5, 10:100) :: A3D double precision x old real (selected_real_kind(15,300) :: x complex :: z logical, parameter :: what_if = .true. character, parameter :: me = \"Jon Squire\" type <name> a new user type, derived type declarations end type <name> type (<name>) :: stuff declaring stuff to be of derived type <name> real, dimension(:,:), allocatable, target :: A real, dimension(:,:), pointer :: P Attributes may be: allocatable no memory used here, allocate later dimension vector or multi dimensional array external will be defined outside this compilation intent argument may be in, inout or out intrinsic declaring function to be an intrinsic optional argument is optional parameter declaring a constant, can not be changed later pointer declaring a pointer private in a module, a private declaration public in a module, a public declaration save keep value from one call to the next, static target can be pointed to by a pointer Note: not all combinations of attributes are legal Key words (other than I/O) note: \"statement\" means key word that starts a statement, one line unless there is a continuation \"&amp;\" \"construct\" means multiple lines, usually ending with \"end ...\" \"attribute\" means it is used in a statement to further define \"old\" means it should not be used in new code allocatable attribute, no space allocated here, later allocate allocate statement, allocate memory space now for variable assign statement, old, assigned go to assignment attribute, means subroutine is assignment (=) block data construct, old, compilation unit, replaced by module call statement, call a subroutine case statement, used in select case structure character statement, basic type, intrinsic data type common statement, old, allowed overlaying of storage complex statement, basic type, intrinsic data type contains statement, internal subroutines and functions follow continue statement, old, a place to put a statement number cycle statement, continue the next iteration of a do loop data statement, old, initialized variables and arrays deallocate statement, free up storage used by specified variable default statement, in a select case structure, all others do construct, start a do loop double precision statement, old, replaced by selected_real_kind(15,300) else construct, part of if else if else end if else if construct, part of if else if else end if elsewhere construct, part of where elsewhere end where end block data construct, old, ends block data end do construct, ends do end function construct, ends function end if construct, ends if end interface construct, ends interface end module construct, ends module end program construct, ends program end select construct, ends select case end subroutine construct, ends subroutine end type construct, ends type end where construct, ends where entry statement, old, another entry point in a procedure equivalence statement, old, overlaid storage exit statement, continue execution outside of a do loop external attribute, old statement, means defines else where function construct, starts the definition of a function go to statement, old, requires fixed form statement number if statement and construct, if(...) statement implicit statement, \"none\" is preferred to help find errors in a keyword for intent, the argument is read only inout a keyword for intent, the argument is read/write integer statement, basic type, intrinsic data type intent attribute, intent(in) or intent(out) or intent(inout) interface construct, begins an interface definition intrinsic statement, says that following names are intrinsic kind attribute, sets the kind of the following variables len attribute, sets the length of a character string logical statement, basic type, intrinsic data type module construct, beginning of a module definition namelist statement, defines a namelist of input/output nullify statement, nullify(some_pointer) now points nowhere only attribute, restrict what comes from a module operator attribute, indicates function is an operator, like + optional attribute, a parameter or argument is optional out a keyword for intent, the argument will be written parameter attribute, old statement, makes variable real only pause old, replaced by stop pointer attribute, defined the variable as a pointer alias private statement and attribute, in a module, visible inside program construct, start of a main program public statement and attribute, in a module, visible outside real statement, basic type, intrinsic data type recursive attribute, allows functions and derived type recursion result attribute, allows naming of function result result(Y) return statement, returns from, exits, subroutine or function save attribute, old statement, keep value between calls select case construct, start of a case construct stop statement, terminate execution of the main procedure subroutine construct, start of a subroutine definition target attribute, allows a variable to take a pointer alias then part of if construct type construct, start of user defined type type ( ) statement, declaration of a variable for a users type use statement, brings in a module where construct, conditional assignment while construct, a while form of a do loop Key words related to I/O backspace statement, back up one record close statement, close a file endfile statement, mark the end of a file format statement, old, defines a format inquire statement, get the status of a unit open statement, open or create a file print statement, performs output to screen read statement, performs input rewind statement, move read or write position to beginning write statement, performs output Operators ** exponentiation * multiplication / division + addition - subtraction // concatenation == .eq. equality /= .ne. not equal < .lt. less than > .gt. greater than <= .le. less than or equal >= .ge. greater than or equal .not. complement, negation .and. logical and .or. logical or .eqv. logical equivalence .neqv. logical not equivalence, exclusive or .eq. == equality, old .ne. /= not equal. old .lt. < less than, old .gt. > greater than, old .le. <= less than or equal, old .ge. >= greater than or equal, old Other punctuation: / ... / used in data, common, namelist and other statements (/ ... /) array constructor, data is separated by commas 6*1.0 in some contexts, 6 copies of 1.0 (i:j:k) in some contexts, a list i, i+k, i+2k, i+3k, ... i+nk&le;j (:j) j and all below (i:) i and all above (:) undefined or all in range Constants Logical constants: .true. True .false. False Integer constants: 0 1 -1 123456789 Real constants: 0.0 1.0 -1.0 123.456 7.1E+10 -52.715E-30 Complex constants: (0.0, 0.0) (-123.456E+30, 987.654E-29) Character constants: \"ABC\" \"a\" \"123'abc$%#@!\" \" a quote \"\" \" 'ABC' 'a' '123\"abc$%#@!' ' a apostrophe '' ' Derived type values: type name character (len=30) :: last character (len=30) :: first character (len=30) :: middle end type name type address character (len=40) :: street character (len=40) :: more character (len=20) :: city character (len=2) :: state integer (selected_int_kind(5)) :: zip_code integer (selected_int_kind(4)) :: route_code end type address type person type (name) lfm type (address) snail_mail end type person type (person) :: a_person = person( name(\"Squire\",\"Jon\",\"S.\"), &amp; address(\"106 Regency Circle\", \"\", \"Linthicum\", \"MD\", 21090, 1936)) a_person%snail_mail%route_code == 1936 Input/Output Statements open (<unit number>) open (unit=<unit number>, file=<file name>, iostat=<variable>) open (unit=<unit number>, ... many more, see below ) close (<unit number>) close (unit=<unit number>, iostat=<variable>, err=<statement number>, status=\"KEEP\") read (<unit number>) <input list> read (unit=<unit number>, fmt=<format>, iostat=<variable>, end=<statement number>, err=<statement number>) <input list> read (unit=<unit number>, rec=<record number>) <input list> write (<unit number>) <output list> write (unit=<unit number>, fmt=<format>, iostat=<variable>, err=<statement number>) <output list> write (unit=<unit number>, rec=<record number>) <output list> print *, <output list> print \"(<your format here, use apostrophe, not quote>)\", <output list> rewind <unit number> rewind (<unit number>, err=<statement number>) backspace <unit number> backspace (<unit number>, iostat=<variable>) endfile <unit number> endfile (<unit number>, err=<statement number>, iostat=<variable>) inquire ( <unit number>, exists = <variable>) inquire ( file=<\"name\">, opened = <variable1>, access = <variable2> ) inquire ( iolength = <variable> ) x, y, A ! gives \"recl\" for \"open\" namelist /<name>/ <variable list> defines a name list read(*,nml=<name>) reads some/all variables in namelist write(*,nml=<name>) writes all variables in namelist &amp;<name> <variable>=<value> ... <variable=value> / data for namelist read Input / Output specifiers access one of \"sequential\" \"direct\" \"undefined\" action one of \"read\" \"write\" \"readwrite\" advance one of \"yes\" \"no\" blank one of \"null\" \"zero\" delim one of \"apostrophe\" \"quote\" \"none\" end = <integer statement number> old eor = <integer statement number> old err = <integer statement number> old exist = <logical variable> file = <\"file name\"> fmt = <\"(format)\"> or <character variable> format form one of \"formatted\" \"unformatted\" \"undefined\" iolength = <integer variable, size of unformatted record> iostat = <integer variable> 0==good, negative==eof, positive==bad name = <character variable for file name> named = <logical variable> nml = <namelist name> nextrec = <integer variable> one greater than written number = <integer variable unit number> opened = <logical variable> pad one of \"yes\" \"no\" position one of \"asis\" \"rewind\" \"append\" rec = <integer record number> recl = <integer unformatted record size> size = <integer variable> number of characters read before eor status one of \"old\" \"new\" \"unknown\" \"replace\" \"scratch\" \"keep\" unit = <integer unit number> Individual questions direct = <character variable> \"yes\" \"no\" \"unknown\" formatted = <character variable> \"yes\" \"no\" \"unknown\" read = <character variable> \"yes\" \"no\" \"unknown\" readwrite = <character variable> \"yes\" \"no\" \"unknown\" sequential = <character variable> \"yes\" \"no\" \"unknown\" unformatted = <character variable> \"yes\" \"no\" \"unknown\" write = <character variable> \"yes\" \"no\" \"unknown\" Formats format an explicit format can replace * in any I/O statement. Include the format in apostrophes or quotes and keep the parenthesis. examples: print \"(3I5,/(2X,3F7.2/))\", <output list> write(6, '(a,E15.6E3/a,G15.2)' ) <output list> read(unit=11, fmt=\"(i4, 4(f3.0,TR1))\" ) <input list> A format includes the opening and closing parenthesis. A format consists of format items and format control items separated by comma. A format may contain grouping parenthesis with an optional repeat count. Format Items, data edit descriptors: key: w is the total width of the field (filled with *** if overflow) m is the least number of digits in the (sub)field (optional) d is the number of decimal digits in the field e is the number of decimal digits in the exponent subfield c is the repeat count for the format item n is number of columns cAw data of type character (w is optional) cBw.m data of type integer with binary base cDw.d data of type real -- same as E, old double precision cEw.d or Ew.dEe data of type real cENw.d or ENw.dEe data of type real -- exponent a multiple of 3 cESw.d or ESw.dEe data of type real -- first digit non zero cFw.d data of type real -- no exponent printed cGw.d or Gw.dEe data of type real -- auto format to F or E nH n characters follow the H, no list item cIw.m data of type integer cLw data of type logical -- .true. or .false. cOw.m data of type integer with octal base cZw.m data of type integer with hexadecimal base \"<string>\" literal characters to output, no list item '<string>' literal characters to output, no list item Format Control Items, control edit descriptors: BN ignore non leading blanks in numeric fields BZ treat nonleading blanks in numeric fields as zeros nP apply scale factor to real format items old S printing of optional plus signs is processor dependent SP print optional plus signs SS do not print optional plus signs Tn tab to specified column TLn tab left n columns TRn tab right n columns nX tab right n columns / end of record (implied / at end of all format statements) : stop format processing if no more list items <input list> can be: a variable an array name an implied do ((A(i,j),j=1,n) ,i=1,m) parenthesis and commas as shown note: when there are more items in the input list than format items, the repeat rules for formats applies. <output list> can be: a constant a variable an expression an array name an implied do ((A(i,j),j=1,n) ,i=1,m) parenthesis and commas as shown note: when there are more items in the output list than format items, the repeat rules for formats applies. Repeat Rules for Formats: Each format item is used with a list item. They are used in order. When there are more list items than format items, then the following rule applies: There is an implied end of record, /, at the closing parenthesis of the format, this is processed. Scan the format backwards to the first left parenthesis. Use the repeat count, if any, in front of this parenthesis, continue to process format items and list items. Note: an infinite loop is possible print \"(3I5/(1X/))\", I, J, K, L may never stop Intrinsic Functions Intrinsic Functions are presented in alphabetical order and then grouped by topic. The function name appears first. The argument(s) and result give an indication of the type(s) of argument(s) and results. [,dim=] indicates an optional argument \"dim\". \"mask\" must be logical and usually conformable. \"character\" and \"string\" are used interchangeably. A brief description or additional information may appear. Intrinsic Functions (alphabetical): abs(integer_real_complex) result(integer_real_complex) achar(integer) result(character) integer to character acos(real) result(real) arccosine |real| &le; 1.0 0&le;result&le;Pi adjustl(character) result(character) left adjust, blanks go to back adjustr(character) result(character) right adjust, blanks to front aimag(complex) result(real) imaginary part aint(real [,kind=]) result(real) truncate to integer toward zero all(mask [,dim]) result(logical) true if all elements of mask are true allocated(array) result(logical) true if array is allocated in memory anint(real [,kind=]) result(real) round to nearest integer any(mask [,dim=}) result(logical) true if any elements of mask are true asin(real) result(real) arcsine |real| &le; 1.0 -Pi/2&le;result&le;Pi/2 associated(pointer [,target=]) result(logical) true if pointing atan(real) result(real) arctangent -Pi/2&le;result&le;Pi/2 atan2(y=real,x=real) result(real) arctangent -Pi&le;result&le;Pi bit_size(integer) result(integer) size in bits in model of argument btest(i=integer,pos=integer) result(logical) true if pos has a 1, pos=0.. ceiling(real) result(real) truncate to integer toward infinity char(integer [,kind=]) result(character) integer to character [of kind] cmplx(x=real [,y=real] [kind=]) result(complex) x+iy conjg(complex) result(complex) reverse the sign of the imaginary part cos(real_complex) result(real_complex) cosine cosh(real) result(real) hyperbolic cosine count(mask [,dim=]) result(integer) count of true entries in mask cshift(array,shift [,dim=]) circular shift elements of array, + is right date_and_time([date=] [,time=] [,zone=] [,values=]) y,m,d,utc,h,m,s,milli dble(integer_real_complex) result(real_kind_double) convert to double digits(integer_real) result(integer) number of bits to represent model dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction dot_product(vector_a,vector_b) result(integer_real_complex) inner product dprod(x=real,y=real) result(x_times_y_double) double precision product eoshift(array,shift [,boundary=] [,dim=]) end-off shift using boundary epsilon(real) result(real) smallest positive number added to 1.0 /= 1.0 exp(real_complex) result(real_complex) e raised to a power exponent(real) result(integer) the model exponent of the argument floor(real) result(real) truncate to integer towards negative infinity fraction(real) result(real) the model fractional part of the argument huge(integer_real) result(integer_real) the largest model number iachar(character) result(integer) position of character in ASCII sequence iand(integer,integer) result(integer) bit by bit logical and ibclr(integer,pos) result(integer) argument with pos bit cleared to zero ibits(integer,pos,len) result(integer) extract len bits starting at pos ibset(integer,pos) result(integer) argument with pos bit set to one ichar(character) result(integer) pos in collating sequence of character ieor(integer,integer) result(integer) bit by bit logical exclusive or index(string,substring [,back=]) result(integer) pos of substring int(integer_real_complex) result(integer) convert to integer ior(integer,integer) result(integer) bit by bit logical or ishft(integer,shift) result(integer) shift bits in argument by shift ishftc(integer, shift) result(integer) shift circular bits in argument kind(any_intrinsic_type) result(integer) value of the kind lbound(array,dim) result(integer) smallest subscript of dim in array len(character) result(integer) number of characters that can be in argument len_trim(character) result(integer) length without trailing blanks lge(string_a,string_b) result(logical) string_a &ge; string_b lgt(string_a,string_b) result(logical) string_a > string_b lle(string_a,string_b) result(logical) string_a &le; string_b llt(string_a,string_b) result(logical) string_a < string_b log(real_complex) result(real_complex) natural logarithm log10(real) result(real) logarithm base 10 logical(logical [,kind=]) convert to logical matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical max(a1,a2,a3,...) result(integer_real) maximum of list of values maxexponent(real) result(integer) maximum exponent of model type maxloc(array [,mask=]) result(integer_vector) indices in array of maximum maxval(array [,dim=] [,mask=]) result(array_element) maximum value merge(true_source,false_source,mask) result(source_type) choose by mask min(a1,a2,a3,...) result(integer-real) minimum of list of values minexponent(real) result(integer) minimum(negative) exponent of model type minloc(array [,mask=]) result(integer_vector) indices in array of minimum minval(array [,dim=] [,mask=]) result(array_element) minimum value mod(a=integer_real,p) result(integer_real) a modulo p modulo(a=integer_real,p) result(integer_real) a modulo p mvbits(from,frompos,len,to,topos) result(integer) move bits nearest(real,direction) result(real) nearest value toward direction nint(real [,kind=]) result(real) round to nearest integer value not(integer) result(integer) bit by bit logical complement pack(array,mask [,vector=]) result(vector) vector of elements from array present(argument) result(logical) true if optional argument is supplied product(array [,dim=] [,mask=]) result(integer_real_complex) product radix(integer_real) result(integer) radix of integer or real model, 2 random_number(harvest=real_out) subroutine, uniform random number 0 to 1 random_seed([size=] [,put=] [,get=]) subroutine to set random number seed range(integer_real_complex) result(integer_real) decimal exponent of model real(integer_real_complex [,kind=]) result(real) convert to real repeat(string,ncopies) result(string) concatenate n copies of string reshape(source,shape,pad,order) result(array) reshape source to array rrspacing(real) result(real) reciprocal of relative spacing of model scale(real,integer) result(real) multiply by 2**integer scan(string,set [,back]) result(integer) position of first of set in string selected_int_kind(integer) result(integer) kind number to represent digits selected_real_kind(integer,integer) result(integer) kind of digits, exp set_exponent(real,integer) result(real) put integer as exponent of real shape(array) result(integer_vector) vector of dimension sizes sign(integer_real,integer_real) result(integer_real) sign of second on first sin(real_complex) result(real_complex) sine of angle in radians sinh(real) result(real) hyperbolic sine of argument size(array [,dim=]) result(integer) number of elements in dimension spacing(real) result(real) spacing of model numbers near argument spread(source,dim,ncopies) result(array) expand dimension of source by 1 sqrt(real_complex) result(real_complex) square root of argument sum(array [,dim=] [,mask=]) result(integer_real_complex) sum of elements system_clock([count=] [,count_rate=] [,count_max=]) subroutine, all out tan(real) result(real) tangent of angle in radians tanh(real) result(real) hyperbolic tangent of angle in radians tiny(real) result(real) smallest positive model representation transfer(source,mold [,size]) result(mold_type) same bits, new type transpose(matrix) result(matrix) the transpose of a matrix trim(string) result(string) trailing blanks are removed ubound(array,dim) result(integer) largest subscript of dim in array unpack(vector,mask,field) result(v_type,mask_shape) field when not mask verify(string,set [,back]) result(integer) pos in string not in set Intrinsic Functions (grouped by topic): Intrinsic Functions (Numeric) abs(integer_real_complex) result(integer_real_complex) acos(real) result(real) arccosine |real| &le; 1.0 0&le;result&le;Pi aimag(complex) result(real) imaginary part aint(real [,kind=]) result(real) truncate to integer toward zero anint(real [,kind=]) result(real) round to nearest integer asin(real) result(real) arcsine |real| &le; 1.0 -Pi/2&le;result&le;Pi/2 atan(real) result(real) arctangent -Pi/2&le;result&le;Pi/2 atan2(y=real,x=real) result(real) arctangent -Pi&le;result&le;Pi ceiling(real) result(real) truncate to integer toward infinity cmplx(x=real [,y=real] [kind=]) result(complex) x+iy conjg(complex) result(complex) reverse the sign of the imaginary part cos(real_complex) result(real_complex) cosine cosh(real) result(real) hyperbolic cosine dble(integer_real_complex) result(real_kind_double) convert to double digits(integer_real) result(integer) number of bits to represent model dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction dot_product(vector_a,vector_b) result(integer_real_complex) inner product dprod(x=real,y=real) result(x_times_y_double) double precision product epsilon(real) result(real) smallest positive number added to 1.0 /= 1.0 exp(real_complex) result(real_complex) e raised to a power exponent(real) result(integer) the model exponent of the argument floor(real) result(real) truncate to integer towards negative infinity fraction(real) result(real) the model fractional part of the argument huge(integer_real) result(integer_real) the largest model number int(integer_real_complex) result(integer) convert to integer log(real_complex) result(real_complex) natural logarithm log10(real) result(real) logarithm base 10 matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical max(a1,a2,a3,...) result(integer_real) maximum of list of values maxexponent(real) result(integer) maximum exponent of model type maxloc(array [,mask=]) result(integer_vector) indices in array of maximum maxval(array [,dim=] [,mask=]) result(array_element) maximum value min(a1,a2,a3,...) result(integer-real) minimum of list of values minexponent(real) result(integer) minimum(negative) exponent of model type minloc(array [,mask=]) result(integer_vector) indices in array of minimum minval(array [,dim=] [,mask=]) result(array_element) minimum value mod(a=integer_real,p) result(integer_real) a modulo p modulo(a=integer_real,p) result(integer_real) a modulo p nearest(real,direction) result(real) nearest value toward direction nint(real [,kind=]) result(real) round to nearest integer value product(array [,dim=] [,mask=]) result(integer_real_complex) product radix(integer_real) result(integer) radix of integer or real model, 2 random_number(harvest=real_out) subroutine, uniform random number 0 to 1 random_seed([size=] [,put=] [,get=]) subroutine to set random number seed range(integer_real_complex) result(integer_real) decimal exponent of model real(integer_real_complex [,kind=]) result(real) convert to real rrspacing(real) result(real) reciprocal of relative spacing of model scale(real,integer) result(real) multiply by 2**integer set_exponent(real,integer) result(real) put integer as exponent of real sign(integer_real,integer_real) result(integer_real) sign of second on first sin(real_complex) result(real_complex) sine of angle in radians sinh(real) result(real) hyperbolic sine of argument spacing(real) result(real) spacing of model numbers near argument sqrt(real_complex) result(real_complex) square root of argument sum(array [,dim=] [,mask=]) result(integer_real_complex) sum of elements tan(real) result(real) tangent of angle in radians tanh(real) result(real) hyperbolic tangent of angle in radians tiny(real) result(real) smallest positive model representation transpose(matrix) result(matrix) the transpose of a matrix Intrinsic Functions (Logical and bit) all(mask [,dim]) result(logical) true if all elements of mask are true any(mask [,dim=}) result(logical) true if any elements of mask are true bit_size(integer) result(integer) size in bits in model of argument btest(i=integer,pos=integer) result(logical) true if pos has a 1, pos=0.. count(mask [,dim=]) result(integer) count of true entries in mask iand(integer,integer) result(integer) bit by bit logical and ibclr(integer,pos) result(integer) argument with pos bit cleared to zero ibits(integer,pos,len) result(integer) extract len bits starting at pos ibset(integer,pos) result(integer) argument with pos bit set to one ieor(integer,integer) result(integer) bit by bit logical exclusive or ior(integer,integer) result(integer) bit by bit logical or ishft(integer,shift) result(integer) shift bits in argument by shift ishftc(integer, shift) result(integer) shift circular bits in argument logical(logical [,kind=]) convert to logical matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical merge(true_source,false_source,mask) result(source_type) choose by mask mvbits(from,frompos,len,to,topos) result(integer) move bits not(integer) result(integer) bit by bit logical complement transfer(source,mold [,size]) result(mold_type) same bits, new type Intrinsic Functions (Character or string) achar(integer) result(character) integer to character adjustl(character) result(character) left adjust, blanks go to back adjustr(character) result(character) right adjust, blanks to front char(integer [,kind=]) result(character) integer to character [of kind] iachar(character) result(integer) position of character in ASCII sequence ichar(character) result(integer) pos in collating sequence of character index(string,substring [,back=]) result(integer) pos of substring len(character) result(integer) number of characters that can be in argument len_trim(character) result(integer) length without trailing blanks lge(string_a,string_b) result(logical) string_a &ge; string_b lgt(string_a,string_b) result(logical) string_a > string_b lle(string_a,string_b) result(logical) string_a &le; string_b llt(string_a,string_b) result(logical) string_a < string_b repeat(string,ncopies) result(string) concatenate n copies of string scan(string,set [,back]) result(integer) position of first of set in string trim(string) result(string) trailing blanks are removed verify(string,set [,back]) result(integer) pos in string not in set Fortran 95 New Features The statement FORALL as an alternative to the DO-statement Partial nesting of FORALL and WHERE statements Masked ELSEWHERE Pure procedures Elemental procedures Pure procedures in specification expressions Revised MINLOC and MAXLOC Extensions to CEILING and FLOOR with the KIND keyword argument Pointer initialization Default initialization of derived type objects Increased compatibility with IEEE arithmetic A CPU_TIME intrinsic subroutine A function NULL to nullify a pointer Automatic deallocation of allocatable arrays at exit of scoping unit Comments in NAMELIST at input Minimal field at input Complete version of END INTERFACE Deleted Features real and double precision DO loop index variables branching to END IF from an outer block PAUSE statements ASSIGN statements and assigned GO TO statements and the use of an assigned integer as a FORMAT specification Hollerith editing in FORMAT See http://www.nsc.liu.se/~boein/f77to90/f95.html#17.5 References http://www.fortran.com/fortran/ Pointer to everything Fortran http://meteora.ucsd.edu/~pierce/fxdr_home_page.html Subroutines to do unformatted I/O across platforms, provided by David Pierce at UCSD http://www.nsc.liu.se/~boein/f77to90/a5.html A good reference for intrinsic functions https://wg5-fortran.org/N1551-N1600/N1579.pdf New Features of Fortran 2003 https://wg5-fortran.org/N1701-N1750/N1729.pdf New Features of Fortran 2008 http://www.nsc.liu.se/~boein/f77to90/ Fortran 90 for the Fortran 77 Programmer Fortran 90 Handbook Complete ANSI/ISO Reference . Jeanne Adams, Walt Brainerd, Jeanne Martin, Brian Smith, Jerrold Wagener Fortran 90 Programming . T. Ellis, Ivor Philips, Thomas Lahey https://github.com/llvm/llvm-project/blob/master/flang/docs/FortranForCProgrammers.md FFT stuff Fortran 95 and beyond","title":"Fortran"},{"location":"Documentation/languages/fortran/f90/#advanced-fortran-90","text":"This document is derived from an HTML page written at the San Diego Supercomper Center many years ago. Its purpose is to Introduce Fortran 90 concepts to Fortran 77 programers. It does this by presenting an example program and introducing concepts as various routines of the program are presented. The original web page has been used over the years and has been translated into several languages.","title":"Advanced Fortran 90"},{"location":"Documentation/languages/fortran/f90/#format-for-our-presentation","text":"We will \"develop\" an application Incorporate f90 features Show source code Explain what and why as we do it Application is a genetic algorithm Easy to understand and program Offers rich opportunities for enhancement We also provide an summary of F90 syntax, key words, operators, constants, and functions","title":"Format for our presentation"},{"location":"Documentation/languages/fortran/f90/#what-was-in-mind-of-the-language-writers-what-were-they-thinking","text":"Enable portable codes Same precision Include many common extensions More reliable programs Getting away from underlying hardware Move toward parallel programming Run old programs Ease of programming Writing Maintaining Understanding Reading Recover C and C++ users","title":"What was in mind of the language writers? What were they thinking?"},{"location":"Documentation/languages/fortran/f90/#why-fortran","text":"Famous Quote: \"I don't know what the technical characteristics of the standard language for scientific and engineering computation in the year 2000 will be... but I know it will be called Fortran.\" John Backus.* ### Note: He claimed that he never said this. Language of choice for Scientific programming Large installed user base. Fortran 90 has most of the features of C . . . and then some The compilers produce better programs","title":"Why Fortran?"},{"location":"Documentation/languages/fortran/f90/#justification-of-topics","text":"Enhance performance Enhance portability Enhance reliability Enhance maintainability","title":"Justification of topics"},{"location":"Documentation/languages/fortran/f90/#classification-of-topics","text":"New useful features Old tricks Power features Overview of F90","title":"Classification of topics"},{"location":"Documentation/languages/fortran/f90/#listing_of_topics_covered","text":"Listing of topics covered What is a Genetic Algorithm Simple algorithm for a GA Our example problem Start of real Fortran 90 discussion Comparing a FORTRAN 77 routine to a Fortran 90 routine Obsolescent features New source Form and related things New data declaration method Kind facility Modules Module functions and subroutines Allocatable arrays (the basics) Passing arrays to subroutines Interface for passing arrays Optional arguments and intent Derived data types Using defined types User defined operators Recursive functions introduction Fortran 90 recursive functions Pointers Function and subroutine overloading Fortran Minval and Minloc routines Pointer assignment More pointer usage, association and nullify Pointer usage to reference an array Data assignment with structures Using the user defined operator Passing arrays with a given arbitrary lower bounds Using pointers to access sections of arrays Allocating an array inside a subroutine Our fitness function Linked lists Linked list usage Our map representation Date and time functions Non advancing and character IO Internal IO Inquire function Namelist Vector valued functions Complete source for recent discussions Some array specific intrinsic functions The rest of our GA Compiler Information Summary Overview of F90 Fortran 95 References","title":"Listing_of_topics_covered"},{"location":"Documentation/languages/fortran/f90/#what-is-a-genetic-algorithm","text":"A \"suboptimization\" system Find good, but maybe not optimal, solutions to difficult problems Often used on NP-Hard or combinatorial optimization problems Requirements Solution(s) to the problem represented as a string A fitness function Takes as input the solution string Output the desirability of the solution A method of combining solution strings to generate new solutions Find solutions to problems by Darwinian evolution Potential solutions ar though of as living entities in a population The strings are the genetic codes for the individuals Fittest individuals are allowed to survive to reproduce","title":"What is a Genetic Algorithm"},{"location":"Documentation/languages/fortran/f90/#simple-algorithm-for-a-ga","text":"Generate a initial population, a collection of strings do for some time evaluate each individual (string) of the population using the fitness function sort the population with fittest coming to the top allow the fittest individuals to \"sexually\" reproduce replacing the old population allow for mutation end do","title":"Simple algorithm for a GA"},{"location":"Documentation/languages/fortran/f90/#our-example-problem","text":"Instance:Given a map of the N states or countries and a fixed number of colors Find a coloring of the map, if it exists, such that no two states that share a boarder have the same color Notes - In general, for a fixed number of colors and an arbitrary map the only known way to find if there is a valid coloring is a brute force search with the number of combinations = (NUMBER_OF_COLORS)**(NSTATES) The strings of our population are integer vectors represent the coloring Our fitness function returns the number of boarder violations The GA searches for a mapping with few, hopefully 0 violations This problem is related to several important NP_HARD problems in computer science Processor scheduling Communication and grid allocation for parallel computing Routing","title":"Our example problem"},{"location":"Documentation/languages/fortran/f90/#start-of-real-fortran-90-discussion","text":"","title":"Start of real Fortran 90 discussion"},{"location":"Documentation/languages/fortran/f90/#comparing-a-fortran-77-routine-to-a-fortran-90-routine","text":"The routine is one of the random number generators from: Numerical Recipes, The Art of Scientific Computing. Press, Teukolsky, Vetterling and Flannery. Cambridge University Press 1986. Changes correct bugs increase functionality aid portability","title":"Comparing a FORTRAN 77 routine to a Fortran 90 routine"},{"location":"Documentation/languages/fortran/f90/#original","text":"function ran1 ( idum ) real ran1 integer idum real r ( 97 ) parameter ( m1 = 259200 , ia1 = 7141 , ic1 = 54773 ) parameter ( m2 = 134456 , ia2 = 8121 , ic2 = 28411 ) parameter ( m3 = 243000 , ia3 = 4561 , ic3 = 51349 ) integer j integer iff , ix1 , ix2 , ix3 data iff / 0 / if ( idum . lt . 0. or . iff . eq . 0 ) then rm1 = 1.0 / m1 rm2 = 1.0 / m2 iff = 1 ix1 = mod ( ic1 - idum , m1 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ix1 , m2 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix3 = mod ( ix1 , m3 ) do 11 j = 1 , 97 ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) r ( j ) = ( real ( ix1 ) + real ( ix2 ) * rm2 ) * rm1 11 continue idum = 1 endif ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) ix3 = mod ( ia3 * ix3 + ic3 , m3 ) j = 1 + ( 97 * ix3 ) / m3 if ( j . gt . 9 7. or . j . lt . 1 ) then write ( * , * ) ' error in ran1 j=' , j stop endif ran1 = r ( j ) r ( j ) = ( real ( ix1 ) + real ( ix2 ) * rm2 ) * rm1 return end","title":"Original"},{"location":"Documentation/languages/fortran/f90/#fortran-90","text":"module ran_mod contains function ran1 ( idum ) use numz implicit none !note after use statement real ( b8 ) ran1 integer , intent ( inout ), optional :: idum real ( b8 ) r ( 97 ), rm1 , rm2 integer , parameter :: m1 = 259200 , ia1 = 7141 , ic1 = 54773 integer , parameter :: m2 = 134456 , ia2 = 8121 , ic2 = 28411 integer , parameter :: m3 = 243000 , ia3 = 4561 , ic3 = 51349 integer j integer iff , ix1 , ix2 , ix3 data iff / 0 / save ! corrects a bug in the original routine if ( present ( idum )) then if ( idum . lt . 0. or . iff . eq . 0 ) then rm1 = 1.0_b8 m1 rm2 = 1.0_b8 m2 iff = 1 ix1 = mod ( ic1 - idum , m1 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ix1 , m2 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix3 = mod ( ix1 , m3 ) do j = 1 , 97 ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) r ( j ) = ( real ( ix1 , b8 ) + real ( ix2 , b8 ) * rm2 ) * rm1 enddo idum = 1 endif endif ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) ix3 = mod ( ia3 * ix3 + ic3 , m3 ) j = 1 + ( 97 * ix3 ) / m3 if ( j . gt . 9 7. or . j . lt . 1 ) then write ( * , * ) ' error in ran1 j=' , j stop endif ran1 = r ( j ) r ( j ) = ( real ( ix1 , b8 ) + real ( ix2 , b8 ) * rm2 ) * rm1 return end function ran1","title":"Fortran 90"},{"location":"Documentation/languages/fortran/f90/#comments","text":"Modules are a way of encapsulating functions an data. More below. The use numz line is similar to an include file. In this case it defines our real data type. real (b8) is a new way to specify percision for data types in a portable way. integer , intent(inout), optional :: idum we are saying idum is an optional input parameter integer , parameter :: just a different syntax The save statement is needed for program correctness present(idum) is a function to determine if ran1 was called with the optional parameter","title":"Comments"},{"location":"Documentation/languages/fortran/f90/#obsolescent-features","text":"The following are available in Fortran 90. On the other hand, the concept of \"obsolescence\" is introduced. This means that some constructs may be removed in the future. - Arithmetic IF-statement - Control variables in a DO-loop which are floating point or double-precision floating-point - Terminating several DO-loops on the same statement - Terminating the DO-loop in some other way than with CONTINUE or END DO - Alternate return - Jump to END IF from an outer block - PAUSE - ASSIGN and assigned GOTO and assigned FORMAT , that is the whole \"statement number variable\" concept. - Hollerith editing in FORMAT.","title":"Obsolescent features"},{"location":"Documentation/languages/fortran/f90/#new-source-form-and-related-things","text":"","title":"New source form and related things"},{"location":"Documentation/languages/fortran/f90/#summary","text":"! now indicates the start of a comment & indicates the next line is a continuation Lines can be longer than 72 characters Statements can start in any column Use ; to put multiple statements on one line New forms for the do loop Many functions are generic 32 character names Many new array assignment techniques","title":"Summary"},{"location":"Documentation/languages/fortran/f90/#features","text":"Flexibility can aid in program readability Readability decreases errors Got ya! Can no longer use C to start a comment Character in column 5 no longer is continue Tab is not a valid character (may produce a warning) Characters past 72 now count program darwin real a ( 10 ), b ( 10 ), c ( 10 ), d ( 10 ), e ( 10 ), x , y integer odd ( 5 ), even ( 5 ) ! this line is continued by using \"&\" write ( * , * ) \"starting \" ,& \"darwin\" ! this line in a continued from above ! multiple statement per line --rarely a good idea x = 1 ; y = 2 ; write ( * , * ) x , y do i = 1 , 10 ! statement lable is not required for do e ( i ) = i enddo odd = ( / 1 , 3 , 5 , 7 , 9 / ) ! array assignment even = ( / 2 , 4 , 6 , 8 , 10 / ) ! array assignment a = 1 ! array assignment, every element of a = 1 b = 2 c = a + b + e ! element by element assignment c ( odd ) = c ( even ) - 1 ! can use arrays of indices on both sides d = sin ( c ) ! element by element application of intrinsics write ( * , * ) d write ( * , * ) abs ( d ) ! many intrinsic functions are generic a_do_loop : do i = 1 , 10 write ( * , * ) i , c ( i ), d ( i ) enddo a_do_loop do if ( c ( 10 ) . lt . 0.0 ) exit c ( 10 ) = c ( 10 ) - 1 enddo write ( * , * ) c ( 10 ) do while ( c ( 9 ) . gt . 0 ) c ( 9 ) = c ( 9 ) - 1 enddo write ( * , * ) c ( 9 ) end program","title":"Features"},{"location":"Documentation/languages/fortran/f90/#new-data-declaration-method","text":"Motivation Variables can now have attributes such as - Parameter - Save - Dimension Attributes are assigned in the variable declaration statement One variable can have several attributes Requires Fortran 90 to have a new statement form integer , parameter :: in2 = 14 real , parameter :: pi = 3.141592653589793239 real , save , dimension ( 10 ) :: cpu_times , wall_times !**** the old way of doing the same ****! !**** real cpu_times(10),wall_times(10) ****! !**** save cpu_times, wall_times ****! - Other Attributes - allocatable - public - private - target - pointer - intent - optional","title":"New data declaration method"},{"location":"Documentation/languages/fortran/f90/#kind-facility","text":"Motivation Assume we have a program that we want to run on two different machines We want the same representation of reals on both machines (same number of significant digits) Problem: different machines have different representations for reals","title":"Kind facility"},{"location":"Documentation/languages/fortran/f90/#digits-of-precision-for-some-old-machines-and-data-type","text":"Machine Real Double Precision IBM (SP) 6 15 Cray (T90) 15 33 Cray (T3E) 15 15","title":"Digits of precision for some (old) machines and data type"},{"location":"Documentation/languages/fortran/f90/#or","text":"We may want to run with at least 6 digits today and at least 14 digits tomorrow Use the Select_Real_Kind(P) function to create a data type with P digits of precision program darwin ! e has at least 4 significant digits real ( selected_real_kind ( 4 )) e ! b8 will be used to define reals with 14 digits integer , parameter :: b8 = selected_real_kind ( 14 ) real ( b8 ), parameter :: pi = 3.141592653589793239_b8 ! note usage of _b8 ! with a constant ! to force precision e = 2.71828182845904523536 write ( * , * ) \"starting \" ,& ! this line is continued by using \"&\" \"darwin\" ! this line in a continued from above write ( * , * ) \"pi has \" , precision ( pi ), \" digits precision \" , pi write ( * , * ) \"e has \" , precision ( e ), \" digits precision \" , e end program","title":"* or *"},{"location":"Documentation/languages/fortran/f90/#example-output","text":"sp001 % darwin starting darwin pi has 15 digits precision 3.14159265358979312 e has 6 digits precision 2.718281746 sp001 % Can convert to/from given precision for all variables created using \"b8\" by changing definition of \"b8\" Use the Select_Real_Kind(P,R) function to create a data type with P digits of precision and exponent range of R","title":"Example output"},{"location":"Documentation/languages/fortran/f90/#modules","text":"Motivation: Common block usage is prone to error Provide most of capability of common blocks but safer Provide capabilities beyond common blocks Modules can contain: Data definitions Data to be shared much like using a labeled common Functions and subroutines Interfaces (more on this later) You \"include\" a module with a \"use\" statement module numz integer , parameter :: b8 = selected_real_kind ( 14 ) real ( b8 ), parameter :: pi = 3.141592653589793239_b8 integergene_size end module program darwin use numz implicit none ! now part of the standard, put it after the use statements write ( * , * ) \"pi has \" , precision ( pi ), \" digits precision \" , pi call set_size () write ( * , * ) \"gene_size=\" , gene_size end program subroutine set_size use numz gene_size = 10 end subroutine","title":"Modules"},{"location":"Documentation/languages/fortran/f90/#an-example-run","text":"pi has 15 digits precision 3.14159265358979312 gene_size=10","title":"An example run"},{"location":"Documentation/languages/fortran/f90/#module-functions-and-subroutines","text":"Motivation: Encapsulate related functions and subroutines Can \"USE\" these functions in a program or subroutine Can be provided as a library Only routines that contain the use statement can see the routines Example is a random number package: module ran_mod ! module contains three functions ! ran1 returns a uniform random number between 0-1 ! spread returns random number between min - max ! normal returns a normal distribution contains function ran1 () !returns random number between 0 - 1 use numz implicit none real ( b8 ) ran1 , x call random_number ( x ) ! built in fortran 90 random number function ran1 = x end function ran1 function spread ( min , max ) !returns random # between min/max use numz implicit none real ( b8 ) spread real ( b8 ) min , max spread = ( max - min ) * ran1 () + min end function spread function normal ( mean , sigma ) !returns a normal distribution use numz implicit none real ( b8 ) normal , tmp real ( b8 ) mean , sigma integer flag real ( b8 ) fac , gsave , rsq , r1 , r2 save flag , gsave data flag / 0 / if ( flag . eq . 0 ) then rsq = 2.0_b8 do while ( rsq . ge . 1.0_b8 . or . rsq . eq . 0.0_b8 ) ! new from for do r1 = 2.0_b8 * ran1 () - 1.0_b8 r2 = 2.0_b8 * ran1 () - 1.0_b8 rsq = r1 * r1 + r2 * r2 enddo fac = sqrt ( - 2.0_b8 * log ( rsq ) / rsq ) gsave = r1 * fac tmp = r2 * fac flag = 1 else tmp = gsave flag = 0 endif normal = tmp * sigma + mean return end function normal end module ran_mod Exersize 1: Write a program that returns 10 uniform random numbers.","title":"Module functions and subroutines"},{"location":"Documentation/languages/fortran/f90/#allocatable-arrays-the-basics","text":"Motivation: At compile time we may not know the size an array needs to be We may want to change problem size without recompiling Allocatable arrays allow us to set the size at run time We set the size of the array using the allocate statement We may want to change the lower bound for an array A simple example: module numz integer , parameter :: b8 = selected_real_kind ( 14 ) integer gene_size , num_genes integer , allocatable :: a_gene (:), many_genes (:,:) end module program darwin use numz implicit none integer ierr call set_size () allocate ( a_gene ( gene_size ), stat = ierr ) !stat= allows for an error code return if ( ierr /= 0 ) write ( * , * ) \"allocation error\" ! /= is .ne. allocate ( many_genes ( gene_size , num_genes ), stat = ierr ) !2d array if ( ierr /= 0 ) write ( * , * ) \"allocation error\" write ( * , * ) lbound ( a_gene ), ubound ( a_gene ) ! get lower and upper bound ! for the array write ( * , * ) size ( many_genes ), size ( many_genes , 1 ) !get total size and size !along 1st dimension deallocate ( many_genes ) ! free the space for the array and matrix deallocate ( a_gene ) allocate ( a_gene ( 0 : gene_size )) ! now allocate starting at 0 instead of 1 write ( * , * ) allocated ( many_genes ), allocated ( a_gene ) ! shows if allocated write ( * , * ) lbound ( a_gene ), ubound ( a_gene ) end program subroutine set_size use numz write ( * , * ) 'enter gene size:' read ( * , * ) gene_size write ( * , * ) 'enter number of genes:' read ( * , * ) num_genes end subroutine set_size","title":"Allocatable arrays (the basics)"},{"location":"Documentation/languages/fortran/f90/#example-run","text":"enter gene size: 10 enter number of genes: 20 1 10 200 10 F T 0 10","title":"Example run"},{"location":"Documentation/languages/fortran/f90/#passing-arrays-to-subroutines","text":"There are several ways to specify arrays for subroutines Explicit shape integer, dimension(8,8)::an_explicit_shape_array Assumed size integer, dimension(i,*)::an_assumed_size_array Assumed Shape integer, dimension(:,:)::an_assumed_shape_array","title":"Passing arrays to subroutines"},{"location":"Documentation/languages/fortran/f90/#example","text":"subroutine arrays ( an_explicit_shape_array ,& i ,& !note we pass all bounds except the last an_assumed_size_array ,& an_assumed_shape_array ) ! Explicit shape integer , dimension ( 8 , 8 ) :: an_explicit_shape_array ! Assumed size integer , dimension ( i , * ) :: an_assumed_size_array ! Assumed Shape integer , dimension (:,:) :: an_assumed_shape_array write ( * , * ) sum ( an_explicit_shape_array ) write ( * , * ) lbound ( an_assumed_size_array ) ! why does sum not work here? write ( * , * ) sum ( an_assumed_shape_array ) end subroutine","title":"Example"},{"location":"Documentation/languages/fortran/f90/#interface-for-passing-arrays","text":"!!!!Warning!!!! When passing assumed shape arrays as arguments you must provide an interface Similar to C prototypes but much more versatile The interface is a copy of the invocation line and the argument definitions Modules are a good place for interfaces If a procedure is part of a \"contains\" section in a module an interface is not required !!!!Warning!!!! The compiler may not tell you that you need an interface module numz integer , parameter :: b8 = selected_real_kind ( 14 ) integer , allocatable :: a_gene (:), many_genes (:,:) end module module face interface fitness function fitness ( vector ) use numz implicit none real ( b8 ) fitness integer , dimension (:) :: vector end function fitness end interface end module program darwin use numz use face implicit none integer i integer vect ( 10 ) ! just a regular array allocate ( a_gene ( 10 )); allocate ( many_genes ( 3 , 10 )) a_gene = 1 !sets every element of a_gene to 1 write ( * , * ) fitness ( a_gene ) vect = 8 write ( * , * ) fitness ( vect ) ! also works with regular arrays many_genes = 3 !sets every element to 3 many_genes ( 1 ,:) = a_gene !sets column 1 to a_gene many_genes ( 2 ,:) = 2 * many_genes ( 1 ,:) do i = 1 , 3 write ( * , * ) fitness ( many_genes ( i ,:)) enddo write ( * , * ) fitness ( many_genes (:, 1 )) !go along other dimension !!!!write(*,*)fitness(many_genes)!!!!does not work end program function fitness ( vector ) use numz implicit none real ( b8 ) fitness integer , dimension (:) :: vector ! must match interface fitness = sum ( vector ) end function Exersize 2: Run this program using the \"does not work line\". Why? Using intrinsic functions make it work? Exersize 3: Prove that f90 does not \"pass by address\".","title":"Interface for passing arrays"},{"location":"Documentation/languages/fortran/f90/#optional-arguments-and-intent","text":"Motivation: We may have a function or subroutine that we may not want to always pass all arguments Initialization Two examples Seeding the intrinsic random number generator requires keyword arguments To define an optional argument in our own function we use the optional attribute integer :: my_seed","title":"Optional arguments and intent"},{"location":"Documentation/languages/fortran/f90/#becomes","text":"integer, optional :: my_seed Used like this: ! ran1 returns a uniform random number between 0-1 ! the seed is optional and used to reset the generator contains function ran1 ( my_seed ) use numz implicit none real ( b8 ) ran1 , r integer , optional , intent ( in ) :: my_seed ! optional argument not changed in the routine integer , allocatable :: seed (:) integer the_size , j if ( present ( my_seed )) then ! use the seed if present call random_seed ( size = the_size ) ! how big is the intrisic seed? allocate ( seed ( the_size )) ! allocate space for seed do j = 1 , the_size ! create the seed seed ( j ) = abs ( my_seed ) + ( j - 1 ) ! abs is generic enddo call random_seed ( put = seed ) ! assign the seed deallocate ( seed ) ! deallocate space endif call random_number ( r ) ran1 = r end function ran1 end module program darwin use numz use ran_mod ! interface required if we have ! optional or intent arguments real ( b8 ) x , y x = ran1 ( my_seed = 12345 ) ! we can specify the name of the argument y = ran1 () write ( * , * ) x , y x = ran1 ( 12345 ) ! with only one optional argument we don't need to y = ran1 () write ( * , * ) x , y end program Intent is a hint to the compiler to enable optimization intent(in) We will not change this value in our subroutine intent(out) We will define this value in our routine intent(inout) The normal situation","title":"becomes"},{"location":"Documentation/languages/fortran/f90/#derived-data-types","text":"Motivation: Derived data types can be used to group different types of data together (integers, reals, character, complex) Can not be done in F77 although people have \"faked\" it Example In our GA we define a collection of genes as a 2d array We call the fitness function for every member of the collection We want to sort the collection of genes based on result of fitness function Define a data type that holds the fitness value and an index into the 2d array Create an array of this data type, 1 for each member of the collection Call fitness function with the result being placed into the new data type along with a pointer into the array Again modules are a good place for data type definitions module galapagos use numz type thefit !the name of the type sequence ! sequence forces the data elements ! to be next to each other in memory ! where might this be useful? real ( b8 ) val ! our result from the fitness function integer index ! the index into our collection of genes end type thefit end module","title":"Derived data types"},{"location":"Documentation/languages/fortran/f90/#using-defined-types","text":"Use the % to reference various components of the derived data type program darwin use numz use galapagos ! the module that contains the type definition use face ! contains various interfaces implicit none ! define an allocatable array of the data type ! than contains an index and a real value type ( thefit ), allocatable , target :: results (:) ! create a single instance of the data type type ( thefit ) best integer , allocatable :: genes (:,:) ! our genes for the genetic algorithm integer j integer num_genes , gene_size num_genes = 10 gene_size = 10 allocate ( results ( num_genes )) ! allocate the data type ! to hold fitness and index allocate ( genes ( num_genes , gene_size )) ! allocate our collection of genes call init_genes ( genes ) ! starting data write ( * , '(\"input\")' ) ! we can put format in write statement do j = 1 , num_genes results ( j )% index = j results ( j )% val = fitness ( genes ( j ,:)) ! just a dummy routine for now write ( * , \"(f10.8,i4)\" ) results ( j )% val , results ( j )% index enddo end program","title":"Using defined types"},{"location":"Documentation/languages/fortran/f90/#user-defined-operators","text":"Motivation With derived data types we may want (need) to define operations (Assignment is predefined) Example: .lt. .gt. == not defined for our data types - We want to find the minimum of our fitness values so we need < operator - In our sort routine we want to do <, >, == - In C++ terms the operators are overloaded We are free to define new operators Two step process to define operators Define a special interface Define the function that performs the operation module sort_mod !defining the interfaces interface operator (. lt .) ! overloads standard .lt. module procedure theless ! the function that does it end interface interface operator (. gt .) ! overloads standard .gt. module procedure thegreat ! the function that does it end interface interface operator (. ge .) ! overloads standard .ge. module procedure thetest ! the function that does it end interface interface operator (. converged .) ! new operator module procedure index_test ! the function that does it end interface contains ! our module will contain ! the required functions function theless ( a , b ) ! overloads .lt. for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical theless ! what we return if ( a % val . lt . b % val ) then ! this is where we do the test theless = . true . else theless = . false . endif return end function theless function thegreat ( a , b ) ! overloads .gt. for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical thegreat if ( a % val . gt . b % val ) then thegreat = . true . else thegreat = . false . endif return end function thegreat function thetest ( a , b ) ! overloads .gt.= for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical thetest if ( a % val >= b % val ) then thetest = . true . else thetest = . false . endif return end function thetest function index_test ( a , b ) ! defines a new operation for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical index_test if ( a % index . gt . b % index ) then ! check the index value for a difference index_test = . true . else index_test = . false . endif return end function index_test","title":"User defined operators"},{"location":"Documentation/languages/fortran/f90/#recursive-functions-introduction","text":"Notes Recursive function is one that calls itself Anything that can be done with a do loop can be done using a recursive function Motivation Sometimes it is easier to think recursively Divide an conquer algorithms are recursive by nature - Fast FFTs - Searching - Sorting","title":"Recursive functions introduction"},{"location":"Documentation/languages/fortran/f90/#algorithm-of-searching-for-minimum-of-an-array","text":"function findmin ( array ) is size of array 1 ? min in the array is first element else find minimum in left half of array using findmin function find minimum in right half of array using findmin function global minimum is min of left and right half end function","title":"Algorithm of searching for minimum of an array"},{"location":"Documentation/languages/fortran/f90/#fortran-90-recursive-functions","text":"Recursive functions should have an interface The result and recursive keywords are required as part of the function definition Example is a function finds the minimum value for an array recursive function realmin ( ain ) result ( themin ) ! recursive and result are required for recursive functions use numz implicit none real ( b8 ) themin , t1 , t2 integer n , right real ( b8 ) , dimension (:) :: ain n = size ( ain ) if ( n == 1 ) then themin = ain ( 1 ) ! if the size is 1 return value return else right = n / 2 t1 = realmin ( ain ( 1 : right )) ! find min in left half t2 = realmin ( ain ( right + 1 : n )) ! find min in right half themin = min ( t1 , t2 ) ! find min of the two sides endif end function Example 2 is the same except the input data is our derived data type !this routine works with the data structure thefit not reals recursive function typemin ( ain ) result ( themin ) use numz use sort_mod use galapagos implicit none real ( b8 ) themin , t1 , t2 integer n , right type ( thefit ) , dimension (:) :: ain ! this line is different n = size ( ain ) if ( n == 1 ) then themin = ain ( 1 )% val ! this line is different return else right = n / 2 t1 = typemin ( ain ( 1 : right )) t2 = typemin ( ain ( right + 1 : n )) themin = min ( t1 , t2 ) endif end function","title":"Fortran 90 recursive functions"},{"location":"Documentation/languages/fortran/f90/#pointers","text":"Motivation Can increase performance Can improve readability Required for some derived data types (linked lists and trees) Useful for allocating \"arrays\" within subroutines Useful for referencing sections of arrays Notes Pointers can be thought of as an alias to another variable In some cases can be used in place of an array To assign a pointer use => instead of just = Unlike C and C++, pointer arithmetic is not allowed First pointer example Similar to the last findmin routine Return a pointer to the minimum recursive function pntmin ( ain ) result ( themin ) ! return a pointer use numz use galapagos use sort_mod ! contains the .lt. operator for thefit type implicit none type ( thefit ), pointer :: themin , t1 , t2 integer n , right type ( thefit ) , dimension (:), target :: ain n = size ( ain ) if ( n == 1 ) then themin => ain ( 1 ) !this is how we do pointer assignment return else right = n / 2 t1 => pntmin ( ain ( 1 : right )) t2 => pntmin ( ain ( right + 1 : n )) if ( t1 . lt . t2 ) then ; themin => t1 ; else ; themin => t2 ; endif endif end function Exercise 4: Carefully write a recursive N! program.","title":"Pointers"},{"location":"Documentation/languages/fortran/f90/#function-and-subroutine-overloading","text":"Motivation Allows us to call functions or subroutine with the same name with different argument types Increases readability Notes: Similar in concept to operator overloading Requires an interface Syntax for subroutines is same as for functions Many intrinsic functions have this capability - abs (reals,complex,integer) - sin,cos,tan,exp(reals, complex) - array functions(reals, complex,integer) Example - Recall we had two functions that did the same thing but with different argument types recursive function realmin(ain) result (themin) real(b8) ,dimension(:) :: ain recursive function typemin(ain) result (themin) type (thefit) ,dimension(:) :: ain - We can define a generic interface for these two functions and call them using the same name ! note we have two functions within the same interface ! this is how we indicate function overloading ! both functions are called \"findmin\" in the main program interface findmin ! the first is called with an array of reals as input recursive function realmin ( ain ) result ( themin ) use numz real ( b8 ) themin real ( b8 ) , dimension (:) :: ain end function ! the second is called with a array of data structures as input recursive function typemin ( ain ) result ( themin ) use numz use galapagos real ( b8 ) themin type ( thefit ) , dimension (:) :: ain end function end interface","title":"Function and subroutine overloading"},{"location":"Documentation/languages/fortran/f90/#example-usage","text":"program darwin use numz use ran_mod use galapagos ! the module that contains the type definition use face ! contains various interfaces use sort_mod ! more about this later it ! contains our sorting routine ! and a few other tricks implicit none ! create an allocatable array of the data type ! than contains an index and a real value type ( thefit ), allocatable , target :: results (:) ! create a single instance of the data type type ( thefit ) best ! pointers to our type type ( thefit ) , pointer :: worst , tmp integer , allocatable :: genes (:,:) ! our genes for the ga integer j integer num_genes , gene_size real ( b8 ) x real ( b8 ), allocatable :: z (:) real ( b8 ), pointer :: xyz (:) ! we'll talk about this next num_genes = 10 gene_size = 10 allocate ( results ( num_genes )) ! allocate the data type to allocate ( genes ( num_genes , gene_size )) ! hold our collection of genes call init_genes ( genes ) ! starting data write ( * , '(\"input\")' ) do j = 1 , num_genes results ( j )% index = j results ( j )% val = fitness ( genes ( j ,:)) ! just a dummy routine write ( * , \"(f10.8,i4)\" ) results ( j )% val , results ( j )% index enddo allocate ( z ( size ( results ))) z = results (:)% val ! copy our results to a real array ! use a recursive subroutine operating on the real array write ( * , * ) \"the lowest fitness: \" , findmin ( z ) ! use a recursive subroutine operating on the data structure write ( * , * ) \"the lowest fitness: \" , findmin ( results ) end program","title":"Example usage"},{"location":"Documentation/languages/fortran/f90/#fortran-minval-and-minloc-routines","text":"Fortran has routines for finding minimum and maximum values in arrays and the locations minval maxval minloc (returns an array) maxloc (returns an array) ! we show two other methods of getting the minimum fitness ! use the built in f90 routines on a real array write ( * , * ) \"the lowest fitness: \" , minval ( z ), minloc ( z )","title":"Fortran Minval and Minloc routines"},{"location":"Documentation/languages/fortran/f90/#pointer-assignment","text":"This is how we use the pointer function defined above worst is a pointer to our data type note the use of => ! use a recursive subroutine operating on the data ! structure and returning a pointer to the result worst=>pntmin(results) ! note pointer assignment ! what will this line write? write(*,*)\"the lowest fitness: \",worst","title":"Pointer assignment"},{"location":"Documentation/languages/fortran/f90/#more-pointer-usage-association-and-nullify","text":"Motivation Need to find if pointers point to anything Need to find if two pointers point to the same thing Need to deallocate and nullify when they are no longer used Usage We can use associated() to tell if a pointer has been set We can use associated() to compare pointers We use nullify to zero a pointer ! This code will print \"true\" when we find a match, ! that is the pointers point to the same object do j = 1 , num_genes tmp => results ( j ) write ( * , \"(f10.8,i4,l3)\" ) results ( j )% val , & results ( j )% index , & associated ( tmp , worst ) enddo nullify ( tmp ) Notes: If a pointer is nullified the object to which it points is not deallocated. In general, pointers as well as allocatable arrays become undefined on leaving a subroutine This can cause a memory leak","title":"More pointer usage, association and nullify"},{"location":"Documentation/languages/fortran/f90/#pointer-usage-to-reference-an-array-without-copying","text":"Motivation Our sort routine calls a recursive sorting routine It is messy and inefficient to pass the array to the recursive routine Solution We define a \"global\" pointer in a module We point the pointer to our input array module Merge_mod_types use galapagos type ( thefit ), allocatable :: work (:) ! a \"global\" work array type ( thefit ), pointer :: a_pntr (:) ! this will be the pointer to our input array end module Merge_mod_types subroutine Sort ( ain , n ) use Merge_mod_types implicit none integer n type ( thefit ), target :: ain ( n ) allocate ( work ( n )) nullify ( a_pntr ) a_pntr => ain ! we assign the pointer to our array ! in RecMergeSort we reference it just like an array call RecMergeSort ( 1 , n ) ! very similar to the findmin functions deallocate ( work ) return end subroutine Sort In our main program sort is called like this: ! our sort routine is also recursive but ! also shows a new usage for pointers call sort ( results , num_genes ) do j = 1 , num_genes write ( * , \"(f10.8,i4)\" ) results ( j )% val , & results ( j )% index enddo","title":"Pointer usage to reference an array without copying"},{"location":"Documentation/languages/fortran/f90/#data-assignment-with-structures","text":"! we can copy a whole structure ! with a single assignment best = results ( 1 ) write ( * , * ) \"best result \" , best","title":"Data assignment with structures"},{"location":"Documentation/languages/fortran/f90/#using-the-user-defined-operator","text":"! using the user defined operator to see if best is worst ! recall that the operator .converged. checks to see if %index matches worst => pntmin ( results ) write ( * , * ) \"worst result \" , worst write ( * , * ) \"converged=\" ,( best . converged . worst )","title":"Using the user defined operator"},{"location":"Documentation/languages/fortran/f90/#passing-arrays-with-a-given-arbitrary-lower-bounds","text":"Motivation Default lower bound within a subroutine is 1 May want to use a different lower bound if ( allocated ( z )) deallocate ( z ) allocate ( z ( - 10 : 10 )) ! a 21 element array do j =- 10 , 10 z ( j ) = j enddo ! pass z and its lower bound ! in this routine we give the array a specific lower ! bound and show how to use a pointer to reference ! different parts of an array using different indices call boink1 ( z , lbound ( z , 1 )) ! why not just lbound(z) instead of lbound(z,1)? ! lbound(z) returns a rank 1 array subroutine boink1 ( a , n ) use numz implicit none integer , intent ( in ) :: n real ( b8 ), dimension ( n :) :: a ! this is how we set lower bounds in a subroutine write ( * , * ) lbound ( a ), ubound ( a ) end subroutine","title":"Passing arrays with a given arbitrary lower bounds"},{"location":"Documentation/languages/fortran/f90/#warning-because-we-are-using-an-assumed-shape-array-we-need-an-interface","text":"","title":"Warning:  because we are using an assumed shape array we need an interface"},{"location":"Documentation/languages/fortran/f90/#using-pointers-to-access-sections-of-arrays","text":"Motivation Can increase efficiency Can increase readability call boink2 ( z , lbound ( z , 1 )) subroutine boink2 ( a , n ) use numz implicit none integer , intent ( in ) :: n real ( b8 ), dimension ( n :), target :: a real ( b8 ), dimension (:), pointer :: b b => a ( n :) ! b(1) \"points\" to a(-10) write ( * , * ) \"a(-10) =\" , a ( - 10 ), \"b(1) =\" , b ( 1 ) b => a ( 0 :) ! b(1) \"points\" to a(0) write ( * , * ) \"a(-6) =\" , a ( - 6 ), \"b(-5) =\" , b ( - 5 ) end subroutine","title":"Using pointers to access sections of arrays"},{"location":"Documentation/languages/fortran/f90/#allocating-an-array-inside-a-subroutine-and-passing-it-back","text":"Motivation Size of arrays are calculated in the subroutine module numz integer , parameter :: b8 = selected_real_kind ( 14 ) end module program bla use numz real ( b8 ), dimension (:) , pointer :: xyz interface boink subroutine boink ( a ) use numz implicit none real ( b8 ), dimension (:), pointer :: a end subroutine end interface nullify ( xyz ) ! nullify sets a pointer to null write ( * , '(l5)' ) associated ( xyz ) ! is a pointer null, should be call boink ( xyz ) write ( * , '(l5)' , advance = \"no\" ) associated ( xyz ) if ( associated ( xyz )) write ( * , '(i5)' ) size ( xyz ) end program subroutine boink ( a ) use numz implicit none real ( b8 ), dimension (:), pointer :: a if ( associated ( a )) deallocate ( a ) allocate ( a ( 10 )) end subroutine","title":"Allocating an array inside a subroutine and passing it back"},{"location":"Documentation/languages/fortran/f90/#an-example-run_1","text":"F T 10","title":"An example run"},{"location":"Documentation/languages/fortran/f90/#our-fitness-function","text":"Given a fixed number of colors, M, and a description of a map of a collection of N states. Find a coloring of the map such that no two states that share a boarder have the same coloring.","title":"Our fitness function"},{"location":"Documentation/languages/fortran/f90/#example-input-is-a-sorted-list-of-22-western-states","text":"22 ar ok tx la mo xx az ca nm ut nv xx ca az nv or xx co nm ut wy ne ks xx ia mo ne sd mn xx id wa or nv ut wy mt xx ks ne co ok mo xx la tx ar xx mn ia sd nd xx mo ar ok ks ne ia xx mt wy id nd xx nd mt sd wy xx ne sd wy co ks mo ia xx nm az co ok tx mn xx nv ca or id ut az xx ok ks nm tx ar mo xx or ca wa id xx sd nd wy ne ia mn xx tx ok nm la ar xx ut nv az co wy id xx wa id or mt xx wy co mt id ut nd sd ne xx Our fitness function takes a potential coloring, that is, an integer vector of length N and a returns the number of boarders that have states of the same coloring How do we represent the map in memory? One way would be to use an array but it would be very sparse Linked lists are often a better way","title":"Example input is a sorted list of 22 western states"},{"location":"Documentation/languages/fortran/f90/#linked-lists","text":"Motivation We have a collection of states and for each state a list of adjoining states. (Do not count a boarder twice.) Problem is that you do not know the length of the list until runtime. List of adjoining states will be different lengths for different states Solution - Linked list are a good way to handle such situations Linked lists use a derived data type with at least two components Data Pointer to next element module list_stuff type llist integer index ! data type ( llist ), pointer :: next ! pointer to the ! next element end type llist end module","title":"Linked lists"},{"location":"Documentation/languages/fortran/f90/#linked-list-usage","text":"One way to fill a linked list is to use a recursive function `fortran recursive subroutine insert (item, root) use list_stuff implicit none type(llist), pointer :: root integer item if (.not. associated(root)) then allocate(root) nullify(root%next) root%index = item else call insert(item,root%next) endif end subroutine - - - - - - ## Our map representation - An array of the derived data type states - State is name of a state - Linked list containing boarders ```fortran type states character(len=2)name type(llist),pointer:: list end type states - Notes: - We have an array of linked lists - This data structure is often used to represent sparse arrays - We could have a linked list of linked lists - State name is not really required","title":"Linked list usage"},{"location":"Documentation/languages/fortran/f90/#date-and-time-functions","text":"Motivation May want to know the date and time of your program Two functions ! all arguments are optional call date_and_time ( date = c_date , & ! character(len=8) ccyymmdd time = c_time , & ! character(len=10) hhmmss.sss zone = c_zone , & ! character(len=10) +/-hhmm (time zone) values = ivalues ) ! integer ivalues(8) all of the above call system_clock ( count = ic , & ! count of system clock (clicks) count_rate = icr , & ! clicks / second count_max = max_c ) ! max value for count","title":"Date and time functions"},{"location":"Documentation/languages/fortran/f90/#non-advancing-and-character-io","text":"Motivation We read the states using the two character identification One line per state and do not know how many boarder states per line Note: Our list of states is presorted character ( len = 2 ) a ! we have a character variable of length 2 read ( 12 , * ) nstates ! read the number of states allocate ( map ( nstates )) ! and allocate our map do i = 1 , nstates read ( 12 , \"(a2)\" , advance = \"no\" ) map ( i )% name ! read the name !write(*,*)\"state:\",map(i)%name nullify ( map ( i )% list ) ! \"zero out\" our list do read ( 12 , \"(1x,a2)\" , advance = \"no\" ) a ! read list of states ! without going to the ! next line if ( lge ( a , \"xx\" ) . and . lle ( a , \"xx\" )) then ! if state == xx backspace ( 12 ) ! go to the next line read ( 12 , \"(1x,a2)\" , end = 1 ) a ! go to the next line exit endif 1 continue if ( llt ( a , map ( i )% name )) then ! we only add a state to ! our list if its name ! is before ours thus we ! only count boarders 1 time ! what we want put into our linked list is an index ! into our map where we find the bordering state ! thus we do the search here ! any ideas on a better way of doing this search? found =- 1 do j = 1 , i - 1 if ( lge ( a , map ( j )% name ) . and . lle ( a , map ( j )% name )) then !write(*,*)a found = j exit endif enddo if ( found == - 1 ) then write ( * , * ) \"error\" stop endif ! found the index of the boarding state insert it into our list ! note we do the insert into the linked list for a particular state call insert ( found , map ( i )% list ) endif enddo enddo","title":"Non advancing and character IO"},{"location":"Documentation/languages/fortran/f90/#internal-io","text":"Motivation May need to create strings on the fly May need to convert from strings to reals and integers Similar to sprintf and sscanf How it works Create a string Do a normal write except write to the string instead of file number Example 1: creating a date and time stamped file name character ( len = 12 ) tmpstr write ( tmpstr , \"(a12)\" )( c_date ( 5 : 8 ) // c_time ( 1 : 4 ) // \".dat\" ) ! // does string concatination write ( * , * ) \"name of file= \" , tmpstr open ( 14 , file = tmpstr ) name of file = 0327111 4. dat Example 2: Creating a format statement at run time (array of integers and a real) ! test_vect is an array that we do not know its length until run time nstate = 9 ! the size of the array write ( fstr , '(\"(\",i4,\"i1,1x,f10.5)\")' ) nstates write ( * , * ) \"format= \" , fstr write ( * , fstr ) test_vect , fstr format = ( 9 i1 , 1 x , f10 . 5 ) Any other ideas for writing an array when you do not know its length? Example 3: Reading from a string integer ht , minut , sec read ( c_time , \"(3i2)\" ) hr , minut , sec","title":"Internal IO"},{"location":"Documentation/languages/fortran/f90/#inquire-function","text":"Motivation Need to get information about I/O Inquire statement has two forms Information about files (23 different requests can be done) Information about space required for binary output of a value Example: find the size of your real relative to the \"standard\" real Useful for inter language programming Useful for determining data types in MPI (MPI_REAL or MPI_DOUBLE_PRECISION) inquire ( iolength = len_real ) 1.0 inquire ( iolength = len_b8 ) 1.0_b8 write ( * , * ) \"len_b8 \" , len_b8 write ( * , * ) \"len_real\" , len_real iratio = len_b8 / len_real select case ( iratio ) case ( 1 ) my_mpi_type = mpi_real case ( 2 ) my_mpi_type = mpi_double_precision case default write ( * , * ) \"type undefined\" my_mpi_type = 0 end select","title":"Inquire function"},{"location":"Documentation/languages/fortran/f90/#an-example-run_2","text":"len_b8 2 len_real 1","title":"An example run"},{"location":"Documentation/languages/fortran/f90/#namelist","text":"Now part of the standard Motivation A convenient method of doing I/O Good for cases where you have similar runs but change one or two variables Good for formatted output Notes: A little flaky No options for overloading format Example: integer ncolor logical force namelist / the_input / ncolor , force ncolor = 4 force = . true . read ( 13 , the_input ) write ( * , the_input ) On input: & THE_INPUT NCOLOR=4,FORCE = F / Output is &THE_INPUT NCOLOR = 4, FORCE = F /","title":"Namelist"},{"location":"Documentation/languages/fortran/f90/#vector-valued-functions","text":"Motivation May want a function that returns a vector Notes Again requires an interface Use explicit or assumed size array Do not return a pointer to a vector unless you really want a pointer Example: Take an integer input vector which represents an integer in some base and add 1 Could be used in our program to find a \"brute force\" solution function add1 ( vector , max ) result ( rtn ) integer , dimension (:), intent ( in ) :: vector integer , dimension ( size ( vector )) :: rtn integer max integer len logical carry len = size ( vector ) rtn = vector i = 0 carry = . true . do while ( carry ) ! just continue until we do not do a carry i = i + 1 rtn ( i ) = rtn ( i ) + 1 if ( rtn ( i ) . gt . max ) then if ( i == len ) then ! role over set everything back to 0 rtn = 0 else rtn ( i ) = 0 endif else carry = . false . endif enddo end function","title":"Vector valued functions"},{"location":"Documentation/languages/fortran/f90/#usage","text":"test_vect = 0 do test_vect = add1 ( test_vect , 3 ) result = fitness ( test_vect ) if ( result . lt . 1.0_b8 ) then write ( * , * ) test_vect stop endif enddo","title":"Usage"},{"location":"Documentation/languages/fortran/f90/#complete-source-for-recent-discussions","text":"recent.f90 fort.13 Exersize 5 Modify the program to use the random number generator given earlier.","title":"Complete source for recent discussions"},{"location":"Documentation/languages/fortran/f90/#some-array-specific-intrinsic-functions","text":"ALL True if all values are true (LOGICAL) ANY True if any value is true (LOGICAL) COUNT Number of true elements in an array (LOGICAL) DOT_PRODUCT Dot product of two rank one arrays MATMUL Matrix multiplication MAXLOC Location of a maximum value in an array MAXVAL Maximum value in an array MINLOC Location of a minimum value in an array MINVAL Minimum value in an array PACK Pack an array into an array of rank one PRODUCT Product of array elements RESHAPE Reshape an array SPREAD Replicates array by adding a dimension SUM Sum of array elements TRANSPOSE Transpose an array of rank two UNPACK Unpack an array of rank one into an array under a mask Examples program matrix real w ( 10 ), x ( 10 ), mat ( 10 , 10 ) call random_number ( w ) call random_number ( mat ) x = matmul ( w , mat ) ! regular matrix multiply USE IT write ( * , '(\"dot(x,x)=\",f10.5)' ), dot_product ( x , x ) end program program allit character ( len = 10 ) :: f1 = \"(3l1)\" character ( len = 10 ) :: f2 = \"(3i2)\" integer b ( 2 , 3 ), c ( 2 , 3 ), one_d ( 6 ) logical l ( 2 , 3 ) one_d = ( / 1 , 3 , 5 , 2 , 4 , 6 / ) b = transpose ( reshape (( / 1 , 3 , 5 , 2 , 4 , 6 / ), shape = ( / 3 , 2 / ))) C = transpose ( reshape (( / 0 , 3 , 5 , 7 , 4 , 8 / ), shape = ( / 3 , 2 / ))) l = ( b . ne . c ) write ( * , f2 )(( b ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f2 )(( c ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f1 )(( l ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f1 ) all ( b . ne . C ) !is .false. write ( * , f1 ) all ( b . ne . C , DIM = 1 ) !is [.true., .false., .false.] write ( * , f1 ) all ( b . ne . C , DIM = 2 ) !is [.false., .false.] end The output is: 1 3 5 2 4 6 0 3 5 7 4 8 TFF TFT F TFF FF","title":"Some array specific intrinsic functions"},{"location":"Documentation/languages/fortran/f90/#the-rest-of-our-ga","text":"Includes Reproduction Mutation Nothing new in either of these files Source and makefile \"git\" Source and makefile \"*tgz\"","title":"The rest of our GA"},{"location":"Documentation/languages/fortran/f90/#compiler-information","text":"","title":"Compiler Information"},{"location":"Documentation/languages/fortran/f90/#gfortran","text":".f, .for, .ftn .f77 fixed-format Fortran source; compile .f90, .f95 free-format Fortran source; compile -fbacktrace Add debug information for runtime traceback -ffree-form -ffixed-form source form -O0, -O1, -O2, -O3 optimization level .fpp, .FPP, .F, .FOR, .FTN, .F90, .F95, .F03 or .F08 Fortran source file with preprocessor directives -fopenmp turn on OpenMP","title":"gfortran"},{"location":"Documentation/languages/fortran/f90/#intel","text":".f, .for, .ftn fixed-format Fortran source; compile .f90, .f95 free-format Fortran source; compile -O0, -O1, -O2, -O3, -O4 optimization level .fpp, .F, .FOR, .FTN, .FPP, .F90 Fortran source file with preprocessor directives -g compile for debug * -traceback -notraceback (default) Add debug information for runtime traceback -nofree, -free Source is fixed or free format -fopenmp turn on OpenMP","title":"Intel"},{"location":"Documentation/languages/fortran/f90/#portland-group-x86","text":".f, .for, .ftn fixed-format Fortran source; compile .f90, .f95, .f03 free-format Fortran source; compile .cuf free-format CUDA Fortran source; compile .CUF free-format CUDA Fortran source; preprocess, compile -O0, -O1, -O2, -O3, -O4 optimization level -g compile for debug * -traceback (default) -notraceback Add debug information for runtime traceback -Mfixed, -Mfree Source is fixed or free format -qmp turn on OpenMP","title":"Portland Group (x86)"},{"location":"Documentation/languages/fortran/f90/#ibm-xlf","text":"xlf, xlf_r, f77, fort77 Compile FORTRAN 77 source files. _r = thread safe xlf90, xlf90_r, f90 Compile Fortran 90 source files. _r = thread safe xlf95, xlf95_r, f95 Compile Fortran 95 source files. _r = thread safe xlf2003, xlf2003_r,f2003 * Compile Fortran 2003 source files. _r = thread safe xlf2008, xlf2008_r, f2008 * Compile Fortran 2008 source files. .f, .f77, .f90, .f95, .f03, .f08 Fortran source file .F, .F77, .F90, .F95, .F03, .F08 Fortran source file with preprocessor directives -qtbtable=full Add debug information for runtime traceback -qsmp=omp turn on OpenMP -O0, -O1, -O2, -O3, -O4, O5 optimization level -g , g0, g1,...g9 compile for debug","title":"IBM xlf"},{"location":"Documentation/languages/fortran/f90/#summary_1","text":"Fortran 90 has features to: Enhance performance Enhance portability Enhance reliability Enhance maintainability Fortran 90 has new language elements Source form Derived data types Dynamic memory allocation functions Kind facility for portability and easy modification Many new intrinsic function Array assignments Examples Help show how things work Reference for future use","title":"Summary"},{"location":"Documentation/languages/fortran/f90/#overview-of-f90","text":"Introduction to Fortran Language Meta language used in this compact summary Structure of files that can be compiled Executable Statements and Constructs Declarations Key words - other than I/O Key words related to I/O Operators Constants Input/Output Statements Formats Intrinsic Functions","title":"Overview of F90"},{"location":"Documentation/languages/fortran/f90/#introduction-to-fortran-language","text":"Brought to you by ANSI committee X3J3 and ISO-IEC/JTC1/SC22/WG5 (Fortran) This is neither complete nor precisely accurate, but hopefully, after a small investment of time it is easy to read and very useful. This is the free form version of Fortran, no statement numbers, no C in column 1, start in column 1 (not column 7), typically indent 2, 3, or 4 spaces per each structure. The typical extension is .f90 . Continue a statement on the next line by ending the previous line with an ampersand &amp; . Start the continuation with &amp; for strings. The rest of any line is a comment starting with an exclamation mark ! . Put more than one statement per line by separating statements with a semicolon ; . Null statements are OK, so lines can end with semicolons. Separate words with space or any form of \"white space\" or punctuation.","title":"Introduction to Fortran Language"},{"location":"Documentation/languages/fortran/f90/#meta-language-used-in-this-compact-summary","text":"<xxx> means fill in something appropriate for xxx and do not type the \"<\" or \">\" . ... ellipsis means the usual, fill in something, one or more lines [stuff] means supply nothing or at most one copy of \"stuff\" [stuff1 [stuff2]] means if \"stuff1\" is included, supply nothing or at most one copy of stuff2. \"old\" means it is in the language, like almost every feature of past Fortran standards, but should not be used to write new programs.","title":"Meta language used in this compact summary"},{"location":"Documentation/languages/fortran/f90/#structure-of-files-that-can-be-compiled","text":"program <name> usually file name is <name>.f90 use <module_name> bring in any needed modules implicit none good for error detection <declarations> <executable statements> order is important, no more declarations end program <name> block data <name> old <declarations> common, dimension, equivalence now obsolete end block data <name> module <name> bring back in with use <name> implicit none good for error detection <declarations> can have private and public and interface end module <name> subroutine <name> use: call <name> to execute implicit none good for error detection <declarations> <executable statements> end subroutine <name> subroutine <name>(par1, par2, ...) use: call <name>(arg1, arg2,... ) to execute implicit none optional, good for error detection <declarations> par1, par2, ... are defined in declarations and can be specified in, inout, pointer, etc. <executable statements> return optional, end causes automatic return entry <name> (par...) old, optional other entries end subroutine <name> function <name>(par1, par2, ...) result(<rslt>) use: <name>(arg1, arg2, ... argn) as variable implicit none optional, good for error detection <declarations> rslt, par1, ... are defined in declarations <executable statements> <rslt> = <expression> required somewhere in execution [return] optional, end causes automatic return end function <name> old <type> function(...) <name> use: <name>(arg1, arg2, ... argn) as variable <declarations> <executable statements> <name> = <expression> required somewhere in execution [return] optional, end causes automatic return end function <name>","title":"Structure of files that can be compiled"},{"location":"Documentation/languages/fortran/f90/#executable-statements-and-constructs","text":"<statement> will mean exactly one statement in this section a construct is multiple lines <label> : <statement> any statement can have a label (a name) <variable> = <expression> assignment statement <pointer> >= <variable> the pointer is now an alias for the variable <pointer1> >= <pointer2> pointer1 now points same place as pointer2 stop can be in any executable statement group, stop <integer> terminates execution of the program, stop <string> can have optional integer or string return exit from subroutine or function do <variable>=<from>,<to> [,<increment&gt] optional: <label> : do ... <statements> exit \\_optional or exit <label&gt if (<boolean expression>) exit / exit the loop cycle \\_optional or cycle <label> if (<boolean expression>) cycle / continue with next loop iteration end do optional: end do <name> do while (<boolean expression>) ... optional exit and cycle allowed end do do ... exit required to end the loop optional cycle can be used end do if ( <boolean expression> ) <statement> execute the statement if the boolean expression is true if ( <boolean expression1> ) then ... execute if expression1 is true else if ( <boolean expression2> ) then ... execute if expression2 is true else if ( <boolean expression3> ) then ... execute if expression3 is true else ... execute if none above are true end if select case (<expression>) optional <name> : select case ... case (<value>) <statements> execute if expression == value case (<value1>:<value2>) <statements> execute if value1 &le; expression &le; value2 ... case default <statements> execute if no values above match end select optional end select <name> real, dimension(10,12) :: A, R a sample declaration for use with \"where\" ... where (A /= 0.0) conditional assignment, only assignment allowed R = 1.0/A elsewhere R = 1.0 elements of R set to 1.0 where A == 0.0 end where go to <statement number> old go to (<statement number list>), <expression> old for I/O statements, see: section 10.0 Input/Output Statements many old forms of statements are not listed","title":"Executable Statements and Constructs"},{"location":"Documentation/languages/fortran/f90/#declarations","text":"There are five (5) basic types: integer, real, complex, character and logical. There may be any number of user derived types. A modern (not old) declaration starts with a type, has attributes, then ::, then variable(s) names integer i, pivot, query old integer, intent (inout) :: arg1 integer (selected_int_kind (5)) :: i1, i2 integer, parameter :: m = 7 integer, dimension(0:4, -5:5, 10:100) :: A3D double precision x old real (selected_real_kind(15,300) :: x complex :: z logical, parameter :: what_if = .true. character, parameter :: me = \"Jon Squire\" type <name> a new user type, derived type declarations end type <name> type (<name>) :: stuff declaring stuff to be of derived type <name> real, dimension(:,:), allocatable, target :: A real, dimension(:,:), pointer :: P Attributes may be: allocatable no memory used here, allocate later dimension vector or multi dimensional array external will be defined outside this compilation intent argument may be in, inout or out intrinsic declaring function to be an intrinsic optional argument is optional parameter declaring a constant, can not be changed later pointer declaring a pointer private in a module, a private declaration public in a module, a public declaration save keep value from one call to the next, static target can be pointed to by a pointer Note: not all combinations of attributes are legal","title":"Declarations"},{"location":"Documentation/languages/fortran/f90/#key-words-other-than-io","text":"note: \"statement\" means key word that starts a statement, one line unless there is a continuation \"&amp;\" \"construct\" means multiple lines, usually ending with \"end ...\" \"attribute\" means it is used in a statement to further define \"old\" means it should not be used in new code allocatable attribute, no space allocated here, later allocate allocate statement, allocate memory space now for variable assign statement, old, assigned go to assignment attribute, means subroutine is assignment (=) block data construct, old, compilation unit, replaced by module call statement, call a subroutine case statement, used in select case structure character statement, basic type, intrinsic data type common statement, old, allowed overlaying of storage complex statement, basic type, intrinsic data type contains statement, internal subroutines and functions follow continue statement, old, a place to put a statement number cycle statement, continue the next iteration of a do loop data statement, old, initialized variables and arrays deallocate statement, free up storage used by specified variable default statement, in a select case structure, all others do construct, start a do loop double precision statement, old, replaced by selected_real_kind(15,300) else construct, part of if else if else end if else if construct, part of if else if else end if elsewhere construct, part of where elsewhere end where end block data construct, old, ends block data end do construct, ends do end function construct, ends function end if construct, ends if end interface construct, ends interface end module construct, ends module end program construct, ends program end select construct, ends select case end subroutine construct, ends subroutine end type construct, ends type end where construct, ends where entry statement, old, another entry point in a procedure equivalence statement, old, overlaid storage exit statement, continue execution outside of a do loop external attribute, old statement, means defines else where function construct, starts the definition of a function go to statement, old, requires fixed form statement number if statement and construct, if(...) statement implicit statement, \"none\" is preferred to help find errors in a keyword for intent, the argument is read only inout a keyword for intent, the argument is read/write integer statement, basic type, intrinsic data type intent attribute, intent(in) or intent(out) or intent(inout) interface construct, begins an interface definition intrinsic statement, says that following names are intrinsic kind attribute, sets the kind of the following variables len attribute, sets the length of a character string logical statement, basic type, intrinsic data type module construct, beginning of a module definition namelist statement, defines a namelist of input/output nullify statement, nullify(some_pointer) now points nowhere only attribute, restrict what comes from a module operator attribute, indicates function is an operator, like + optional attribute, a parameter or argument is optional out a keyword for intent, the argument will be written parameter attribute, old statement, makes variable real only pause old, replaced by stop pointer attribute, defined the variable as a pointer alias private statement and attribute, in a module, visible inside program construct, start of a main program public statement and attribute, in a module, visible outside real statement, basic type, intrinsic data type recursive attribute, allows functions and derived type recursion result attribute, allows naming of function result result(Y) return statement, returns from, exits, subroutine or function save attribute, old statement, keep value between calls select case construct, start of a case construct stop statement, terminate execution of the main procedure subroutine construct, start of a subroutine definition target attribute, allows a variable to take a pointer alias then part of if construct type construct, start of user defined type type ( ) statement, declaration of a variable for a users type use statement, brings in a module where construct, conditional assignment while construct, a while form of a do loop","title":"Key words (other than I/O)"},{"location":"Documentation/languages/fortran/f90/#key-words-related-to-io","text":"backspace statement, back up one record close statement, close a file endfile statement, mark the end of a file format statement, old, defines a format inquire statement, get the status of a unit open statement, open or create a file print statement, performs output to screen read statement, performs input rewind statement, move read or write position to beginning write statement, performs output","title":"Key words related to I/O"},{"location":"Documentation/languages/fortran/f90/#operators","text":"** exponentiation * multiplication / division + addition - subtraction // concatenation == .eq. equality /= .ne. not equal < .lt. less than > .gt. greater than <= .le. less than or equal >= .ge. greater than or equal .not. complement, negation .and. logical and .or. logical or .eqv. logical equivalence .neqv. logical not equivalence, exclusive or .eq. == equality, old .ne. /= not equal. old .lt. < less than, old .gt. > greater than, old .le. <= less than or equal, old .ge. >= greater than or equal, old Other punctuation: / ... / used in data, common, namelist and other statements (/ ... /) array constructor, data is separated by commas 6*1.0 in some contexts, 6 copies of 1.0 (i:j:k) in some contexts, a list i, i+k, i+2k, i+3k, ... i+nk&le;j (:j) j and all below (i:) i and all above (:) undefined or all in range","title":"Operators"},{"location":"Documentation/languages/fortran/f90/#constants","text":"Logical constants: .true. True .false. False Integer constants: 0 1 -1 123456789 Real constants: 0.0 1.0 -1.0 123.456 7.1E+10 -52.715E-30 Complex constants: (0.0, 0.0) (-123.456E+30, 987.654E-29) Character constants: \"ABC\" \"a\" \"123'abc$%#@!\" \" a quote \"\" \" 'ABC' 'a' '123\"abc$%#@!' ' a apostrophe '' ' Derived type values: type name character (len=30) :: last character (len=30) :: first character (len=30) :: middle end type name type address character (len=40) :: street character (len=40) :: more character (len=20) :: city character (len=2) :: state integer (selected_int_kind(5)) :: zip_code integer (selected_int_kind(4)) :: route_code end type address type person type (name) lfm type (address) snail_mail end type person type (person) :: a_person = person( name(\"Squire\",\"Jon\",\"S.\"), &amp; address(\"106 Regency Circle\", \"\", \"Linthicum\", \"MD\", 21090, 1936)) a_person%snail_mail%route_code == 1936","title":"Constants"},{"location":"Documentation/languages/fortran/f90/#inputoutput-statements","text":"open (<unit number>) open (unit=<unit number>, file=<file name>, iostat=<variable>) open (unit=<unit number>, ... many more, see below ) close (<unit number>) close (unit=<unit number>, iostat=<variable>, err=<statement number>, status=\"KEEP\") read (<unit number>) <input list> read (unit=<unit number>, fmt=<format>, iostat=<variable>, end=<statement number>, err=<statement number>) <input list> read (unit=<unit number>, rec=<record number>) <input list> write (<unit number>) <output list> write (unit=<unit number>, fmt=<format>, iostat=<variable>, err=<statement number>) <output list> write (unit=<unit number>, rec=<record number>) <output list> print *, <output list> print \"(<your format here, use apostrophe, not quote>)\", <output list> rewind <unit number> rewind (<unit number>, err=<statement number>) backspace <unit number> backspace (<unit number>, iostat=<variable>) endfile <unit number> endfile (<unit number>, err=<statement number>, iostat=<variable>) inquire ( <unit number>, exists = <variable>) inquire ( file=<\"name\">, opened = <variable1>, access = <variable2> ) inquire ( iolength = <variable> ) x, y, A ! gives \"recl\" for \"open\" namelist /<name>/ <variable list> defines a name list read(*,nml=<name>) reads some/all variables in namelist write(*,nml=<name>) writes all variables in namelist &amp;<name> <variable>=<value> ... <variable=value> / data for namelist read Input / Output specifiers access one of \"sequential\" \"direct\" \"undefined\" action one of \"read\" \"write\" \"readwrite\" advance one of \"yes\" \"no\" blank one of \"null\" \"zero\" delim one of \"apostrophe\" \"quote\" \"none\" end = <integer statement number> old eor = <integer statement number> old err = <integer statement number> old exist = <logical variable> file = <\"file name\"> fmt = <\"(format)\"> or <character variable> format form one of \"formatted\" \"unformatted\" \"undefined\" iolength = <integer variable, size of unformatted record> iostat = <integer variable> 0==good, negative==eof, positive==bad name = <character variable for file name> named = <logical variable> nml = <namelist name> nextrec = <integer variable> one greater than written number = <integer variable unit number> opened = <logical variable> pad one of \"yes\" \"no\" position one of \"asis\" \"rewind\" \"append\" rec = <integer record number> recl = <integer unformatted record size> size = <integer variable> number of characters read before eor status one of \"old\" \"new\" \"unknown\" \"replace\" \"scratch\" \"keep\" unit = <integer unit number> Individual questions direct = <character variable> \"yes\" \"no\" \"unknown\" formatted = <character variable> \"yes\" \"no\" \"unknown\" read = <character variable> \"yes\" \"no\" \"unknown\" readwrite = <character variable> \"yes\" \"no\" \"unknown\" sequential = <character variable> \"yes\" \"no\" \"unknown\" unformatted = <character variable> \"yes\" \"no\" \"unknown\" write = <character variable> \"yes\" \"no\" \"unknown\"","title":"Input/Output Statements"},{"location":"Documentation/languages/fortran/f90/#formats","text":"format an explicit format can replace * in any I/O statement. Include the format in apostrophes or quotes and keep the parenthesis. examples: print \"(3I5,/(2X,3F7.2/))\", <output list> write(6, '(a,E15.6E3/a,G15.2)' ) <output list> read(unit=11, fmt=\"(i4, 4(f3.0,TR1))\" ) <input list> A format includes the opening and closing parenthesis. A format consists of format items and format control items separated by comma. A format may contain grouping parenthesis with an optional repeat count. Format Items, data edit descriptors: key: w is the total width of the field (filled with *** if overflow) m is the least number of digits in the (sub)field (optional) d is the number of decimal digits in the field e is the number of decimal digits in the exponent subfield c is the repeat count for the format item n is number of columns cAw data of type character (w is optional) cBw.m data of type integer with binary base cDw.d data of type real -- same as E, old double precision cEw.d or Ew.dEe data of type real cENw.d or ENw.dEe data of type real -- exponent a multiple of 3 cESw.d or ESw.dEe data of type real -- first digit non zero cFw.d data of type real -- no exponent printed cGw.d or Gw.dEe data of type real -- auto format to F or E nH n characters follow the H, no list item cIw.m data of type integer cLw data of type logical -- .true. or .false. cOw.m data of type integer with octal base cZw.m data of type integer with hexadecimal base \"<string>\" literal characters to output, no list item '<string>' literal characters to output, no list item Format Control Items, control edit descriptors: BN ignore non leading blanks in numeric fields BZ treat nonleading blanks in numeric fields as zeros nP apply scale factor to real format items old S printing of optional plus signs is processor dependent SP print optional plus signs SS do not print optional plus signs Tn tab to specified column TLn tab left n columns TRn tab right n columns nX tab right n columns / end of record (implied / at end of all format statements) : stop format processing if no more list items <input list> can be: a variable an array name an implied do ((A(i,j),j=1,n) ,i=1,m) parenthesis and commas as shown note: when there are more items in the input list than format items, the repeat rules for formats applies. <output list> can be: a constant a variable an expression an array name an implied do ((A(i,j),j=1,n) ,i=1,m) parenthesis and commas as shown note: when there are more items in the output list than format items, the repeat rules for formats applies. Repeat Rules for Formats: Each format item is used with a list item. They are used in order. When there are more list items than format items, then the following rule applies: There is an implied end of record, /, at the closing parenthesis of the format, this is processed. Scan the format backwards to the first left parenthesis. Use the repeat count, if any, in front of this parenthesis, continue to process format items and list items. Note: an infinite loop is possible print \"(3I5/(1X/))\", I, J, K, L may never stop","title":"Formats"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions","text":"Intrinsic Functions are presented in alphabetical order and then grouped by topic. The function name appears first. The argument(s) and result give an indication of the type(s) of argument(s) and results. [,dim=] indicates an optional argument \"dim\". \"mask\" must be logical and usually conformable. \"character\" and \"string\" are used interchangeably. A brief description or additional information may appear.","title":"Intrinsic Functions"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-alphabetical","text":"abs(integer_real_complex) result(integer_real_complex) achar(integer) result(character) integer to character acos(real) result(real) arccosine |real| &le; 1.0 0&le;result&le;Pi adjustl(character) result(character) left adjust, blanks go to back adjustr(character) result(character) right adjust, blanks to front aimag(complex) result(real) imaginary part aint(real [,kind=]) result(real) truncate to integer toward zero all(mask [,dim]) result(logical) true if all elements of mask are true allocated(array) result(logical) true if array is allocated in memory anint(real [,kind=]) result(real) round to nearest integer any(mask [,dim=}) result(logical) true if any elements of mask are true asin(real) result(real) arcsine |real| &le; 1.0 -Pi/2&le;result&le;Pi/2 associated(pointer [,target=]) result(logical) true if pointing atan(real) result(real) arctangent -Pi/2&le;result&le;Pi/2 atan2(y=real,x=real) result(real) arctangent -Pi&le;result&le;Pi bit_size(integer) result(integer) size in bits in model of argument btest(i=integer,pos=integer) result(logical) true if pos has a 1, pos=0.. ceiling(real) result(real) truncate to integer toward infinity char(integer [,kind=]) result(character) integer to character [of kind] cmplx(x=real [,y=real] [kind=]) result(complex) x+iy conjg(complex) result(complex) reverse the sign of the imaginary part cos(real_complex) result(real_complex) cosine cosh(real) result(real) hyperbolic cosine count(mask [,dim=]) result(integer) count of true entries in mask cshift(array,shift [,dim=]) circular shift elements of array, + is right date_and_time([date=] [,time=] [,zone=] [,values=]) y,m,d,utc,h,m,s,milli dble(integer_real_complex) result(real_kind_double) convert to double digits(integer_real) result(integer) number of bits to represent model dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction dot_product(vector_a,vector_b) result(integer_real_complex) inner product dprod(x=real,y=real) result(x_times_y_double) double precision product eoshift(array,shift [,boundary=] [,dim=]) end-off shift using boundary epsilon(real) result(real) smallest positive number added to 1.0 /= 1.0 exp(real_complex) result(real_complex) e raised to a power exponent(real) result(integer) the model exponent of the argument floor(real) result(real) truncate to integer towards negative infinity fraction(real) result(real) the model fractional part of the argument huge(integer_real) result(integer_real) the largest model number iachar(character) result(integer) position of character in ASCII sequence iand(integer,integer) result(integer) bit by bit logical and ibclr(integer,pos) result(integer) argument with pos bit cleared to zero ibits(integer,pos,len) result(integer) extract len bits starting at pos ibset(integer,pos) result(integer) argument with pos bit set to one ichar(character) result(integer) pos in collating sequence of character ieor(integer,integer) result(integer) bit by bit logical exclusive or index(string,substring [,back=]) result(integer) pos of substring int(integer_real_complex) result(integer) convert to integer ior(integer,integer) result(integer) bit by bit logical or ishft(integer,shift) result(integer) shift bits in argument by shift ishftc(integer, shift) result(integer) shift circular bits in argument kind(any_intrinsic_type) result(integer) value of the kind lbound(array,dim) result(integer) smallest subscript of dim in array len(character) result(integer) number of characters that can be in argument len_trim(character) result(integer) length without trailing blanks lge(string_a,string_b) result(logical) string_a &ge; string_b lgt(string_a,string_b) result(logical) string_a > string_b lle(string_a,string_b) result(logical) string_a &le; string_b llt(string_a,string_b) result(logical) string_a < string_b log(real_complex) result(real_complex) natural logarithm log10(real) result(real) logarithm base 10 logical(logical [,kind=]) convert to logical matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical max(a1,a2,a3,...) result(integer_real) maximum of list of values maxexponent(real) result(integer) maximum exponent of model type maxloc(array [,mask=]) result(integer_vector) indices in array of maximum maxval(array [,dim=] [,mask=]) result(array_element) maximum value merge(true_source,false_source,mask) result(source_type) choose by mask min(a1,a2,a3,...) result(integer-real) minimum of list of values minexponent(real) result(integer) minimum(negative) exponent of model type minloc(array [,mask=]) result(integer_vector) indices in array of minimum minval(array [,dim=] [,mask=]) result(array_element) minimum value mod(a=integer_real,p) result(integer_real) a modulo p modulo(a=integer_real,p) result(integer_real) a modulo p mvbits(from,frompos,len,to,topos) result(integer) move bits nearest(real,direction) result(real) nearest value toward direction nint(real [,kind=]) result(real) round to nearest integer value not(integer) result(integer) bit by bit logical complement pack(array,mask [,vector=]) result(vector) vector of elements from array present(argument) result(logical) true if optional argument is supplied product(array [,dim=] [,mask=]) result(integer_real_complex) product radix(integer_real) result(integer) radix of integer or real model, 2 random_number(harvest=real_out) subroutine, uniform random number 0 to 1 random_seed([size=] [,put=] [,get=]) subroutine to set random number seed range(integer_real_complex) result(integer_real) decimal exponent of model real(integer_real_complex [,kind=]) result(real) convert to real repeat(string,ncopies) result(string) concatenate n copies of string reshape(source,shape,pad,order) result(array) reshape source to array rrspacing(real) result(real) reciprocal of relative spacing of model scale(real,integer) result(real) multiply by 2**integer scan(string,set [,back]) result(integer) position of first of set in string selected_int_kind(integer) result(integer) kind number to represent digits selected_real_kind(integer,integer) result(integer) kind of digits, exp set_exponent(real,integer) result(real) put integer as exponent of real shape(array) result(integer_vector) vector of dimension sizes sign(integer_real,integer_real) result(integer_real) sign of second on first sin(real_complex) result(real_complex) sine of angle in radians sinh(real) result(real) hyperbolic sine of argument size(array [,dim=]) result(integer) number of elements in dimension spacing(real) result(real) spacing of model numbers near argument spread(source,dim,ncopies) result(array) expand dimension of source by 1 sqrt(real_complex) result(real_complex) square root of argument sum(array [,dim=] [,mask=]) result(integer_real_complex) sum of elements system_clock([count=] [,count_rate=] [,count_max=]) subroutine, all out tan(real) result(real) tangent of angle in radians tanh(real) result(real) hyperbolic tangent of angle in radians tiny(real) result(real) smallest positive model representation transfer(source,mold [,size]) result(mold_type) same bits, new type transpose(matrix) result(matrix) the transpose of a matrix trim(string) result(string) trailing blanks are removed ubound(array,dim) result(integer) largest subscript of dim in array unpack(vector,mask,field) result(v_type,mask_shape) field when not mask verify(string,set [,back]) result(integer) pos in string not in set","title":"Intrinsic Functions (alphabetical):"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-grouped-by-topic","text":"","title":"Intrinsic Functions (grouped by topic):"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-numeric","text":"abs(integer_real_complex) result(integer_real_complex) acos(real) result(real) arccosine |real| &le; 1.0 0&le;result&le;Pi aimag(complex) result(real) imaginary part aint(real [,kind=]) result(real) truncate to integer toward zero anint(real [,kind=]) result(real) round to nearest integer asin(real) result(real) arcsine |real| &le; 1.0 -Pi/2&le;result&le;Pi/2 atan(real) result(real) arctangent -Pi/2&le;result&le;Pi/2 atan2(y=real,x=real) result(real) arctangent -Pi&le;result&le;Pi ceiling(real) result(real) truncate to integer toward infinity cmplx(x=real [,y=real] [kind=]) result(complex) x+iy conjg(complex) result(complex) reverse the sign of the imaginary part cos(real_complex) result(real_complex) cosine cosh(real) result(real) hyperbolic cosine dble(integer_real_complex) result(real_kind_double) convert to double digits(integer_real) result(integer) number of bits to represent model dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction dot_product(vector_a,vector_b) result(integer_real_complex) inner product dprod(x=real,y=real) result(x_times_y_double) double precision product epsilon(real) result(real) smallest positive number added to 1.0 /= 1.0 exp(real_complex) result(real_complex) e raised to a power exponent(real) result(integer) the model exponent of the argument floor(real) result(real) truncate to integer towards negative infinity fraction(real) result(real) the model fractional part of the argument huge(integer_real) result(integer_real) the largest model number int(integer_real_complex) result(integer) convert to integer log(real_complex) result(real_complex) natural logarithm log10(real) result(real) logarithm base 10 matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical max(a1,a2,a3,...) result(integer_real) maximum of list of values maxexponent(real) result(integer) maximum exponent of model type maxloc(array [,mask=]) result(integer_vector) indices in array of maximum maxval(array [,dim=] [,mask=]) result(array_element) maximum value min(a1,a2,a3,...) result(integer-real) minimum of list of values minexponent(real) result(integer) minimum(negative) exponent of model type minloc(array [,mask=]) result(integer_vector) indices in array of minimum minval(array [,dim=] [,mask=]) result(array_element) minimum value mod(a=integer_real,p) result(integer_real) a modulo p modulo(a=integer_real,p) result(integer_real) a modulo p nearest(real,direction) result(real) nearest value toward direction nint(real [,kind=]) result(real) round to nearest integer value product(array [,dim=] [,mask=]) result(integer_real_complex) product radix(integer_real) result(integer) radix of integer or real model, 2 random_number(harvest=real_out) subroutine, uniform random number 0 to 1 random_seed([size=] [,put=] [,get=]) subroutine to set random number seed range(integer_real_complex) result(integer_real) decimal exponent of model real(integer_real_complex [,kind=]) result(real) convert to real rrspacing(real) result(real) reciprocal of relative spacing of model scale(real,integer) result(real) multiply by 2**integer set_exponent(real,integer) result(real) put integer as exponent of real sign(integer_real,integer_real) result(integer_real) sign of second on first sin(real_complex) result(real_complex) sine of angle in radians sinh(real) result(real) hyperbolic sine of argument spacing(real) result(real) spacing of model numbers near argument sqrt(real_complex) result(real_complex) square root of argument sum(array [,dim=] [,mask=]) result(integer_real_complex) sum of elements tan(real) result(real) tangent of angle in radians tanh(real) result(real) hyperbolic tangent of angle in radians tiny(real) result(real) smallest positive model representation transpose(matrix) result(matrix) the transpose of a matrix","title":"Intrinsic Functions (Numeric)"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-logical-and-bit","text":"all(mask [,dim]) result(logical) true if all elements of mask are true any(mask [,dim=}) result(logical) true if any elements of mask are true bit_size(integer) result(integer) size in bits in model of argument btest(i=integer,pos=integer) result(logical) true if pos has a 1, pos=0.. count(mask [,dim=]) result(integer) count of true entries in mask iand(integer,integer) result(integer) bit by bit logical and ibclr(integer,pos) result(integer) argument with pos bit cleared to zero ibits(integer,pos,len) result(integer) extract len bits starting at pos ibset(integer,pos) result(integer) argument with pos bit set to one ieor(integer,integer) result(integer) bit by bit logical exclusive or ior(integer,integer) result(integer) bit by bit logical or ishft(integer,shift) result(integer) shift bits in argument by shift ishftc(integer, shift) result(integer) shift circular bits in argument logical(logical [,kind=]) convert to logical matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical merge(true_source,false_source,mask) result(source_type) choose by mask mvbits(from,frompos,len,to,topos) result(integer) move bits not(integer) result(integer) bit by bit logical complement transfer(source,mold [,size]) result(mold_type) same bits, new type","title":"Intrinsic Functions (Logical and bit)"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-character-or-string","text":"achar(integer) result(character) integer to character adjustl(character) result(character) left adjust, blanks go to back adjustr(character) result(character) right adjust, blanks to front char(integer [,kind=]) result(character) integer to character [of kind] iachar(character) result(integer) position of character in ASCII sequence ichar(character) result(integer) pos in collating sequence of character index(string,substring [,back=]) result(integer) pos of substring len(character) result(integer) number of characters that can be in argument len_trim(character) result(integer) length without trailing blanks lge(string_a,string_b) result(logical) string_a &ge; string_b lgt(string_a,string_b) result(logical) string_a > string_b lle(string_a,string_b) result(logical) string_a &le; string_b llt(string_a,string_b) result(logical) string_a < string_b repeat(string,ncopies) result(string) concatenate n copies of string scan(string,set [,back]) result(integer) position of first of set in string trim(string) result(string) trailing blanks are removed verify(string,set [,back]) result(integer) pos in string not in set","title":"Intrinsic Functions (Character or string)"},{"location":"Documentation/languages/fortran/f90/#fortran-95","text":"New Features The statement FORALL as an alternative to the DO-statement Partial nesting of FORALL and WHERE statements Masked ELSEWHERE Pure procedures Elemental procedures Pure procedures in specification expressions Revised MINLOC and MAXLOC Extensions to CEILING and FLOOR with the KIND keyword argument Pointer initialization Default initialization of derived type objects Increased compatibility with IEEE arithmetic A CPU_TIME intrinsic subroutine A function NULL to nullify a pointer Automatic deallocation of allocatable arrays at exit of scoping unit Comments in NAMELIST at input Minimal field at input Complete version of END INTERFACE Deleted Features real and double precision DO loop index variables branching to END IF from an outer block PAUSE statements ASSIGN statements and assigned GO TO statements and the use of an assigned integer as a FORMAT specification Hollerith editing in FORMAT See http://www.nsc.liu.se/~boein/f77to90/f95.html#17.5","title":"Fortran 95"},{"location":"Documentation/languages/fortran/f90/#references","text":"http://www.fortran.com/fortran/ Pointer to everything Fortran http://meteora.ucsd.edu/~pierce/fxdr_home_page.html Subroutines to do unformatted I/O across platforms, provided by David Pierce at UCSD http://www.nsc.liu.se/~boein/f77to90/a5.html A good reference for intrinsic functions https://wg5-fortran.org/N1551-N1600/N1579.pdf New Features of Fortran 2003 https://wg5-fortran.org/N1701-N1750/N1729.pdf New Features of Fortran 2008 http://www.nsc.liu.se/~boein/f77to90/ Fortran 90 for the Fortran 77 Programmer Fortran 90 Handbook Complete ANSI/ISO Reference . Jeanne Adams, Walt Brainerd, Jeanne Martin, Brian Smith, Jerrold Wagener Fortran 90 Programming . T. Ellis, Ivor Philips, Thomas Lahey https://github.com/llvm/llvm-project/blob/master/flang/docs/FortranForCProgrammers.md FFT stuff Fortran 95 and beyond","title":"References"},{"location":"Documentation/languages/python/","text":"Python Eagle tutorials Python environments : Utilize a specific version of Python on Eagle and install packages Dask : Parallelize your Python code Jupyter notebooks : Run interactive notebooks on Eagle HPC Python Links to External resources: MPI4PY Python bindings to use MPI to distribute computations across cluster nodes Dask Easily launch Dask workers on one node or across nodes Numba Optimize your Python code to run faster PyCUDA Utilize GPUs to accelerate computations","title":"Python"},{"location":"Documentation/languages/python/#python","text":"","title":"Python"},{"location":"Documentation/languages/python/#eagle-tutorials","text":"Python environments : Utilize a specific version of Python on Eagle and install packages Dask : Parallelize your Python code Jupyter notebooks : Run interactive notebooks on Eagle","title":"Eagle tutorials"},{"location":"Documentation/languages/python/#hpc-python","text":"Links to External resources: MPI4PY Python bindings to use MPI to distribute computations across cluster nodes Dask Easily launch Dask workers on one node or across nodes Numba Optimize your Python code to run faster PyCUDA Utilize GPUs to accelerate computations","title":"HPC Python"},{"location":"Documentation/languages/python/NREL_python/","text":"Python on NREL HPC By design, the HPC is a time-shared multi-machine system which necessarily warrants some nuanced consideration for how environments are managed relative to a single machine with a single user. Sometimes, the default workflow for environment creation and usage is not the most optimal for some use-cases. Below is a list of common pitfalls that users have encountered historically while using Python and Anaconda on NREL HPC. Running a SLURM job that uses a conda environment which is stored in $HOME . Exhausting the $HOME storage quota (50GB on the current HPC system) usually because of conda's package cache combined with their user environments. Trying to share a conda environment from another user's /home directory. Forgetting to install jupyter in a new conda environment, resulting in using the base installation's version which doesn't have your dependencies installed. Let's discuss strategies to mitigate or avoid these kinds of problems Installing Conda Environments in Different Directories By default, conda will install new environments in $HOME/.conda . Generally speaking, this a sensible default\u2014it just happens to be the starting point to frequent issues that users have experienced historically. Something to consider is that conda has a --prefix flag which allows one to arbitrate where a conda environment gets installed to, notably allowing you to place environments on other file-systems and block devices besides the /home network-storage that is mounted on NREL HPC systems. For example, here is how one might create a project in their /scratch directory: ENV_PREFIX = \"/scratch/$USER/demo_scratch_env\" import os ; os . environ [ 'ENV_PREFIX' ] = ENV_PREFIX # Export this variable for cells below ! conda create -- quiet -- use - local -- yes \\ -- prefix $ ENV_PREFIX # `--prefix` in action \\ python = 3.7 Collecting package metadata: ...working... done Solving environment: ...working... done ## Package Plan ## environment location: /scratch/mbartlet/demo_scratch_env Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done ! ls - ld $ ENV_PREFIX drwxr-xr-x. 3 mbartlet mbartlet 4096 Dec 3 11:10 /scratch/mbartlet/demo_scratch_env # Delete the demo environment for cleanliness ! conda - env remove -- yes -- quiet -- prefix $ ENV_PREFIX &>/ dev / null Below is a table which discusses the pros and cons of each block-device mount on NREL HPC as a location for storing your software environments. Block-device mounts Situations where you would want to use this block device for your conda environments Caveats to consider when using this mount /home $HOME/.conda is the default location for environments. For one-off environments, or if you don't create environments often, this is a reasonable location for your environments and doesn't require any extra flags or parameters. Files in \\$ HOME will not be purged so long as you have an active NREL HPC account. However, \\$ HOME is limited to a 50GB storage quota so you may have to take care to monitor your storage footprint. /scratch /scratch or /projects is ultimately where you want your environment to end up if your jobs have more than 1 node\u2014if your environment is in /home then every node in your job will be competing for read-access over a non-parallel network fabric to the source files of your environment. /scratch provides simultaneous access to all the nodes. A sensible approach is copying your environments from /home to /scratch as part of your job's initialization. /scratch storage is unlimited. /scratch is a parallel filesystem, meaning simultaneous filesystem operations by several nodes is possible and performant. However, the contents of /scratch are subject to purge after 28 days of inactivity. /projects This is a great place to put a conda environment that you anticipate sharing with your colleagues who are also working on the project. You can structure the permissions such that others in the project have read-only, write-only, or no access (we also encourage restoring these permissions at a later date so others on the project can manage your files without a hassle). /projects is also a parallel filesystem which reaps the same benefits as mentioned above. However, access to projects is contingent on having access to an HPC project allocation. Moreover, the storage quota allotted to each project is relative to the reasonableness of its requested needs, although a conda environment is very unlikely to have a significant storage footprint. As mentioned above, let's demonstrate one might go about copying an environment from /home to /scratch in a SLURM job. The below cell will generate a nice code block based on variables used earlier in this notebook, as well as environment variables within your user account: # Acquire a default project handle to procedurally generate a SLURM job import subprocess command = \"/nopt/nrel/utils/bin/hours_report | tail -1 | awk '{print $1}'\" # Grab a valid project handle command_array = [ '/bin/bash' , '-c' , command ] project_handle = subprocess . run ( command_array , stdout = subprocess . PIPE ) . stdout . decode ( 'utf-8' )[: - 1 ] import os ; os . environ [ 'DEFAULT_HANDLE' ] = project_handle # Export handle for cells below ! echo $ DEFAULT_HANDLE wks conda_home_env = \"py3\" # Acquire info about the default conda environment import subprocess command = f \"module load conda && . activate { conda_home_env } && echo $CONDA_PREFIX\" command_array = [ '/bin/bash' , '-lc' , # Have to run this from a login-shell command ] conda_home_env_prefix = subprocess . run ( command_array , stdout = subprocess . PIPE ) . stdout . decode ( 'utf-8' )[: - 1 ] import os ; os . environ [ 'CONDA_HOME_ENV_PREFIX' ] = conda_home_env_prefix # Export handle for cells below ! echo $ CONDA_HOME_ENV_PREFIX /home/mbartlet/.conda/envs/py3 from IPython.display import Markdown as md from os import environ as env SCRATCH_ENV = f \"/scratch/ { env [ 'USER' ] } /home_conda_clone\" body = f \"\"\" ```bash #!/usr/bin/env bash #SBATCH --account { env [ 'DEFAULT_HANDLE' ] } #SBATCH --time 5 #SBATCH --partition debug #SBATCH --nodes 2 export SCRATCH_ENV=\" { SCRATCH_ENV } \" rsync -avz --ignore-existing \" { env [ 'CONDA_HOME_ENV_PREFIX' ] } \" \"$SCRATCH_ENV\" &>/dev/null srun bash -l <<EOF module purge module load conda . activate \"$SCRATCH_ENV\" which python EOF rm -rf \"$SCRATCH_ENV\" # Optional clean-up \"\"\" #!/usr/bin/env bash #SBATCH --account wks #SBATCH --time 5 #SBATCH --partition debug #SBATCH --nodes 2 export SCRATCH_ENV = \"/scratch/mbartlet/home_conda_clone\" rsync -avz --ignore-existing \"/home/mbartlet/.conda/envs/py3\" \" $SCRATCH_ENV \" & >/dev/null srun bash -l <<EOF module purge module load conda . activate \"$SCRATCH_ENV\" which python EOF rm -rf \" $SCRATCH_ENV \" # Optional clean-up And after running what was generated above: [ mbartlet@el1 ~ ] $ cat slurm-1845968.out /scratch/mbartlet/home_conda_clone/bin/python /scratch/mbartlet/home_conda_clone/bin/python Which shows both nodes sourced the environment from /scratch","title":"NREL HPC Python"},{"location":"Documentation/languages/python/NREL_python/#python-on-nrel-hpc","text":"By design, the HPC is a time-shared multi-machine system which necessarily warrants some nuanced consideration for how environments are managed relative to a single machine with a single user. Sometimes, the default workflow for environment creation and usage is not the most optimal for some use-cases. Below is a list of common pitfalls that users have encountered historically while using Python and Anaconda on NREL HPC. Running a SLURM job that uses a conda environment which is stored in $HOME . Exhausting the $HOME storage quota (50GB on the current HPC system) usually because of conda's package cache combined with their user environments. Trying to share a conda environment from another user's /home directory. Forgetting to install jupyter in a new conda environment, resulting in using the base installation's version which doesn't have your dependencies installed. Let's discuss strategies to mitigate or avoid these kinds of problems","title":"Python on NREL HPC"},{"location":"Documentation/languages/python/NREL_python/#installing-conda-environments-in-different-directories","text":"By default, conda will install new environments in $HOME/.conda . Generally speaking, this a sensible default\u2014it just happens to be the starting point to frequent issues that users have experienced historically. Something to consider is that conda has a --prefix flag which allows one to arbitrate where a conda environment gets installed to, notably allowing you to place environments on other file-systems and block devices besides the /home network-storage that is mounted on NREL HPC systems. For example, here is how one might create a project in their /scratch directory: ENV_PREFIX = \"/scratch/$USER/demo_scratch_env\" import os ; os . environ [ 'ENV_PREFIX' ] = ENV_PREFIX # Export this variable for cells below ! conda create -- quiet -- use - local -- yes \\ -- prefix $ ENV_PREFIX # `--prefix` in action \\ python = 3.7 Collecting package metadata: ...working... done Solving environment: ...working... done ## Package Plan ## environment location: /scratch/mbartlet/demo_scratch_env Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done ! ls - ld $ ENV_PREFIX drwxr-xr-x. 3 mbartlet mbartlet 4096 Dec 3 11:10 /scratch/mbartlet/demo_scratch_env # Delete the demo environment for cleanliness ! conda - env remove -- yes -- quiet -- prefix $ ENV_PREFIX &>/ dev / null Below is a table which discusses the pros and cons of each block-device mount on NREL HPC as a location for storing your software environments. Block-device mounts Situations where you would want to use this block device for your conda environments Caveats to consider when using this mount /home $HOME/.conda is the default location for environments. For one-off environments, or if you don't create environments often, this is a reasonable location for your environments and doesn't require any extra flags or parameters. Files in \\$ HOME will not be purged so long as you have an active NREL HPC account. However, \\$ HOME is limited to a 50GB storage quota so you may have to take care to monitor your storage footprint. /scratch /scratch or /projects is ultimately where you want your environment to end up if your jobs have more than 1 node\u2014if your environment is in /home then every node in your job will be competing for read-access over a non-parallel network fabric to the source files of your environment. /scratch provides simultaneous access to all the nodes. A sensible approach is copying your environments from /home to /scratch as part of your job's initialization. /scratch storage is unlimited. /scratch is a parallel filesystem, meaning simultaneous filesystem operations by several nodes is possible and performant. However, the contents of /scratch are subject to purge after 28 days of inactivity. /projects This is a great place to put a conda environment that you anticipate sharing with your colleagues who are also working on the project. You can structure the permissions such that others in the project have read-only, write-only, or no access (we also encourage restoring these permissions at a later date so others on the project can manage your files without a hassle). /projects is also a parallel filesystem which reaps the same benefits as mentioned above. However, access to projects is contingent on having access to an HPC project allocation. Moreover, the storage quota allotted to each project is relative to the reasonableness of its requested needs, although a conda environment is very unlikely to have a significant storage footprint. As mentioned above, let's demonstrate one might go about copying an environment from /home to /scratch in a SLURM job. The below cell will generate a nice code block based on variables used earlier in this notebook, as well as environment variables within your user account: # Acquire a default project handle to procedurally generate a SLURM job import subprocess command = \"/nopt/nrel/utils/bin/hours_report | tail -1 | awk '{print $1}'\" # Grab a valid project handle command_array = [ '/bin/bash' , '-c' , command ] project_handle = subprocess . run ( command_array , stdout = subprocess . PIPE ) . stdout . decode ( 'utf-8' )[: - 1 ] import os ; os . environ [ 'DEFAULT_HANDLE' ] = project_handle # Export handle for cells below ! echo $ DEFAULT_HANDLE wks conda_home_env = \"py3\" # Acquire info about the default conda environment import subprocess command = f \"module load conda && . activate { conda_home_env } && echo $CONDA_PREFIX\" command_array = [ '/bin/bash' , '-lc' , # Have to run this from a login-shell command ] conda_home_env_prefix = subprocess . run ( command_array , stdout = subprocess . PIPE ) . stdout . decode ( 'utf-8' )[: - 1 ] import os ; os . environ [ 'CONDA_HOME_ENV_PREFIX' ] = conda_home_env_prefix # Export handle for cells below ! echo $ CONDA_HOME_ENV_PREFIX /home/mbartlet/.conda/envs/py3 from IPython.display import Markdown as md from os import environ as env SCRATCH_ENV = f \"/scratch/ { env [ 'USER' ] } /home_conda_clone\" body = f \"\"\" ```bash #!/usr/bin/env bash #SBATCH --account { env [ 'DEFAULT_HANDLE' ] } #SBATCH --time 5 #SBATCH --partition debug #SBATCH --nodes 2 export SCRATCH_ENV=\" { SCRATCH_ENV } \" rsync -avz --ignore-existing \" { env [ 'CONDA_HOME_ENV_PREFIX' ] } \" \"$SCRATCH_ENV\" &>/dev/null srun bash -l <<EOF module purge module load conda . activate \"$SCRATCH_ENV\" which python EOF rm -rf \"$SCRATCH_ENV\" # Optional clean-up \"\"\" #!/usr/bin/env bash #SBATCH --account wks #SBATCH --time 5 #SBATCH --partition debug #SBATCH --nodes 2 export SCRATCH_ENV = \"/scratch/mbartlet/home_conda_clone\" rsync -avz --ignore-existing \"/home/mbartlet/.conda/envs/py3\" \" $SCRATCH_ENV \" & >/dev/null srun bash -l <<EOF module purge module load conda . activate \"$SCRATCH_ENV\" which python EOF rm -rf \" $SCRATCH_ENV \" # Optional clean-up And after running what was generated above: [ mbartlet@el1 ~ ] $ cat slurm-1845968.out /scratch/mbartlet/home_conda_clone/bin/python /scratch/mbartlet/home_conda_clone/bin/python Which shows both nodes sourced the environment from /scratch","title":"Installing Conda Environments in Different Directories"},{"location":"Documentation/languages/python/dask/dask/","text":"Dask Dask provides a way to parallelize Python code either on a single node or across the cluster. It is similar to the functionality provided by Apache Spark, with easier setup. It provides a similar API to other common Python packages such as NumPY, Pandas, and others. Dask single node Dask can be used locally on your laptop or an individual node. Additionally, it provides wrappers for multiprocessing and threadpools. The advantage of using LocalCluster though is you can easily drop in another cluster configuration to further parallelize. from distributed import Client , LocalCluster import dask import time import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): cluster = LocalCluster ( n_workers = 2 ) client = Client ( cluster ) zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main () Dask MPI Dask-MPI can be used to parallelize calculations across a number of nodes as part of a batch job submitted to slurm. Dask will automatically create a scheduler on rank 0 and workers will be created on all other ranks. Install Note: The version of dask-mpi installed via Conda may be incompatible with the MPI libaries on Eagle. Use the pip install instead. conda create -n daskmpi python=3.7 conda activate daskmpi pip install dask-mpi Python script : This script holds the calculation to be performed in the test function. The script relies on the Dask cluster setup on MPI which is created in the from distributed import Client , LocalCluster import dask import time from dask_mpi import initialize import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): initialize ( nanny = False , interface = 'ib0' , protocol = 'tcp' , memory_limit = 0.8 , local_directory = '/tmp/scratch/dask' , nthreads = 1 ) client = Client () zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main () Running the above script with MPI will automatically set a Dask worker on each MPI rank. mpiexec -np 30 python dask_mpi.py Dask jobqueue Dask can also run using the Slurm scheduler already installed on Eagle. The Jobqueue library can handle submission of a computation to the cluster. This is particularly useful when running an interactive notebook or similar and you need to scale workers. import dask import time from dask_jobqueue import SLURMCluster from distributed import Client import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): cluster = SLURMCluster ( cores = 18 , memory = '24GB' , queue = 'short' , project = 'hpcapps' , walltime = '00:30:00' , interface = 'ib0' , processes = 18 , ) cluster . scale ( jobs = 2 ) client = Client ( cluster ) zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main () References Dask documentation Dask Jobqueue Dask MPI","title":"Dask"},{"location":"Documentation/languages/python/dask/dask/#dask","text":"Dask provides a way to parallelize Python code either on a single node or across the cluster. It is similar to the functionality provided by Apache Spark, with easier setup. It provides a similar API to other common Python packages such as NumPY, Pandas, and others.","title":"Dask"},{"location":"Documentation/languages/python/dask/dask/#dask-single-node","text":"Dask can be used locally on your laptop or an individual node. Additionally, it provides wrappers for multiprocessing and threadpools. The advantage of using LocalCluster though is you can easily drop in another cluster configuration to further parallelize. from distributed import Client , LocalCluster import dask import time import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): cluster = LocalCluster ( n_workers = 2 ) client = Client ( cluster ) zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main ()","title":"Dask single node"},{"location":"Documentation/languages/python/dask/dask/#dask-mpi","text":"Dask-MPI can be used to parallelize calculations across a number of nodes as part of a batch job submitted to slurm. Dask will automatically create a scheduler on rank 0 and workers will be created on all other ranks.","title":"Dask MPI"},{"location":"Documentation/languages/python/dask/dask/#install","text":"Note: The version of dask-mpi installed via Conda may be incompatible with the MPI libaries on Eagle. Use the pip install instead. conda create -n daskmpi python=3.7 conda activate daskmpi pip install dask-mpi Python script : This script holds the calculation to be performed in the test function. The script relies on the Dask cluster setup on MPI which is created in the from distributed import Client , LocalCluster import dask import time from dask_mpi import initialize import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): initialize ( nanny = False , interface = 'ib0' , protocol = 'tcp' , memory_limit = 0.8 , local_directory = '/tmp/scratch/dask' , nthreads = 1 ) client = Client () zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main () Running the above script with MPI will automatically set a Dask worker on each MPI rank. mpiexec -np 30 python dask_mpi.py","title":"Install"},{"location":"Documentation/languages/python/dask/dask/#dask-jobqueue","text":"Dask can also run using the Slurm scheduler already installed on Eagle. The Jobqueue library can handle submission of a computation to the cluster. This is particularly useful when running an interactive notebook or similar and you need to scale workers. import dask import time from dask_jobqueue import SLURMCluster from distributed import Client import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): cluster = SLURMCluster ( cores = 18 , memory = '24GB' , queue = 'short' , project = 'hpcapps' , walltime = '00:30:00' , interface = 'ib0' , processes = 18 , ) cluster . scale ( jobs = 2 ) client = Client ( cluster ) zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main ()","title":"Dask jobqueue"},{"location":"Documentation/languages/python/dask/dask/#references","text":"Dask documentation Dask Jobqueue Dask MPI","title":"References"},{"location":"blog/2020-12-01-numba/","text":"Numba is a just in time (JIT) compiler for Python and NumPy code. From their official website, \"Numba translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library. Numba-compiled numerical algorithms in Python can approach the speeds of C or FORTRAN.\" @jit ( nopython = True ) def function_to_be_compiled (): # Standard numerical/NumPy code here ... Importantly, many functions require no changes or refactoring to gain this speedup. In this getting-started guide , we build an example environment on Eagle, test the performance of a Numba-compiled function using the most common implementation of the @jit decorator, and discuss what sorts of functions will see performance improvements when compiled.","title":"Speeding up Python Code with Numba"},{"location":"blog/2021-05-06-tf/","text":"TensorFlow is a widely used and powerful symbolic math library commonly used for a variety of machine learning techniques. TensorFlow has built in API support for regression, clustering, classification, hidden Markov models, neural networks, reinforcement learning, as well as, a variety of activation functions, loss function, and optimizers. TensorFlow has received growing adoption among scientists, researchers, and industry professionals for its broad applicability and flexibility. TensorFlow versions obtained from pip or conda installs may not be optimized for the CPU and GPU architectures found on Eagle. To address this, pre-compiled versions which are optimized both for the CPU and GPU architectures have been created and offer computational benefits compared to other installation approaches. These versions can easily be installed from the wheels provided in /nopt/nrel/apps/wheels/ which contains different TensorFlow versions. Here is an example of how you can install an optimized version of TensorFlow to your environment. pip install --upgrade --no-deps --force-reinstall /nopt/nrel/apps/wheels/tensorflow-2.4.0-cp38-cp38-linux_x86_64.whl These builds provide a significant advantage as illustrated below over the standard conda install of TensorFlow. A recent tutorial was given on this topic, for more information see the recording or checkout the tutorial materials","title":"Faster Machine Learning with Custom Built TensorFlow on Eagle"},{"location":"blog/2021-06-18-srun/","text":"Subjects covered Basics Pointers to Examples Why not just use mpiexec/mpirun? Simple runs Threaded (OpenMP) runs Hybrid MPI/OpenMPI MPMD - a simple distribution MPMD multinode 1. Basics Eagle uses the Slurm scheduler and applications run on a compute node must be run via the scheduler. For batch runs users write a script and submit the script using the sbatch command. The script tells the scheduler what resources are required including a limit on the time to run. The script also normally contains \"charging\" or account information. Here is a very basic script that just runs hostname to list the nodes allocated for a job. #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --time=00:01:00 #SBATCH --account=hpcapps srun hostname Note we used the srun command to launch multiple (parallel) instances of our application hostname . This article primarily discusses options for the srun command to enable good parallel execution. In the script above we have asked for two nodes --nodes=2 and each node will run a single instance of hostname --ntasks-per-node=1 . If srun is not given options on the command line it will determine the number of tasks to run from the arguments in the header. Thus our output from the script given above will be two lines, a list of nodes allocated for the job. 2. Pointers to examples The page https://www.nrel.gov/hpc/eagle-batch-jobs.html has information about running jobs under Slurm including a link to example batch scripts. The page https://github.com/NREL/HPC/tree/master/slurm has many slurm examples ranging from simple to complex. This article is based on the second page. 3. Why not just use mpiexec/mpirun? The srun command is an integral part of the Slurm scheduling system. It \"knows\" the configuration of the machine and recognizes the environmental variables set by the scheduler, such as cores per nodes. Mpiexec and mpirun come with the MPI compilers. The amount of integration with the scheduler is implementation and install methodology dependent. They may not enable the best performance for your applications. In some cases they flat out just don't work correctly on Eagle. For example, when trying to run MPMD applications (different programs running on different cores) using the mpt version of mpiexec, the same programs gets launched on all cores. 4. Simple runs For our srun examples we will use two glorified \"Hello World\" programs, one in Fortran and the other in C. They are essentially the same program written in the two languages. They can be compiled as MPI, OpenMP, or as hybrid MPI/OpenMP. They are available from the NREL HPC repository https://github.com/NREL/HPC.git in the slurm/source directory or by running the wget commands shown below. wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/fhostone.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/mympi.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/phostone.c wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/makehello -O makefile After the files are downloaded you can build the programs using the mpt MPI compilers module purge module load mpt gcc/10.1.0 make or using Intel MPI compilers module purge module load intel-mpi gcc/10.1.0 make You will end up with the executables: fomp - Fortran Openmp program fhybrid - Fortran hybrid MPI/Openmp program fmpi - Fortran MPI program comp - C hybrid Openmp program chybrid - C hybrid MPI/Openmp program cmpi - C MPI program These programs have many options. Running with the command line option -h will show them. Not all options are applicable for all versions. Run without options the programs just print the hostname on which they were run. We look at our simple example again. Here we ask for 2 nodes, 4 tasks per node for a total of 8 tasks. #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 #SBATCH --ntasks=8 #SBATCH --time=00:10:00 srun ./cmpi This will produce (sorted) output like: r105u33 r105u33 r105u33 r105u33 r105u37 r105u37 r105u37 r105u37 In the above script we have nodes,ntasks-per-node and ntasks. You do not need to specify all three parameters but values that are specified must be consistent. If nodes is not specified it will default to 1. If ntasks is not specified it will default to 1 tasks per node. You can put --ntasks-per-node and/or --ntasks on the srun line. For example, to run a total of 9 tasks, 5 on one node and 4 on the second: #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --nodes=2 #SBATCH --time=00:10:00 srun --ntasks=9 ./cmpi 5. Threaded (OpenMP) runs The variable used to tell the operating system how many threads to use for an OpenMP program is OMP_NUM_THREADS. In the ideal world you could just set OMP_NUM_THREADS to a value, say 36, the number of cores on each Eagle node, and each thread would be assigned to a core. Unfortunately without setting additional variables you will get the requested number of threads but threads might not be spread across all cores. This can result in a significant slowdown. For a program that is computationally intensive if two threads get mapped to the same core the runtime will increase 100%. If all threads end up on the same core, the slowdown could actually be greater than the number of cores. Our example programs, phostone.c and fhostone.f90, have a nice feature. If you add -F to the command line they will produce a report showing on which core each thread runs. We are going to look at the C version of the code and compile it with both the Intel version of C, icc and with the Gnu compiler gcc. ml comp-intel/2020.1.217 gcc/10.1.0 gcc -fopenmp -DNOMPI phostone.c -o comp.gcc icc -fopenmp -DNOMPI phostone.c -o comp.icc Run the script... #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --cpus-per-task=36 ## ask for 10 minutes #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug export OMP_NUM_THREADS=36 srun ./comp.gcc -F > gcc.out srun ./comp.gcc -F > icc.out Note we have added the line #SBATCH --cpus-per-task=36 . cpus-per-task should match the value of OMP_NUM_THREADS. We now look at the sorted head of each of the output files el3:nslurm> cat icc.out | sort -k6,6 task thread node name first task # on node core 0000 0030 r5i7n35 0000 0000 0000 0000 0001 r5i7n35 0000 0000 0001 0000 0034 r5i7n35 0000 0000 0001 0000 0002 r5i7n35 0000 0000 0002 0000 0035 r5i7n35 0000 0000 0002 0000 0032 r5i7n35 0000 0000 0003 . . . el3:nslurm> cat gcc.out | sort -k6,6 task thread node name first task # on node core 0000 0031 r5i7n35 0000 0000 0000 0000 0001 r5i7n35 0000 0000 0001 0000 0002 r5i7n35 0000 0000 0002 0000 0034 r5i7n35 0000 0000 0002 0000 0003 r5i7n35 0000 0000 0003 0000 0004 r5i7n35 0000 0000 0004 . . . The last column shows the core on which a thread is run. We see that there is duplication of cores, potentially leading to poor performance. There are two sets of environmental variables that can be used to map threads to cores. One variable is specific to the Intel compilers, KMP_AFFINITY. The others are general for OpenMP compilers and should work for any OpenMP compiler, OMP_PLACES and OMP_PROC_BIND. These are documented at: https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html https://www.openmp.org/spec-html/5.0/openmpse52.html https://www.openmp.org/spec-html/5.0/openmpse53.html We ran each version of our code 100 times with 5 different settings. The settings were: export KMP_AFFINITY=verbose,scatter export KMP_AFFINITY=verbose,compact export OMP_PLACES=cores export OMP_PROC_BIND=spread export OMP_PLACES=cores export OMP_PROC_BIND=close NONE The table below shows the results of our runs. In particular, it shows the minimum number of cores used with the particular settings. 36 is the desired value. We see that for gcc the following settings worked well: export OMP_PLACES=cores export OMP_PROC_BIND=spread or export OMP_PLACES=cores export OMP_PROC_BIND=clone Setting KMP_AFFINITY did not work for gcc but for the Intel compiler KMP_AFFINITY also gave good results. Compiler Setting Worked min cores mean cores max cores gcc cores, close yes 36 36 36 gcc cores, spread yes 36 36 36 gcc KMP_AFFINITY=compact no 25 34.18 36 gcc KMP_AFFINITY=scatter no 26 34.56 36 gcc none no 28 34.14 36 icc cores, close yes 36 36 36 icc cores, spread yes 36 36 36 icc KMP_AFFINITY=compact yes 36 36 36 icc KMP_AFFINITY=scatter yes 36 36 36 icc none no 19 23.56 29 So our final working script for OpenMP programs could be: #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --cpus-per-task=36 ## ask for 10 minutes #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug export OMP_NUM_THREADS=36 export OMP_PLACES=cores export OMP_PROC_BIND=close #export OMP_PROC_BIND=spread srun ./comp.gcc -F > gcc.out srun ./comp.gcc -F > icc.out When a job is run the SLURM_CPUS_PER_TASK is set to cpus-per-task so you may want to export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK More on this in the next section. 6. Hybrid MPI/OpenMPI The next script is just an extension of the last. We now run hybrid, a combination of MPI and OpenMP. Our base example programs, fhostame.f90 and phostname.c can be compiled in hybrid mode as well as in pure MPI and pure OpenMP. First we look at the (sorted) output from our program run in hybrid mode with 4 tasks on two nodes and 4 threads. MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r5i0n4 0000 0000 0000 0000 0001 r5i0n4 0000 0000 0004 0000 0002 r5i0n4 0000 0000 0009 0000 0003 r5i0n4 0000 0000 0014 0001 0000 r5i0n4 0000 0001 0018 0001 0001 r5i0n4 0000 0001 0022 0001 0002 r5i0n4 0000 0001 0027 0001 0003 r5i0n4 0000 0001 0032 0002 0000 r5i0n28 0002 0000 0000 0002 0001 r5i0n28 0002 0000 0004 0002 0002 r5i0n28 0002 0000 0009 0002 0003 r5i0n28 0002 0000 0014 0003 0000 r5i0n28 0002 0001 0018 0003 0001 r5i0n28 0002 0001 0022 0003 0002 r5i0n28 0002 0001 0027 0003 0003 r5i0n28 0002 0001 0032 total time 3.009 The first column is the MPI task number followed by the thread, then the node. The last column is the core on which that give task/thread was run. We can cat a list of unique combinations of nodes and cores by piping the file into grep ^0 | awk '{print $3, $6}' | sort -u | wc -l` We get 16 which is the number of tasks times the number of threads. That is, we have each task/thread assigned to its own core. This will give good performance. The script below runs on a fixed number of tasks (4 = 2 per node * 2 nodes) and using from 1 to cpus-per-task=18 threads. The variable SLURM_CPUS_PER_TASK is set by slurm to be cpus-per-task. After the srun line we post process the output to report core usage. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=2 #SBATCH --partition=short #SBATCH --cpus-per-task=18 #SBATCH --ntasks=4 module purge module load intel-mpi/2020.1.217 gcc/10.1.0 export OMP_PLACES=cores export OMP_PROC_BIND=spread echo \"CPT TASKS THREADS cores\" for n in `seq 1 $SLURM_CPUS_PER_TASK` ; do request=`python -c \"print($n*$SLURM_NTASKS)\"` have=72 if ((request <= have)); then export OMP_NUM_THREADS=$n srun --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc -F -t 3 > out.$SLURM_NTASKS.$OMP_NUM_THREADS # post process cores=`cat out.$SLURM_NTASKS.$OMP_NUM_THREADS | grep ^0 | awk '{print $3, $6}' | sort -u | wc -l` echo $SLURM_CPUS_PER_TASK \" \" $SLURM_NTASKS \" \" $OMP_NUM_THREADS \" \" $cores fi done Our final output from this script is: el3:stuff> cat slurm-7002718.out CPT TASKS THREADS cores 18 4 1 4 18 4 2 8 18 4 3 12 18 4 4 16 18 4 5 20 18 4 6 24 18 4 7 28 18 4 8 32 18 4 9 36 18 4 10 40 18 4 11 44 18 4 12 48 18 4 13 52 18 4 14 56 18 4 15 60 18 4 16 64 18 4 17 68 18 4 18 72 el3:stuff> The important lines are: #SBATCH --cpus-per-task=18 . . . export OMP_PLACES=cores export OMP_PROC_BIND=spread . . . srun --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc We need to set cpus-per-task to tell slurm we are going to run multithreaded and how many cores we are going to use for our threads. This should be set to the maximum number of threads per task we expect to use. We use the OMP variables to map threads to cores. IMPORTANT: using KMP_AFFINTY will not give the desired results. It will cause all threads for a task to be mapped to a single core. We can run this script for hybrid MPI/OpenMP programs as is or set the number of cpus-per-task and tasks on the sbatch command line. For example: sbatch --cpus-per-task=9 --ntasks=8 simple gives us: el3:stuff> cat slurm-7002858.out CPT TASKS THREADS cores 9 8 1 8 9 8 2 16 9 8 3 24 9 8 4 32 9 8 5 40 9 8 6 48 9 8 7 56 9 8 8 64 9 8 9 72 el3:stuff> 7. MPMD - a simple distribution Here we look at launching Multi Program Multi Data runs. We use a the --multi-prog option with srun. This involves creating a config_file that lists the programs we are going to run along with the task ID. See: https://computing.llnl.gov/tutorials/linux_clusters/multi-prog.html for a quick description of the format for the config_file. Here we create the file on the fly but it could be done beforehand. We have two MPI programs to run together, phostone and fhostone. They are actually the same program written in C and Fortran. In the real world MPMD applications would maybe run a GUI or a manager for one task and rest doing compute. The syntax for running MPMD programs is srun --multi-prog mapfile where mapfile is a config_file that lists the programs to run. It is possible to pass different arguments to each program as discussed in the link above. Here we just add command line arguments for task 0. Our mapfile has 8 programs listed. The even tasks are running phostone and the odd fhostone. Our script uses two for loops to add lines to the mapfile and then uses sed to append command line arguments to the first line. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug #SBATCH --cpus-per-task=1 # create our mapfile app1=./phostone for n in 0 2 4 6 ; do echo $n $app1 >> mapfile done app2=./fhostone for n in 1 3 5 7 ; do echo $n $app2 >> mapfile done # add a command line option to the first line # sed does an in-place change to the first line # of our mapfile adding *-F* sed -i \"1 s/$/ -F /\" mapfile cat mapfile export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK srun -n8 --multi-prog mapfile Here is the complete output including the mapfile and output from our two programs. Lines with three digits for core number were created by the Fortran version of the program. el3:stuff> cat *7003104* 0 ./phostone -F 2 ./phostone 4 ./phostone 6 ./phostone 1 ./fhostone 3 ./fhostone 5 ./fhostone 7 ./fhostone MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r1i7n35 0000 0000 0022 0001 0000 r1i7n35 0001 0000 021 0002 0000 r1i7n35 0000 0001 0027 0003 0000 r1i7n35 0003 0000 023 0004 0000 r1i7n35 0000 0002 0020 0005 0000 r1i7n35 0005 0000 025 0006 0000 r1i7n35 0000 0003 0026 0007 0000 r1i7n35 0007 0000 019 el3:stuff> 8. MPMD multinode Our final example again just extends the previous one. We want to add the capability to launch different numbers of tasks on a set of nodes and at the same time have different programs on each of the nodes. We create a mapfile to list the programs to run as was done above. In this case for illustration purposes we are running one copy of phostone and seven instances of fhostone. We add to that a hostfile that lists the nodes on which to run. The hostfile has one host per MPI task. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=2 #SBATCH --partition=debug export OMP_NUM_THREADS=1 # Create our mapfile rm -rf mapfile app1=./phostone for n in 0 ; do echo $n $app1 >> mapfile done app2=./fhostone for n in 1 2 3 4 5 6 7 ; do echo $n $app2 >> mapfile done # Add a command line option to the first line # sed does an in-place change to the first line # of our mapfile adding *-F* sed -i \"1 s/$/ -F /\" mapfile # Count of each app to run on a node counts=\"1 7\" # Get a list of nodes on a single line nodes=`scontrol show hostnames | tr '\\n' ' '` # Create our hostfile and tell slrum its name export SLURM_HOSTFILE=hostlist # It is possible to do this in bash but # I think this is easier to understand # in python. It uses the values for # counts and nodes set above. python - > $SLURM_HOSTFILE << EOF c=\"$counts\".split() nodes=\"$nodes\".split() k=0 for i in c: i=int(i) node=nodes[k] for j in range(0,i): print(node) k=k+1 EOF srun -n 8 --multi-prog mapfile Here is the output from our run including the mapfile and hostlist. Notice that the first instance of the set of running programs is the C version. It is the only thing running on the first nodes. The rest of the MPI tasks are the Fortran version of the program running on the second node. el3:stuff> cat slurm-7003587.out | sort -k3,3 -k1,1 MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r102u34 0000 0000 0004 0001 0000 r102u35 0001 0000 003 0002 0000 r102u35 0002 0000 000 0003 0000 r102u35 0001 0001 006 0004 0000 r102u35 0004 0000 007 0005 0000 r102u35 0001 0002 004 0006 0000 r102u35 0002 0001 005 0007 0000 r102u35 0001 0003 002 el3:stuff> cat mapfile 0 ./phostone -F 1 ./fhostone 2 ./fhostone 3 ./fhostone 4 ./fhostone 5 ./fhostone 6 ./fhostone 7 ./fhostone el3:stuff> c el3:stuff> cat hostlist r102u34 r102u35 r102u35 r102u35 r102u35 r102u35 r102u35 r102u35 el3:stuff>","title":"Using srun to launch applications under slurm"},{"location":"blog/2021-06-18-srun/#subjects-covered","text":"Basics Pointers to Examples Why not just use mpiexec/mpirun? Simple runs Threaded (OpenMP) runs Hybrid MPI/OpenMPI MPMD - a simple distribution MPMD multinode","title":"Subjects covered"},{"location":"blog/2021-06-18-srun/#1-basics","text":"Eagle uses the Slurm scheduler and applications run on a compute node must be run via the scheduler. For batch runs users write a script and submit the script using the sbatch command. The script tells the scheduler what resources are required including a limit on the time to run. The script also normally contains \"charging\" or account information. Here is a very basic script that just runs hostname to list the nodes allocated for a job. #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --time=00:01:00 #SBATCH --account=hpcapps srun hostname Note we used the srun command to launch multiple (parallel) instances of our application hostname . This article primarily discusses options for the srun command to enable good parallel execution. In the script above we have asked for two nodes --nodes=2 and each node will run a single instance of hostname --ntasks-per-node=1 . If srun is not given options on the command line it will determine the number of tasks to run from the arguments in the header. Thus our output from the script given above will be two lines, a list of nodes allocated for the job.","title":"1. Basics"},{"location":"blog/2021-06-18-srun/#2-pointers-to-examples","text":"The page https://www.nrel.gov/hpc/eagle-batch-jobs.html has information about running jobs under Slurm including a link to example batch scripts. The page https://github.com/NREL/HPC/tree/master/slurm has many slurm examples ranging from simple to complex. This article is based on the second page.","title":"2. Pointers to examples"},{"location":"blog/2021-06-18-srun/#3-why-not-just-use-mpiexecmpirun","text":"The srun command is an integral part of the Slurm scheduling system. It \"knows\" the configuration of the machine and recognizes the environmental variables set by the scheduler, such as cores per nodes. Mpiexec and mpirun come with the MPI compilers. The amount of integration with the scheduler is implementation and install methodology dependent. They may not enable the best performance for your applications. In some cases they flat out just don't work correctly on Eagle. For example, when trying to run MPMD applications (different programs running on different cores) using the mpt version of mpiexec, the same programs gets launched on all cores.","title":"3. Why not just use mpiexec/mpirun?"},{"location":"blog/2021-06-18-srun/#4-simple-runs","text":"For our srun examples we will use two glorified \"Hello World\" programs, one in Fortran and the other in C. They are essentially the same program written in the two languages. They can be compiled as MPI, OpenMP, or as hybrid MPI/OpenMP. They are available from the NREL HPC repository https://github.com/NREL/HPC.git in the slurm/source directory or by running the wget commands shown below. wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/fhostone.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/mympi.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/phostone.c wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/makehello -O makefile After the files are downloaded you can build the programs","title":"4. Simple runs"},{"location":"blog/2021-06-18-srun/#using-the-mpt-mpi-compilers","text":"module purge module load mpt gcc/10.1.0 make","title":"using the mpt MPI compilers"},{"location":"blog/2021-06-18-srun/#or-using-intel-mpi-compilers","text":"module purge module load intel-mpi gcc/10.1.0 make You will end up with the executables: fomp - Fortran Openmp program fhybrid - Fortran hybrid MPI/Openmp program fmpi - Fortran MPI program comp - C hybrid Openmp program chybrid - C hybrid MPI/Openmp program cmpi - C MPI program These programs have many options. Running with the command line option -h will show them. Not all options are applicable for all versions. Run without options the programs just print the hostname on which they were run. We look at our simple example again. Here we ask for 2 nodes, 4 tasks per node for a total of 8 tasks. #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 #SBATCH --ntasks=8 #SBATCH --time=00:10:00 srun ./cmpi This will produce (sorted) output like: r105u33 r105u33 r105u33 r105u33 r105u37 r105u37 r105u37 r105u37 In the above script we have nodes,ntasks-per-node and ntasks. You do not need to specify all three parameters but values that are specified must be consistent. If nodes is not specified it will default to 1. If ntasks is not specified it will default to 1 tasks per node. You can put --ntasks-per-node and/or --ntasks on the srun line. For example, to run a total of 9 tasks, 5 on one node and 4 on the second: #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --nodes=2 #SBATCH --time=00:10:00 srun --ntasks=9 ./cmpi","title":"or using Intel MPI compilers"},{"location":"blog/2021-06-18-srun/#5-threaded-openmp-runs","text":"The variable used to tell the operating system how many threads to use for an OpenMP program is OMP_NUM_THREADS. In the ideal world you could just set OMP_NUM_THREADS to a value, say 36, the number of cores on each Eagle node, and each thread would be assigned to a core. Unfortunately without setting additional variables you will get the requested number of threads but threads might not be spread across all cores. This can result in a significant slowdown. For a program that is computationally intensive if two threads get mapped to the same core the runtime will increase 100%. If all threads end up on the same core, the slowdown could actually be greater than the number of cores. Our example programs, phostone.c and fhostone.f90, have a nice feature. If you add -F to the command line they will produce a report showing on which core each thread runs. We are going to look at the C version of the code and compile it with both the Intel version of C, icc and with the Gnu compiler gcc. ml comp-intel/2020.1.217 gcc/10.1.0 gcc -fopenmp -DNOMPI phostone.c -o comp.gcc icc -fopenmp -DNOMPI phostone.c -o comp.icc Run the script... #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --cpus-per-task=36 ## ask for 10 minutes #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug export OMP_NUM_THREADS=36 srun ./comp.gcc -F > gcc.out srun ./comp.gcc -F > icc.out Note we have added the line #SBATCH --cpus-per-task=36 . cpus-per-task should match the value of OMP_NUM_THREADS. We now look at the sorted head of each of the output files el3:nslurm> cat icc.out | sort -k6,6 task thread node name first task # on node core 0000 0030 r5i7n35 0000 0000 0000 0000 0001 r5i7n35 0000 0000 0001 0000 0034 r5i7n35 0000 0000 0001 0000 0002 r5i7n35 0000 0000 0002 0000 0035 r5i7n35 0000 0000 0002 0000 0032 r5i7n35 0000 0000 0003 . . . el3:nslurm> cat gcc.out | sort -k6,6 task thread node name first task # on node core 0000 0031 r5i7n35 0000 0000 0000 0000 0001 r5i7n35 0000 0000 0001 0000 0002 r5i7n35 0000 0000 0002 0000 0034 r5i7n35 0000 0000 0002 0000 0003 r5i7n35 0000 0000 0003 0000 0004 r5i7n35 0000 0000 0004 . . . The last column shows the core on which a thread is run. We see that there is duplication of cores, potentially leading to poor performance. There are two sets of environmental variables that can be used to map threads to cores. One variable is specific to the Intel compilers, KMP_AFFINITY. The others are general for OpenMP compilers and should work for any OpenMP compiler, OMP_PLACES and OMP_PROC_BIND. These are documented at: https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html https://www.openmp.org/spec-html/5.0/openmpse52.html https://www.openmp.org/spec-html/5.0/openmpse53.html We ran each version of our code 100 times with 5 different settings. The settings were: export KMP_AFFINITY=verbose,scatter export KMP_AFFINITY=verbose,compact export OMP_PLACES=cores export OMP_PROC_BIND=spread export OMP_PLACES=cores export OMP_PROC_BIND=close NONE The table below shows the results of our runs. In particular, it shows the minimum number of cores used with the particular settings. 36 is the desired value. We see that for gcc the following settings worked well: export OMP_PLACES=cores export OMP_PROC_BIND=spread or export OMP_PLACES=cores export OMP_PROC_BIND=clone Setting KMP_AFFINITY did not work for gcc but for the Intel compiler KMP_AFFINITY also gave good results. Compiler Setting Worked min cores mean cores max cores gcc cores, close yes 36 36 36 gcc cores, spread yes 36 36 36 gcc KMP_AFFINITY=compact no 25 34.18 36 gcc KMP_AFFINITY=scatter no 26 34.56 36 gcc none no 28 34.14 36 icc cores, close yes 36 36 36 icc cores, spread yes 36 36 36 icc KMP_AFFINITY=compact yes 36 36 36 icc KMP_AFFINITY=scatter yes 36 36 36 icc none no 19 23.56 29 So our final working script for OpenMP programs could be: #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --cpus-per-task=36 ## ask for 10 minutes #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug export OMP_NUM_THREADS=36 export OMP_PLACES=cores export OMP_PROC_BIND=close #export OMP_PROC_BIND=spread srun ./comp.gcc -F > gcc.out srun ./comp.gcc -F > icc.out When a job is run the SLURM_CPUS_PER_TASK is set to cpus-per-task so you may want to export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK More on this in the next section.","title":"5. Threaded (OpenMP) runs"},{"location":"blog/2021-06-18-srun/#6-hybrid-mpiopenmpi","text":"The next script is just an extension of the last. We now run hybrid, a combination of MPI and OpenMP. Our base example programs, fhostame.f90 and phostname.c can be compiled in hybrid mode as well as in pure MPI and pure OpenMP. First we look at the (sorted) output from our program run in hybrid mode with 4 tasks on two nodes and 4 threads. MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r5i0n4 0000 0000 0000 0000 0001 r5i0n4 0000 0000 0004 0000 0002 r5i0n4 0000 0000 0009 0000 0003 r5i0n4 0000 0000 0014 0001 0000 r5i0n4 0000 0001 0018 0001 0001 r5i0n4 0000 0001 0022 0001 0002 r5i0n4 0000 0001 0027 0001 0003 r5i0n4 0000 0001 0032 0002 0000 r5i0n28 0002 0000 0000 0002 0001 r5i0n28 0002 0000 0004 0002 0002 r5i0n28 0002 0000 0009 0002 0003 r5i0n28 0002 0000 0014 0003 0000 r5i0n28 0002 0001 0018 0003 0001 r5i0n28 0002 0001 0022 0003 0002 r5i0n28 0002 0001 0027 0003 0003 r5i0n28 0002 0001 0032 total time 3.009 The first column is the MPI task number followed by the thread, then the node. The last column is the core on which that give task/thread was run. We can cat a list of unique combinations of nodes and cores by piping the file into grep ^0 | awk '{print $3, $6}' | sort -u | wc -l` We get 16 which is the number of tasks times the number of threads. That is, we have each task/thread assigned to its own core. This will give good performance. The script below runs on a fixed number of tasks (4 = 2 per node * 2 nodes) and using from 1 to cpus-per-task=18 threads. The variable SLURM_CPUS_PER_TASK is set by slurm to be cpus-per-task. After the srun line we post process the output to report core usage. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=2 #SBATCH --partition=short #SBATCH --cpus-per-task=18 #SBATCH --ntasks=4 module purge module load intel-mpi/2020.1.217 gcc/10.1.0 export OMP_PLACES=cores export OMP_PROC_BIND=spread echo \"CPT TASKS THREADS cores\" for n in `seq 1 $SLURM_CPUS_PER_TASK` ; do request=`python -c \"print($n*$SLURM_NTASKS)\"` have=72 if ((request <= have)); then export OMP_NUM_THREADS=$n srun --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc -F -t 3 > out.$SLURM_NTASKS.$OMP_NUM_THREADS # post process cores=`cat out.$SLURM_NTASKS.$OMP_NUM_THREADS | grep ^0 | awk '{print $3, $6}' | sort -u | wc -l` echo $SLURM_CPUS_PER_TASK \" \" $SLURM_NTASKS \" \" $OMP_NUM_THREADS \" \" $cores fi done Our final output from this script is: el3:stuff> cat slurm-7002718.out CPT TASKS THREADS cores 18 4 1 4 18 4 2 8 18 4 3 12 18 4 4 16 18 4 5 20 18 4 6 24 18 4 7 28 18 4 8 32 18 4 9 36 18 4 10 40 18 4 11 44 18 4 12 48 18 4 13 52 18 4 14 56 18 4 15 60 18 4 16 64 18 4 17 68 18 4 18 72 el3:stuff> The important lines are: #SBATCH --cpus-per-task=18 . . . export OMP_PLACES=cores export OMP_PROC_BIND=spread . . . srun --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc We need to set cpus-per-task to tell slurm we are going to run multithreaded and how many cores we are going to use for our threads. This should be set to the maximum number of threads per task we expect to use. We use the OMP variables to map threads to cores. IMPORTANT: using KMP_AFFINTY will not give the desired results. It will cause all threads for a task to be mapped to a single core. We can run this script for hybrid MPI/OpenMP programs as is or set the number of cpus-per-task and tasks on the sbatch command line. For example: sbatch --cpus-per-task=9 --ntasks=8 simple gives us: el3:stuff> cat slurm-7002858.out CPT TASKS THREADS cores 9 8 1 8 9 8 2 16 9 8 3 24 9 8 4 32 9 8 5 40 9 8 6 48 9 8 7 56 9 8 8 64 9 8 9 72 el3:stuff>","title":"6. Hybrid MPI/OpenMPI"},{"location":"blog/2021-06-18-srun/#7-mpmd-a-simple-distribution","text":"Here we look at launching Multi Program Multi Data runs. We use a the --multi-prog option with srun. This involves creating a config_file that lists the programs we are going to run along with the task ID. See: https://computing.llnl.gov/tutorials/linux_clusters/multi-prog.html for a quick description of the format for the config_file. Here we create the file on the fly but it could be done beforehand. We have two MPI programs to run together, phostone and fhostone. They are actually the same program written in C and Fortran. In the real world MPMD applications would maybe run a GUI or a manager for one task and rest doing compute. The syntax for running MPMD programs is srun --multi-prog mapfile where mapfile is a config_file that lists the programs to run. It is possible to pass different arguments to each program as discussed in the link above. Here we just add command line arguments for task 0. Our mapfile has 8 programs listed. The even tasks are running phostone and the odd fhostone. Our script uses two for loops to add lines to the mapfile and then uses sed to append command line arguments to the first line. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug #SBATCH --cpus-per-task=1 # create our mapfile app1=./phostone for n in 0 2 4 6 ; do echo $n $app1 >> mapfile done app2=./fhostone for n in 1 3 5 7 ; do echo $n $app2 >> mapfile done # add a command line option to the first line # sed does an in-place change to the first line # of our mapfile adding *-F* sed -i \"1 s/$/ -F /\" mapfile cat mapfile export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK srun -n8 --multi-prog mapfile Here is the complete output including the mapfile and output from our two programs. Lines with three digits for core number were created by the Fortran version of the program. el3:stuff> cat *7003104* 0 ./phostone -F 2 ./phostone 4 ./phostone 6 ./phostone 1 ./fhostone 3 ./fhostone 5 ./fhostone 7 ./fhostone MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r1i7n35 0000 0000 0022 0001 0000 r1i7n35 0001 0000 021 0002 0000 r1i7n35 0000 0001 0027 0003 0000 r1i7n35 0003 0000 023 0004 0000 r1i7n35 0000 0002 0020 0005 0000 r1i7n35 0005 0000 025 0006 0000 r1i7n35 0000 0003 0026 0007 0000 r1i7n35 0007 0000 019 el3:stuff>","title":"7. MPMD - a simple distribution"},{"location":"blog/2021-06-18-srun/#8-mpmd-multinode","text":"Our final example again just extends the previous one. We want to add the capability to launch different numbers of tasks on a set of nodes and at the same time have different programs on each of the nodes. We create a mapfile to list the programs to run as was done above. In this case for illustration purposes we are running one copy of phostone and seven instances of fhostone. We add to that a hostfile that lists the nodes on which to run. The hostfile has one host per MPI task. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=2 #SBATCH --partition=debug export OMP_NUM_THREADS=1 # Create our mapfile rm -rf mapfile app1=./phostone for n in 0 ; do echo $n $app1 >> mapfile done app2=./fhostone for n in 1 2 3 4 5 6 7 ; do echo $n $app2 >> mapfile done # Add a command line option to the first line # sed does an in-place change to the first line # of our mapfile adding *-F* sed -i \"1 s/$/ -F /\" mapfile # Count of each app to run on a node counts=\"1 7\" # Get a list of nodes on a single line nodes=`scontrol show hostnames | tr '\\n' ' '` # Create our hostfile and tell slrum its name export SLURM_HOSTFILE=hostlist # It is possible to do this in bash but # I think this is easier to understand # in python. It uses the values for # counts and nodes set above. python - > $SLURM_HOSTFILE << EOF c=\"$counts\".split() nodes=\"$nodes\".split() k=0 for i in c: i=int(i) node=nodes[k] for j in range(0,i): print(node) k=k+1 EOF srun -n 8 --multi-prog mapfile Here is the output from our run including the mapfile and hostlist. Notice that the first instance of the set of running programs is the C version. It is the only thing running on the first nodes. The rest of the MPI tasks are the Fortran version of the program running on the second node. el3:stuff> cat slurm-7003587.out | sort -k3,3 -k1,1 MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r102u34 0000 0000 0004 0001 0000 r102u35 0001 0000 003 0002 0000 r102u35 0002 0000 000 0003 0000 r102u35 0001 0001 006 0004 0000 r102u35 0004 0000 007 0005 0000 r102u35 0001 0002 004 0006 0000 r102u35 0002 0001 005 0007 0000 r102u35 0001 0003 002 el3:stuff> cat mapfile 0 ./phostone -F 1 ./fhostone 2 ./fhostone 3 ./fhostone 4 ./fhostone 5 ./fhostone 6 ./fhostone 7 ./fhostone el3:stuff> c el3:stuff> cat hostlist r102u34 r102u35 r102u35 r102u35 r102u35 r102u35 r102u35 r102u35 el3:stuff>","title":"8. MPMD multinode"},{"location":"blog/2022-10-04-python2to3/","text":"Running Legacy Python 2 Code on Eagle What is Legacy Code? One definition of \"legacy\" code or software is code was written in the past using currently outdated, obsolete, or otherwise deprecated, compilers, functions, methods, or methodology. While Python 2 was sunset on January 1, 2020 in favor of Python 3, there is still \"legacy\" Python 2 software that may need to be run on Eagle. We always encourage Eagle users to upgrade their code to Python 3.x to continue receiving official updates, bug fixes, and security patches. But we do understand that there will always be code that is not worth porting to Python 3. In those cases here are some options you have Set up a custom Python 2 environment Check for updates Find an alternative tool that fits your needs Convert Python 2 code to Python 3 1. Set up a custom Python 2 environment using Conda or containers It is best to create this python environment within conda . For example, conda create --name my_environment python = 2 This conda environment can be made even more portable by using Docker or Apptainer . Currently, Eagle only supports Apptainer . Subsequently, the outputs of the Python 2 code can be incorporated, e.g., using a file-based approach. It is possible to run legacy Python 2 code in parallel within a container or a conda environment, but your mileage may vary. 2. Check for updates If the software or module is still under active development, it's highly likely that the authors have transitioned the software to Python 3.x. If an updated version is available, you should strongly consider it over an outdated Python 2 version. It will likely be more secure, have better performance, be more reliable, and have a longer shelf life for reproducibility in the future. 3. Find an alternative tool that fits your needs There are usually multiple alternatives to your software of concern that have the same or slightly different features. Some of the potential advantages and disadvantages include: Advantages If they are written in Python 3, they are newer and might offer better software support. They allow for efficient parallelism, enabling better use of Eagle. Leverages Python 3 features and packages, e.g., Abstract Base Classes. Disadvantages It may break your build environment. It may require a significant code rewrite to extract the best performance. Additional code may need to be written if the alternative does not have all the features as the Python 2 software. 4. Convert Python 2 code to Python 3 This is an option when the Python 2 code under consideration does not have a lot of dependencies, you have the source code, and the software license allows you to make changes to the source code. For scientific software, we do encourage developers to make this version jump as this enables code longevitiy and accessibility. Several online resource are available to enable porting your code to Python 3. Some of them include: Porting Python 2 Code to Python 3 The Conservative Python 3 Porting Guide Supporting Python 3: An in-depth guide Python FAQ: How do I port to Python 3? How to Port Python 2 Code to Python 3 2to3 \u2014 Automated Python 2 to 3 code translation Note that the above is not an exhasutive list and we encourage the user to look at other resources as well. Additionally, depending on the needs of your project, you can also reach out to HPC-Help@nrel.gov to help with this port. Remember , since Python 2 has been officially deprecated, more and more code is either being updated or rewritten entirely in Python 3 as time passes. Additionally, the community-maintained Python 2 packages in the default conda channels will likely disappear at some point in the future. Keeping your code modernized to the latest standards will help ensure both the longevity and reproducibility of your software and your results.","title":"Running Legacy Python 2 Code on Eagle"},{"location":"blog/2022-10-04-python2to3/#running-legacy-python-2-code-on-eagle","text":"","title":"Running Legacy Python 2 Code on Eagle"},{"location":"blog/2022-10-04-python2to3/#what-is-legacy-code","text":"One definition of \"legacy\" code or software is code was written in the past using currently outdated, obsolete, or otherwise deprecated, compilers, functions, methods, or methodology. While Python 2 was sunset on January 1, 2020 in favor of Python 3, there is still \"legacy\" Python 2 software that may need to be run on Eagle. We always encourage Eagle users to upgrade their code to Python 3.x to continue receiving official updates, bug fixes, and security patches. But we do understand that there will always be code that is not worth porting to Python 3. In those cases here are some options you have Set up a custom Python 2 environment Check for updates Find an alternative tool that fits your needs Convert Python 2 code to Python 3","title":"What is Legacy Code?"},{"location":"blog/2022-10-04-python2to3/#1-set-up-a-custom-python-2-environment-using-conda-or-containers","text":"It is best to create this python environment within conda . For example, conda create --name my_environment python = 2 This conda environment can be made even more portable by using Docker or Apptainer . Currently, Eagle only supports Apptainer . Subsequently, the outputs of the Python 2 code can be incorporated, e.g., using a file-based approach. It is possible to run legacy Python 2 code in parallel within a container or a conda environment, but your mileage may vary.","title":"1. Set up a custom Python 2 environment using Conda or containers"},{"location":"blog/2022-10-04-python2to3/#2-check-for-updates","text":"If the software or module is still under active development, it's highly likely that the authors have transitioned the software to Python 3.x. If an updated version is available, you should strongly consider it over an outdated Python 2 version. It will likely be more secure, have better performance, be more reliable, and have a longer shelf life for reproducibility in the future.","title":"2. Check for updates"},{"location":"blog/2022-10-04-python2to3/#3-find-an-alternative-tool-that-fits-your-needs","text":"There are usually multiple alternatives to your software of concern that have the same or slightly different features. Some of the potential advantages and disadvantages include: Advantages If they are written in Python 3, they are newer and might offer better software support. They allow for efficient parallelism, enabling better use of Eagle. Leverages Python 3 features and packages, e.g., Abstract Base Classes. Disadvantages It may break your build environment. It may require a significant code rewrite to extract the best performance. Additional code may need to be written if the alternative does not have all the features as the Python 2 software.","title":"3. Find an alternative tool that fits your needs"},{"location":"blog/2022-10-04-python2to3/#4-convert-python-2-code-to-python-3","text":"This is an option when the Python 2 code under consideration does not have a lot of dependencies, you have the source code, and the software license allows you to make changes to the source code. For scientific software, we do encourage developers to make this version jump as this enables code longevitiy and accessibility. Several online resource are available to enable porting your code to Python 3. Some of them include: Porting Python 2 Code to Python 3 The Conservative Python 3 Porting Guide Supporting Python 3: An in-depth guide Python FAQ: How do I port to Python 3? How to Port Python 2 Code to Python 3 2to3 \u2014 Automated Python 2 to 3 code translation Note that the above is not an exhasutive list and we encourage the user to look at other resources as well. Additionally, depending on the needs of your project, you can also reach out to HPC-Help@nrel.gov to help with this port. Remember , since Python 2 has been officially deprecated, more and more code is either being updated or rewritten entirely in Python 3 as time passes. Additionally, the community-maintained Python 2 packages in the default conda channels will likely disappear at some point in the future. Keeping your code modernized to the latest standards will help ensure both the longevity and reproducibility of your software and your results.","title":"4. Convert Python 2 code to Python 3"}]}